Automatically generated by Mendeley Desktop 1.18
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Nushi2016a,
abstract = {We study the problem of troubleshooting machine learning systems that rely on analytical pipelines of distinct components. Understanding and fixing errors that arise in such integrative systems is difficult as failures can occur at multiple points in the execution workflow. Moreover, errors can propagate, become amplified or be suppressed, making blame assignment difficult. We propose a human-in-the-loop methodology which leverages human intellect for troubleshooting system failures. The approach simulates potential component fixes through human computation tasks and measures the expected improvements in the holistic behavior of the system. The method provides guidance to designers about how they can best improve the system. We demonstrate the effectiveness of the approach on an automated image captioning system that has been pressed into real-world use.},
archivePrefix = {arXiv},
arxivId = {1611.08309},
author = {Nushi, Besmira and Kamar, Ece and Horvitz, Eric and Kossmann, Donald},
eprint = {1611.08309},
file = {:Users/Walter/Documents/Literature/1611.08309.pdf:pdf},
title = {{On Human Intellect and Machine Failures: Troubleshooting Integrative Machine Learning Systems}},
url = {http://arxiv.org/abs/1611.08309},
year = {2016}
}
@article{Johansson2011,
abstract = {Random forest is an often used ensemble technique, renowned for its high predictive performance. Random forests models are, however, due to their sheer complexity inherently opaque, making human interpretation and analysis impossible. This paper presents a method of approximating the random forest with just one decision tree. The approach uses oracle coaching, a recently suggested technique where a weaker but transparent model is generated using combinations of regular training data and test data initially labeled by a strong classifier, called the oracle. In this study, the random forest plays the part of the oracle, while the transparent models are decision trees generated by either the standard tree inducer J48, or by evolving genetic programs. Evaluation on 30 data sets from the UCI repository shows that oracle coaching significantly improves both accuracy and area under ROC curve, compared to using training data only. As a matter of fact, resulting single tree models are as accurate as the random forest, on the specific test instances. Most importantly, this is not achieved by inducing or evolving huge trees having perfect fidelity; a large majority of all trees are instead rather compact and clearly comprehensible. The experiments also show that the evolution outperformed J48, with regard to accuracy, but that this came at the expense of slightly larger trees.},
annote = {Create one decision tree to replace a random forest},
author = {Johansson, Ulf and Sonstrod, Cecilia and Lofstrom, Tuve},
doi = {10.1109/CEC.2011.5949785},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Johansson, Sonstrod, Lofstrom - 2011 - One tree to explain them all.pdf:pdf},
isbn = {9781424478347},
issn = {Pending},
journal = {2011 IEEE Congress of Evolutionary Computation, CEC 2011},
pages = {1444--1451},
title = {{One tree to explain them all}},
year = {2011}
}
@article{Szegedy2013,
abstract = {Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninter- pretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extent. We can cause the network to misclas- sify an image by applying a certain hardly perceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.},
archivePrefix = {arXiv},
arxivId = {arXiv:1312.6199v4},
author = {Szegedy, Christian and Zaremba, W and Sutskever, I},
doi = {10.1021/ct2009208},
eprint = {arXiv:1312.6199v4},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Szegedy, Zaremba, Sutskever - 2013 - Intriguing properties of neural networks.pdf:pdf},
isbn = {1549-9618},
issn = {15499618},
journal = {arXiv preprint arXiv: {\ldots}},
pages = {1--10},
pmid = {22545027},
title = {{Intriguing properties of neural networks}},
url = {http://arxiv.org/abs/1312.6199},
year = {2013}
}
@article{Ribeiro2016,
abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
archivePrefix = {arXiv},
arxivId = {1602.04938},
author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
doi = {10.1145/2939672.2939778},
eprint = {1602.04938},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Ribeiro, Singh, Guestrin - 2016 - {\&}quotWhy Should I Trust You{\&}quot Explaining the Predictions of Any Classifier.pdf:pdf},
isbn = {9781450321389},
issn = {9781450321389},
title = {{"Why Should I Trust You?": Explaining the Predictions of Any Classifier}},
url = {http://arxiv.org/abs/1602.04938},
year = {2016}
}
@article{Cano2007,
abstract = {The generation of predictive models is a frequent task in data mining with the objective of generating highly precise and interpretable models. The data reduction is an interesting preprocessing approach that can allow us to obtain predictive models with these characteristics in large size data sets. In this paper, we analyze the rule classification model based on decision trees using a training selected set via evolutionary stratified instance selection. This method faces the scaling problem that appears in the evaluation of large size data sets, and the trade off interpretability-precision of the generated models. {\textcopyright} 2006 Elsevier B.V. All rights reserved.},
author = {Cano and Herrera, Francisco and Lozano, Manuel},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Cano, Herrera, Lozano - 2007 - Evolutionary stratified training set selection for extracting classification rules with trade off prec(2).pdf:pdf},
issn = {0169023X},
journal = {Data and Knowledge Engineering},
keywords = {Decision trees,Evolutionary algorithms,Interpretability,Precision,Rule classification,Training set selection},
pages = {90--108},
title = {{Evolutionary stratified training set selection for extracting classification rules with trade off precision-interpretability}},
volume = {60},
year = {2007}
}
@article{Hastie2009,
abstract = {During the past decade there has been an explosion in computation and information technology. With it has come vast amounts of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of color graphics. It should be a valuable resource for statisticians and anyone interested in data mining in science or industry. The book's coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics include neural networks, support vector machines, classification trees and boosting-the first comprehensive treatment of this topic in any book. Trevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at Stanford University. They are prominent researchers in this area: Hastie and Tibshirani developed generalized additive models and wrote a popular book of that title. Hastie wrote much of the statistical modeling software in S-PLUS and invented principal curves and surfaces. Tibshirani proposed the Lasso and is co-author of the very successful An Introduction to the Bootstrap. Friedman is the co-inventor of many data-mining tools including CART, MARS, and projection pursuit. FROM THE REVIEWS: TECHNOMETRICS "This is a vast and complex book. Generally, it concentrates on explaining why and how the methods work, rather than how to use them. Examples and especially the visualizations are principle features...As a source for the methods of statistical learning...it will probably be a long time before there is a competitor to this book."},
author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Hastie, Tibshirani, Friedman - 2009 - The Elements of Statistical Learning(2).pdf:pdf},
isbn = {9780387848570},
issn = {03436993},
journal = {Elements},
pages = {337--387},
pmid = {15512507},
title = {{The Elements of Statistical Learning}},
volume = {1},
year = {2009}
}
@phdthesis{Bennette2011,
author = {Bennette, Walter Dean},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Bennette - 2011 - Instance selection for simplified decision trees through the generation and selection of instance candidate subsets(2).pdf:pdf},
school = {Iowa State University},
title = {{Instance selection for simplified decision trees through the generation and selection of instance candidate subsets}},
year = {2011}
}
@article{Rajabioun2011,
abstract = {In this paper a novel evolutionary algorithm, suitable for continuous nonlinear optimization problems, is introduced. This optimization algorithm is inspired by the life of a bird family, called Cuckoo. Special lifestyle of these birds and their characteristics in egg laying and breeding has been the basic motivation for development of this new evolutionary optimization algorithm. Similar to other evolutionary methods, Cuckoo Optimization Algorithm (COA) starts with an initial population. The cuckoo population, in different societies, is in two types: mature cuckoos and eggs. The effort to survive among cuckoos constitutes the basis of Cuckoo Optimization Algorithm. During the survival competition some of the cuckoos or their eggs, demise. The survived cuckoo societies immigrate to a better environment and start reproducing and laying eggs. Cuckoos' survival effort hopefully converges to a state that there is only one cuckoo society, all with the same profit values. Application of the proposed algorithm to some benchmark functions and a real problem has proven its capability to deal with difficult optimization problems. ?? 2011 Elsevier B.V. All rights reserved.},
annote = {Even though human expertise should be used when it is needed and available; it often proves less adquate for automated problem-solving routines},
author = {Rajabioun, Ramin},
doi = {10.1016/j.asoc.2011.05.008},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Rajabioun - 2011 - Cuckoo optimization algorithm.pdf:pdf},
isbn = {0780394844},
issn = {15684946},
journal = {Applied Soft Computing Journal},
keywords = {Cuckoo Optimization Algorithm (COA),Evolutionary algorithms,Nonlinear optimization},
number = {8},
pages = {5508--5518},
publisher = {Elsevier B.V.},
title = {{Cuckoo optimization algorithm}},
url = {http://dx.doi.org/10.1016/j.asoc.2011.05.008},
volume = {11},
year = {2011}
}
@article{Chen2018,
abstract = {When building a classifier in interactive machine learning, human knowledge about the target class can be a powerful ref-erence to make the classifier robust to unseen items. The main challenge lies in finding unlabeled items that can either help discover or refine concepts for which the current classifier has no corresponding features (i.e., it has feature blindness). Yet it is unrealistic to ask humans to come up with an exhaustive list of items, especially for rare concepts that are hard to recall. This paper presents AnchorViz, an interactive visualization that facilitates error discovery through semantic data exploration. By creating example-based anchors, users create a topology to spread data based on their similarity to the anchors and examine the inconsistencies between data points that are se-mantically related. The results from our user study show that AnchorViz helps users discover more prediction errors than stratified random and uncertainty sampling methods.},
author = {Chen, Nan-Chen and Suh, Jina and Verwey, Johan and Ramos, Gonzalo and Drucker, Steven and Simard, Patrice},
doi = {10.1145/3172944.3172950},
file = {:Users/Walter/Documents/Literature/AnchorViz-camera-ready.pdf:pdf},
isbn = {9781450349451},
journal = {23rd International Conference on Intelligent User Interfaces},
keywords = {error discovery,interactive machine learning,semantic data exploration,unlabeled data,visualization},
pages = {269--280},
title = {{AnchorViz: Facilitating Classifier Error Discovery through Interactive Semantic Data Exploration}},
url = {http://delivery.acm.org.ezproxy.uniandes.edu.co:8080/10.1145/3180000/3172950/p269-chen.pdf?ip=157.253.50.50{\&}id=3172950{\&}acc=OPEN{\&}key=4D9619BEF5D5941F.F94FA2F060A28848.4D4702B0C3E38B35.6D218144511F3437{\&}{\_}{\_}acm{\_}{\_}=1521117778{\_}f7ef77ffb00a6d07bbad9d34e8603268{\#}URL},
year = {2018}
}
@article{Marx2013,
abstract = {As they grapple with increasingly large data sets, biologists and computer scientists uncork new bottlenecks.},
author = {Marx, Vivien},
isbn = {0028-0836},
issn = {0028-0836},
journal = {Nature},
pages = {255--260},
pmid = {23765498},
title = {{Biology: The big challenges of big data}},
volume = {498},
year = {2013}
}
@article{Czarnowski2010,
author = {Czarnowski, Ireneusz and Jȩdrzejowicz, Piotr},
doi = {10.1007/978-3-642-16693-8_37},
isbn = {364216692X},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {PART 1},
pages = {353--362},
title = {{Cluster integration for the cluster-based instance selection}},
volume = {6421 LNAI},
year = {2010}
}
@article{Garcia-Pedrajas2011,
author = {Garc{\'{i}}a-Pedrajas, Nicol{\'{a}}s},
issn = {19424787},
journal = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
month = {nov},
number = {6},
pages = {512--523},
title = {{Evolutionary computation for training set selection}},
url = {http://doi.wiley.com/10.1002/widm.44},
volume = {1},
year = {2011}
}
@article{Kubat1997,
abstract = {Existing concept learning systems can fail when the negative examples heavily outnumber the positive examples. The paper discusses one essential trouble brought about by imbalanced training sets and presents a learning algorithm addressing this issue. The experiments (with synthetic and real-world data) focus on 2-class problems with examples described with binary and continuous attributes.$\backslash$n$\backslash$nUsing G-mean!$\backslash$n},
annote = {Looks like an interesting method, wonder if it has been adapted for multiclass, wonder how the decision boundary would look if we simply call everything within the ranges positive

- good use cases of imbalanced learning
- why abundant negatives hurt
- introduces SHRINK which seems to just label everything as positive in the positive region
- ill suited for dijunctive concepts},
author = {Kubat, Miroslav and Holte, Robert and Matwin, Stan},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Kubat, Holte, Matwin - 1997 - Learning when Negative Examples Abound.pdf:pdf},
isbn = {3540628584},
journal = {Machine Learning: ECML-97},
pages = {146--153},
title = {{Learning when Negative Examples Abound}},
year = {1997}
}
@article{Mahabal2011,
abstract = {Exploration of the time domain - variable and transient objects and phenomena - is rapidly becoming a vibrant research frontier, touching on essentially every field of astronomy and astrophysics, from the Solar system to cosmology. Time domain astronomy is being enabled by the advent of the new generation of synoptic sky surveys that cover large areas on the sky repeatedly, and generating massive data streams. Their scientific exploration poses many challenges, driven mainly by the need for a real-time discovery, classification, and follow-up of the interesting events. Here we describe the Catalina Real-Time Transient Survey (CRTS), that discovers and publishes transient events at optical wavelengths in real time, thus benefiting the entire community. We describe some of the scientific results to date, and then focus on the challenges of the automated classification and prioritization of transient events. CRTS represents a scientific and a technological testbed and precursor for the larger surveys in the future, including the Large Synoptic Survey Telescope (LSST) and the Square Kilometer Array (SKA). [ABSTRACT FROM AUTHOR]},
archivePrefix = {arXiv},
arxivId = {1111.0313},
author = {Mahabal, A. A. and Djorgovski, S. G. and Drake, A. J. and Donalek, C. and Graham, M. J. and Williams, R. D. and Chen, Y. and Moghaddam, B. and Turmon, M. and Beshore, E. and Larson, S.},
eprint = {1111.0313},
file = {:Users/Walter/Documents/Literature/1111.0313.pdf:pdf},
issn = {03049523},
journal = {Bulletin of the Astronomical Society of India},
keywords = {Galaxies: active,Quasars,Stars: variables: other,Supernovae,Surveys},
number = {3},
pages = {387--408},
title = {{Discovery, classiffcation, and scientiffc exploration of transient events from the Catalina Real-Time Transient Survey}},
volume = {39},
year = {2011}
}
@article{Rokach2010,
abstract = {The idea of ensemble methodology is to build a predictive model by integrating multiple models. It is well-known that ensemble methods can be used for improving predic- tion performance. Researchers from various disciplines such as statistics and AI considered the use of ensemble methodology. This paper, review existing ensemble techniques and can be served as a tutorial for practitioners who are interested in building ensemble based systems.},
author = {Rokach, Lior},
doi = {10.1007/s10462-009-9124-7},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Rokach - 2010 - Ensemble-based classifiers.pdf:pdf},
isbn = {0269-2821},
issn = {02692821},
journal = {Artificial Intelligence Review},
keywords = {Boosting,Classification,Ensemble of classifiers,Supervised learning},
number = {1-2},
pages = {1--39},
title = {{Ensemble-based classifiers}},
volume = {33},
year = {2010}
}
@article{Guo2004,
abstract = {Learning from imbalanced data sets, where the number of examples of one (majority) class is much higher than the others, presents an important challenge to the machine learning community. Traditional machine learning algorithms may be biased towards the majority class, thus producing poor predictive accuracy over the minority class. In this paper, we describe a new approach that combines boosting, an ensemble-based learning algorithm, with data generation to improve the predictive power of classifiers against imbalanced data sets consisting of two classes. In the DataBoost-IM method, hard examples from both the majority and minority classes are identified during execution of the boosting algorithm. Subsequently, the hard examples are used to separately generate synthetic examples for the majority and minority classes. The synthetic data are then added to the original training set, and the class distribution and the total weights of the different classes in the new training set are rebalanced. The DataBoost-IM method was evaluated, in terms of the F-measures, G-mean and overall accuracy, against seventeen highly and moderately imbalanced data sets using decision trees as base classifiers. Our results are promising and show that the DataBoost-IM method compares well in comparison with a base classifier, a standard benchmarking boosting algorithm and three advanced boosting-based algorithms for imbalanced data set. Results indicate that our approach does not sacrifice one class in favor of the other, but produces high predictions against both minority and majority classes.},
author = {Guo, Hongyu and Herna, Victor},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Guo, Herna - 2004 - Learning from Imbalanced Data Sets with Boosting and Data Generation The DataBoost-IM Approach.pdf:pdf},
journal = {ACM SIGKDD Explorations Newsletter 6.1},
keywords = {data mining,ensembles of classifiers,imbalanced data sets},
number = {1},
pages = {30--39},
title = {{Learning from Imbalanced Data Sets with Boosting and Data Generation : The DataBoost-IM Approach}},
volume = {6},
year = {2004}
}
@article{Tamagnini2017,
abstract = {To realize the full potential of machine learning in diverse real-world domains, it is necessary for model predictions to be readily interpretable and actionable for the human in the loop. Analysts, who are the users but not the developers of machine learning mod-els, often do not trust a model because of the lack of transparency in associating predictions with the underlying data space. To address this problem, we propose Rivelo, a visual analytics interface that enables analysts to understand the causes behind predictions of binary classiiers by interactively exploring a set of instance-level explanations. These explanations are model-agnostic, treating a model as a black box, and they help analysts in interactively prob-ing the high-dimensional binary data space for detecting features relevant to predictions. We demonstrate the utility of the interface with a case study analyzing a random forest model on the sentiment of Yelp reviews about doctors.},
author = {Tamagnini, Paolo and Krause, Josua and Dasgupta, Aritra and Bertini, Enrico},
doi = {10.1145/3077257.3077260},
file = {:Users/Walter/Documents/Literature/Enrico Bertini - Interpreting Black-Box Classifiers.pdf:pdf},
isbn = {9781450350297},
issn = {15232867},
journal = {Proceedings of the 2nd Workshop on Human-In-the-Loop Data Analytics  - HILDA'17},
keywords = {2017,acm reference format,and enrico bertini,aritra dasgupta,classi cation,explanation,josua krause,machine learning,paolo tamagnini,visual analytics},
pages = {1--6},
title = {{Interpreting Black-Box Classifiers Using Instance-Level Visual Explanations}},
url = {http://dl.acm.org/citation.cfm?doid=3077257.3077260},
year = {2017}
}
@book{Eshelman1991,
author = {Eshelman},
publisher = {Morgan-Kaufmann},
title = {{The CHC Adaptive Search Algorithm. Foundations of Genetic Algorithms}},
year = {1991}
}
@article{Cover1967,
abstract = {The nearest neighbor decision rule assigns to an unclassified sample point the classification of the nearest of a set of previously classified points. This rule is independent of the underlying joint distribution on the sample points and their classifications, and hence the probability of error{\textless}tex{\textgreater}R{\textless}/tex{\textgreater}of such a rule must be at least as great as the Bayes probability of error{\textless}tex{\textgreater}R{\^{}}{\{}ast{\}}{\textless}/tex{\textgreater}--the minimum probability of error over all decision rules taking underlying probability structure into account. However, in a large sample analysis, we will show in the{\textless}tex{\textgreater}M{\textless}/tex{\textgreater}-category case that{\textless}tex{\textgreater}R{\^{}}{\{}ast{\}} leq R leq R{\^{}}{\{}ast{\}}(2 --MR{\^{}}{\{}ast{\}}/(M-1)){\textless}/tex{\textgreater}, where these bounds are the tightest possible, for all suitably smooth underlying distributions. Thus for any number of categories, the probability of error of the nearest neighbor rule is bounded above by twice the Bayes probability of error. In this sense, it may be said that half the classification information in an infinite sample set is contained in the nearest neighbor.},
author = {Cover, T. and Hart, P.},
doi = {10.1109/TIT.1967.1053964},
isbn = {0018-9448},
issn = {0018-9448},
journal = {IEEE Transactions on Information Theory},
number = {1},
pages = {21--27},
pmid = {21919855},
title = {{Nearest neighbor pattern classification}},
volume = {13},
year = {1967}
}
@book{Cheng2014,
author = {Cheng, Shi and Ting, T O and Yang, Xin-she},
booktitle = {Springer Proceedings in Mathematics {\&} Statistics},
doi = {10.1007/978-3-319-08985-0},
isbn = {978-3-319-08984-3},
keywords = {large-scale},
pages = {241--253},
title = {{Solving Computationally Expensive Engineering Problems}},
url = {http://link.springer.com/10.1007/978-3-319-08985-0},
volume = {97},
year = {2014}
}
@article{Majnik2013,
abstract = {The use of ROC (receiver operating characteristics) analysis as a tool to evaluate performance of classiﬁcation models in machine learning has been increasing in the last decade. Among the most notable advances in this area is that the two-class ROC analysis has been extended to multi-class problems and that the ROC analysis in cost-sensitive learning has been considered, as well. Methods exist which take instance-varying costs into account. The purpose of our paper is to present a survey of this ﬁeld with the aim to gather important achievements in one place. In the paper, we present application areas of the ROC analysis in machine learning, describe its problems and challenges and provide a summarized list of alternative approaches to ROC analysis. In addition to presented theory, we also provide a couple of examples intended to illustrate the described approaches.},
author = {Majnik, Matja{\v{z}} and Bosni{\'{c}}, Zoran},
issn = {1088467X},
journal = {Intelligent Data Analysis},
keywords = {ROC,ROC analysis,classification,machine learning,performance},
pages = {531--558},
title = {{ROC analysis of classifiers in machine learning: A survey}},
volume = {17},
year = {2013}
}
@book{Hosmer2004,
author = {Hosmer, David and Lemeshow, Stanley},
doi = {10.1080/00401706.1992.10485291},
file = {:Users/Walter/Documents/Literature/Applied{\_}Logistic{\_}Regression.pdf:pdf},
isbn = {9780470582473},
pages = {392},
title = {{Applied Logistic Regression}},
volume = {398},
year = {2004}
}
@article{Atkinson1982,
author = {Atkinson, A.C},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Atkinson - 1982 - Developments in the Design of Experiments.pdf:pdf},
number = {2},
pages = {161--177},
title = {{Developments in the Design of Experiments}},
volume = {50},
year = {1982}
}
@article{Wasikowski2010,
author = {Wasikowski, Mike and Chen, Xue-wen},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Wasikowski, Chen - 2010 - Combating the Small Sample Class Imbalance Problem Using Feature Selection.pdf:pdf},
number = {10},
pages = {1388--1400},
title = {{Combating the Small Sample Class Imbalance Problem Using Feature Selection}},
volume = {22},
year = {2010}
}
@article{Zheng2004,
abstract = {A number of feature selection metrics have been explored in text categorization, among which information gain (IG), chi-square (CHI), correlation coefficient (CC) and odds ratios (OR) are considered most effective. CC and OR are one-sided metrics while IG and CHI are two-sided. Feature selection using one-sided metrics selects the features most indicative of membership only, while feature selection using two-sided metrics implicitly combines the features most indicative of membership (e.g. positive features) and non-membership (e.g. negative features) by ignoring the signs of features. The former never consider the negative features, which are quite valuable, while the latter cannot ensure the optimal combination of the two kinds of features especially on imbalanced data. In this work, we investigate the usefulness of explicit control of that combination within a proposed feature selection framework. Using multinomial na{\"{i}}ve Bayes and regularized logistic regression as classifiers, our experiments show both great potential and actual merits of explicitly combining positive and negative features in a nearly optimal fashion according to the imbalanced data.},
author = {Zheng, Zhaohui},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Zheng - 2004 - Feature Selection for Text Categorization on Imbalanced Data.pdf:pdf},
journal = {ACM SIGKDD Explorations Newsletter},
number = {1},
pages = {80--89},
title = {{Feature Selection for Text Categorization on Imbalanced Data}},
volume = {6},
year = {2004}
}
@article{Bau2017,
abstract = {We propose a general framework called Network Dissection for quantifying the interpretability of latent representations of CNNs by evaluating the alignment between individual hidden units and a set of semantic concepts. Given any CNN model, the proposed method draws on a broad data set of visual concepts to score the semantics of hidden units at each intermediate convolutional layer. The units with semantics are given labels across a range of objects, parts, scenes, textures, materials, and colors. We use the proposed method to test the hypothesis that interpretability of units is equivalent to random linear combinations of units, then we apply our method to compare the latent representations of various networks when trained to solve different supervised and self-supervised training tasks. We further analyze the effect of training iterations, compare networks trained with different initializations, examine the impact of network depth and width, and measure the effect of dropout and batch normalization on the interpretability of deep visual representations. We demonstrate that the proposed method can shed light on characteristics of CNN models and training methods that go beyond measurements of their discriminative power.},
archivePrefix = {arXiv},
arxivId = {1704.05796},
author = {Bau, David and Zhou, Bolei and Khosla, Aditya and Oliva, Aude and Torralba, Antonio},
doi = {10.1109/CVPR.2017.354},
eprint = {1704.05796},
file = {:Users/Walter/Documents/Literature/final-network-dissection.pdf:pdf},
title = {{Network Dissection: Quantifying Interpretability of Deep Visual Representations}},
url = {http://arxiv.org/abs/1704.05796},
year = {2017}
}
@misc{Dietterich2002,
author = {Dietterich, Thomas G.},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Dietterich - 2002 - Ensemble Learning.pdf:pdf},
title = {{Ensemble Learning}},
year = {2002}
}
@article{CorinnaCortes2005,
abstract = {Inmany applications, good ranking is a highly desirable performance for a classifier. The criterion commonly used to measure the ranking quality of a classification algorithm is the area under the ROC curve (AUC). To report it properly, it is crucial to determine an interval of confidence for its value. This paper provides confidence intervals for the AUC based on a statistical and combinatorial analysis using only simple parameters such as the error rate and the number of positive and negative examples. The analysis is distribution-independent, it makes no assumption about the distribution of the scores of negative or positive examples. The results are of practical use and can be viewed as the equivalent for AUC of the standard confidence intervals given in the case of the error rate. They are comparedwith previous approaches in several standard classification tasks demonstrating the benefits of our analysis. 1},
author = {{Corinna Cortes}, Mehryar Mohri},
journal = {Advances in Neural Information Processing Systems {\ldots}},
pages = {305--312},
title = {{Confidence intervals for the area under the ROC curve}},
url = {http://books.google.com/books?hl=en{\&}lr={\&}id=etp-l5VrbHsC{\&}oi=fnd{\&}pg=PA305{\&}dq=Confidence+Intervals+for+the+Area+under+the+ROC+Curve{\&}ots={\_}K6x0GtGxG{\&}sig={\_}clX-1y-IV17gIDSI5c63gBewSg},
volume = {17},
year = {2005}
}
@article{Augusto,
author = {Augusto, Douglas A and Barbosa, Helio J C},
file = {:Users/Walter/Documents/Literature/fcda32f0367460cddd0762d6bd95181aec6e.pdf:pdf},
keywords = {data reduction,instance selection,support vector machines},
title = {{A Support Vector Machine-Based Technique for Instance Selection 1 INTRODUCTION}}
}
@article{Xiong2012,
abstract = {Metric learning makes it plausible to learn distances for complex distributions of data from labeled data. However, to date, most metric learning methods are based on a single Mahalanobis metric, which cannot handle heterogeneous data well. Those that learn multiple metrics throughout the space have demonstrated superior accuracy, but at the cost of computational efficiency. Here, we take a new angle to the metric learning problem and learn a single metric that is able to implicitly adapt its distance function throughout the feature space. This metric adaptation is accomplished by using a random forest-based classifier to underpin the distance function and incorporate both absolute pairwise position and standard relative position into the representation. We have implemented and tested our method against state of the art global and multi-metric methods on a variety of data sets. Overall, the proposed method outperforms both types of methods in terms of accuracy (consistently ranked first) and is an order of magnitude faster than state of the art multi-metric methods (16x faster in the worst case).},
annote = {- Says lots of people use Mahalnobis distance
- References a method that requires solving a programming problem},
archivePrefix = {arXiv},
arxivId = {arXiv:1201.0610v1},
author = {Xiong, Caiming and Johnson, David and Xu, Ran and Corso, Jason J.},
doi = {10.1145/2339530.2339680},
eprint = {arXiv:1201.0610v1},
file = {:Users/Walter/Documents/Literature/1201.0610.pdf:pdf},
isbn = {9781450314626},
issn = {9781450314626},
journal = {Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD '12},
keywords = {metric learning,pairwise constraints,random forests},
pages = {958},
title = {{Random forests for metric learning with implicit pairwise position dependence}},
url = {http://dl.acm.org/citation.cfm?doid=2339530.2339680},
year = {2012}
}
@article{Chen2013,
abstract = {Support vector machine (SVM) has shown prominent performance for binary classification. How to effectively apply it to massive datasets with large number of classes and instances is still a serious challenge. Instance selection methods have been proposed and shown significant efficacy for reducing the training complexity of SVM, but more or less trade off the generalization performance. This paper presents an instance selection method especially for multi-class problems. With cluster centers of positive class as reference points instances are selected for each one-versus-rest SVM model. The purpose of clustering here is to improve the efficiency of instance selection, other than to select instances directly from clusters as previous methods did. Experiments on a wide variety of datasets demonstrate that the proposed method selects fewer instances than most competitive algorithms and keeps the highest classification accuracy on most datasets. Additionally, experimental results show that this method also performs superiorly for binary problems. {\textcopyright} 2013 Elsevier B.V. All rights reserved.},
annote = {review of TSS for svm, filter

make clusters, then select based off of distance to cluster centers},
author = {Chen, Jingnian and Zhang, Caiming and Xue, Xiaoping and Liu, Cheng Lin},
isbn = {09507051},
issn = {09507051},
journal = {Knowledge-Based Systems},
keywords = {Classification,Clustering,Instance selection,Multi-class,SVM},
pages = {1--7},
publisher = {Elsevier B.V.},
title = {{Fast instance selection for speeding up support vector machines}},
volume = {45},
year = {2013}
}
@article{Lopez2014,
abstract = {A wide number of real word applications presents a class distribution where examples belonging to one class heavily outnumber the examples in the other class. This is an arduous situation where standard classification techniques usually decrease their performance, creating a handicap to correctly identify the minority class, which is precisely the case under consideration in these applications.In this work, we propose the usage of the Iterative Instance Adjustment for Imbalanced Domains (IPADE-ID) algorithm. It is an evolutionary framework, which uses an instance generation technique, designed to face the existing imbalance modifying the original training set. The method, iteratively learns the appropriate number of examples that represent the classes and their particular positioning. The learning process contains three key operations in its design: a customized initialization procedure, an evolutionary optimization of the positioning of the examples and a selection of the most representative examples for each class.An experimental analysis is carried out with a wide range of highly imbalanced datasets over the proposal and recognized solutions to the problem. The results obtained, which have been contrasted through non-parametric statistical tests, show that our proposal outperforms previously proposed methods. {\textcopyright} 2013 Elsevier B.V.},
author = {L{\'{o}}pez, Victoria and Triguero, Isaac and Carmona, Crist{\'{o}}bal J. and Garc{\'{i}}a, Salvador and Herrera, Francisco},
doi = {10.1016/j.neucom.2013.01.050},
file = {:Users/Walter/Documents/Literature/10.1.1.413.211.pdf:pdf},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Decision tree,Differential evolution,Imbalanced datasets,Instance generation,Nearest neighbor},
pages = {15--28},
title = {{Addressing imbalanced classification with instance generation techniques: IPADE-ID}},
volume = {126},
year = {2014}
}
@article{Passini2013,
annote = {working on the reduction problem

List the class names for Reuters4 and Reuters10

Sort of a sloppy study, can't believe this was accepted. How are they running CHC? what is their objective? why so few iterations of CHC?

good reference for times CHC is used

do call out datasets to use},
author = {Passini, C and Luiza, M},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Passini, Luiza - 2013 - A Strategy for Training Set Selection in Text Classification Problems.pdf:pdf},
journal = {International Journal of Advanced Computer Science {\&} Applications},
keywords = {feature selection,problems,text mining},
number = {6},
pages = {54--60},
title = {{A Strategy for Training Set Selection in Text Classification Problems.}},
url = {http://search.ebscohost.com/login.aspx?direct=true{\&}profile=ehost{\&}scope=site{\&}authtype=crawler{\&}jrnl=21565570{\&}AN=89084544{\&}h=yaRSjH255uKuIcFW1GzSvbDChZJAlOdEqyMOs6O0mp6IAR9RkykF3dPI4f5/7m9CSgzvDn9Gl+8LKVWDL1fFCQ=={\&}crl=c},
volume = {4},
year = {2013}
}
@article{Oliver2018,
archivePrefix = {arXiv},
arxivId = {1804.09170},
author = {Oliver, Avital and Odena, Augustus and Raffel, Colin and Cubuk, Ekin D and Goodfellow, Ian J and Brain, Google},
eprint = {1804.09170},
file = {:Users/Walter/Documents/Literature/1804.09170.pdf:pdf},
pages = {1--15},
title = {{Realistic Evaluation of Semi-Supervised Learning Algorithms}},
year = {2018}
}
@article{Patel2014,
author = {Patel, Vm and Gopalan, Raghuraman and Li, Ruonan and Chellappa, R},
doi = {10.1109/MSP.2014.2347059},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Patel et al. - 2014 - Visual Domain Adaptation An Overview of Recent Advances.pdf:pdf},
issn = {10535888},
pages = {1--34},
title = {{Visual Domain Adaptation: An Overview of Recent Advances}},
url = {http://www.umiacs.umd.edu/users/pvishalm/Journal{\_}pub/SPM{\_}DA{\_}v7{\_}embeded.pdf},
year = {2014}
}
@article{Mannila1997,
author = {Mannila, Heikki and Toivonen, Hannu and {Inkeri Verkamo}, A.},
doi = {10.1023/A:1009748302351},
issn = {13845810},
journal = {Data Mining and Knowledge Discovery},
keywords = {event sequences,frequent episodes,sequence analysis},
number = {3},
pages = {259--289},
title = {{Discovery of Frequent Episodes in Event Sequences}},
url = {http://link.springer.com/10.1023/A:1009748302351},
volume = {1},
year = {1997}
}
@article{Guyon2003,
abstract = {Variable and feature selection have become the focus of much research in areas of application for which datasets with tens or hundreds of thousands of variables are available. These areas include text processing of internet documents, gene expression array analysis, and combinatorial chemistry. The objective of variable selection is three-fold: improving the prediction performance of the pre- dictors, providing faster andmore cost-effective predictors, and providing a better understanding of the underlying process that generated the data. The contributions of this special issue cover a wide range of aspects of such problems: providing a better definition of the objective function, feature construction, feature ranking, multivariate feature selection, efficient search methods, and feature validity assessment methods.},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Guyon, Isabelle and Elisseeff, Andr{\'{e}}},
doi = {10.1016/j.aca.2011.07.027},
eprint = {1111.6189v1},
file = {:Users/Walter/Documents/Literature/guyon03a.pdf:pdf},
isbn = {0885-6125},
issn = {00032670},
journal = {Journal of Machine Learning Research (JMLR)},
keywords = {Biochemical oxygen demand,Kernel discriminant analysis,Kernel partial least squares,Support vector classification,Support vector regression,Water quality},
number = {3},
pages = {1157--1182},
pmid = {21889629},
title = {{An Introduction to Variable and Feature Selection}},
volume = {3},
year = {2003}
}
@inproceedings{Kim2001,
author = {Kim, S and Park, S and Chu, W},
booktitle = {Proceedings of the 17th international conference on data engineering},
pages = {607--614},
title = {{An index-based approach for similarity search supporting time warping in large sequence databases}},
year = {2001}
}
@article{Ritter1975,
abstract = {A procedure is introduced to approximate nearest neighbor (INN) decision boundaries. The algorithm produces a selective subset of the original data so that 1) the subset is consistent, 2) the distance between any sample and its nearest selective neighbor is less than the distance from the sample to any sample of the other class, and 3) the subset is the smallest possible.},
author = {Ritter, G. and Woodruff, H. and Lowry, S. and Isenhour, T.},
issn = {0018-9448},
journal = {IEEE Transactions on Information Theory},
number = {May 1969},
title = {{An algorithm for a selective nearest neighbor decision rule (Corresp.)}},
volume = {21},
year = {1975}
}
@article{Moreno-Torres2012,
abstract = {The field of dataset shift has received a growing amount of interest in the last few years. The fact that most real-world applications have to cope with some form of shift makes its study highly relevant. The literature on the topic is mostly scattered, and different authors use different names to refer to the same concepts, or use the same name for different concepts. With this work, we attempt to present a unifying framework through the review and comparison of some of the most important works in the literature. {\textcopyright} 2011 Elsevier Ltd. All rights reserved.},
author = {Moreno-Torres, Jose G. and Raeder, Troy and Alaiz-Rodr{\'{i}}guez, Roc{\'{i}}o and Chawla, Nitesh V. and Herrera, Francisco},
doi = {10.1016/j.patcog.2011.06.019},
file = {:Users/Walter/Documents/Literature/dataset-shift.pdf:pdf},
isbn = {0031-3203},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Changing environments,Covariate shift,Data fracture,Dataset shift,Differing training and test populations,Non-stationary distributions,Sample selection bias},
number = {1},
pages = {521--530},
title = {{A unifying view on dataset shift in classification}},
volume = {45},
year = {2012}
}
@article{Bloom2012,
abstract = {ABSTRACT.The rate of image acquisition in modern synoptic imaging surveys has already begun to outpace the feasibility of keeping astronomers in the real-time discovery and classification loop. Here we present the inner workings of a framework, based on machine-learning algorithms, that captures expert training and ground-truth knowledge about the variable and transient sky to automate (1) the process of discovery on image differences, and (2) the generation of preliminary science-type classifications of discovered sources. Since follow-up resources for extracting novel science from fast-changing transients are precious, self-calibrating classification probabilities must be couched in terms of efficiencies for discovery and purity of the samples generated. We estimate the purity and efficiency in identifying real sources with a two-epoch image-difference discovery algorithm for the Palomar Transient Factory (PTF) survey. Once given a source discovery, using machine-learned classification trained on PTF da...},
archivePrefix = {arXiv},
arxivId = {1106.5491},
author = {Bloom, J. S. and Richards, J. W. and Nugent, P. E. and Quimby, R. M. and Kasliwal, M. M. and Starr, D. L. and Poznanski, D. and Ofek, E. O. and Cenko, S. B. and Butler, N. R. and Kulkarni, S. R. and Gal-Yam, A. and Law, N.},
doi = {10.1086/668468},
eprint = {1106.5491},
file = {:Users/Walter/Documents/Literature/Bloom{\_}2012{\_}PASP{\_}124{\_}1175.pdf:pdf},
issn = {00046280},
journal = {Publications of the Astronomical Society of the Pacific},
number = {921},
pages = {1175--1196},
title = {{Automating Discovery and Classification of Transients and Variable Stars in the Synoptic Survey Era}},
url = {http://iopscience.iop.org/article/10.1086/668468},
volume = {124},
year = {2012}
}
@article{Hendricks2016,
abstract = {Clearly explaining a rationale for a classification decision to an end-user can be as important as the decision itself. Existing approaches for deep visual recognition are generally opaque and do not output any justification text; contemporary vision-language models can describe image content but fail to take into account class-discriminative image aspects which justify visual predictions. We propose a new model that focuses on the discriminating properties of the visible object, jointly predicts a class label, and explains why the predicted label is appropriate for the image. We propose a novel loss function based on sampling and reinforcement learning that learns to generate sentences that realize a global sentence property, such as class specificity. Our results on a fine-grained bird species classification dataset show that our model is able to generate explanations which are not only consistent with an image but also more discriminative than descriptions produced by existing captioning methods.},
archivePrefix = {arXiv},
arxivId = {1603.08507},
author = {Hendricks, Lisa Anne and Akata, Zeynep and Rohrbach, Marcus and Donahue, Jeff and Schiele, Bernt and Darrell, Trevor},
doi = {10.1007/978-3-319-46493-0_1},
eprint = {1603.08507},
file = {:Users/Walter/Documents/Literature/1603.08507.pdf:pdf},
isbn = {9783319464923},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Image description,Language and vision,Visual explanation},
pages = {3--19},
pmid = {10463930},
title = {{Generating visual explanations}},
volume = {9908 LNCS},
year = {2016}
}
@article{C.K.Nebelecky2014,
author = {{C. K. Nebelecky}, J. L. Crassidis and P. Singla},
file = {:Users/Walter/Documents/Literature/f187df57aa9a4ae2026b748feac82ca664b3.pdf:pdf},
journal = {17th International Conference on, Salamanca},
pages = {1--8},
title = {{A model error formulation of the multiple model adaptive estimation algorithm}},
year = {2014}
}
@article{Nikolaev2014a,
abstract = {Applications of carbon nanotubes continue to advance, with substantial progress in nanotube electronics, conductive wires, and transparent conductors to name a few. However, wider application remains impeded by a lack of control over production of nanotubes with the desired purity, perfection, chirality, and number of walls. This is partly due to the fact that growth experiments are time-consuming, taking about 1 day per run, thus making it challenging to adequately explore the many parameters involved in growth. We endeavored to speed up the research process by automating CVD growth experimentation. The adaptive rapid experimentation and in situ spectroscopy CVD system described in this contribution conducts over 100 experiments in a single day, with automated control and in situ Raman characterization. Linear regression modeling was used to map regions of selectivity toward single-wall and multiwall carbon nanotube growth in the complex parameter space of the water-assisted CVD synthesis. This development of the automated rapid serial experimentation is a significant progress toward an autonomous closed-loop learning system: a Robot Scientist.},
author = {Nikolaev, Pavel and Hooper, Daylond and Perea-L{\'{o}}pez, Nestor and Terrones, Mauricio and Maruyama, Benji},
doi = {10.1021/nn503347a},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Nikolaev et al. - 2014 - Discovery of wall-selective carbon nanotube growth conditions via automated experimentation(2).pdf:pdf;:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Nikolaev et al. - 2014 - Discovery of wall-selective carbon nanotube growth conditions via automated experimentation.pdf:pdf},
issn = {1936-086X},
journal = {ACS nano},
keywords = {automatic,carbon nanotube synthesis,selectivity},
number = {10},
pages = {10214--22},
pmid = {25299482},
title = {{Discovery of wall-selective carbon nanotube growth conditions via automated experimentation.}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84908423977{\&}partnerID=tZOtx3y1},
volume = {8},
year = {2014}
}
@article{Bien2011,
abstract = {Prototype methods seek a minimal subset of samples that can serve as a distillation or condensed view of a data set. As the size of modern data sets grows, being able to present a domain specialist with a short list of “representative” samples chosen from the data set is of increasing interpretative value. While much recent statistical research has been focused on producing sparse-in-the-variables methods, this paper aims at achieving sparsity in the samples.$\backslash$n$\backslash$nWe discuss a method for selecting prototypes in the classification setting (in which the samples fall into known discrete categories). Our method of focus is derived from three basic properties that we believe a good prototype set should satisfy. This intuition is translated into a set cover optimization problem, which we solve approximately using standard approaches. While prototype selection is usually viewed as purely a means toward building an efficient classifier, in this paper we emphasize the inherent value of having a set of prototypical elements. That said, by using the nearest-neighbor rule on the set of prototypes, we can of course discuss our method as a classifier as well.$\backslash$n$\backslash$nWe demonstrate the interpretative value of producing prototypes on the well-known USPS ZIP code digits data set and show that as a classifier it performs reasonably well. We apply the method to a proteomics data set in which the samples are strings and therefore not naturally embedded in a vector space. Our method is compatible with any dissimilarity measure, making it amenable to situations in which using a non-Euclidean metric is desirable or even necessary.},
annote = {Interesting formulation of instance selection as an integer program (mine!)
Good intuition of what type of instances you want to keep
Perhaps could improve by displaying most similar and dissimilar instances...or something},
archivePrefix = {arXiv},
arxivId = {arXiv:1202.5933v1},
author = {Bien, Jacob and Tibshirani, Robert},
doi = {10.1214/11-AOAS495},
eprint = {arXiv:1202.5933v1},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Bien, Tibshirani - 2011 - Prototype selection for interpretable classification.pdf:pdf},
isbn = {1932-6157},
issn = {19326157},
journal = {Annals of Applied Statistics},
keywords = {Classification,Integer program,Nearest neighbors,Prototypes,Set cover},
number = {4},
pages = {2403--2424},
title = {{Prototype selection for interpretable classification}},
volume = {5},
year = {2011}
}
@article{Sakai2006,
abstract = {This paper presents a pedestrian detection system based on the fusion of sensors for LIDAR and convolutional neural network based image classification. By using LIDAR our method achieves a processing speed of over 10 frames/second. The focus of this paper is the evaluation of the effects of fusing the two systems compared to the image-only system. The evaluation results indicate that fusing the LIDAR and image classifier can reduce the number of false positives by a factor of 2 and reduce the processing time by a factor of 4. The single frame detection accuracy of the system is above 90{\%} when there is 1 false positive/s.},
author = {Sakai, Utsushi and Ogata, Jun},
file = {:Users/Walter/Documents/Literature/mate{\_}szarvas{\_}real{\_}time{\_}pedestrian{\_}detection{\_}using{\_}lidar{\_}and{\_}convolutional{\_}neural{\_}networks{\_}iv{\_}2006{\_}final.pdf:pdf},
journal = {Proceeding of the 13th World Congress on ITS in London},
keywords = {LIDAR,convolutional neural network},
title = {{Real-time pedestrian detection using LIDAR and convolutional neural networks}},
url = {http://trid.trb.org/view.aspx?id=846875},
year = {2006}
}
@article{Charikar2001,
abstract = {Facility location problems are traditionally investigated with the assumption that all the clients are to be provided ser-vice. A significant shortcoming of this formulation is that a few very distant clients, called outliers, can exert a dispro-portionately strong influence over the final solution. In this paper we explore a generalization of various facility loca-tion problems (K-center, K-median, uncapacitated facility location etc) to the case when only a specified fraction of the customers are to be served. What makes the problems harder is that we have to also select the subset that should get service. We provide generalizations of various approxi-mation algorithms to deal with this added constraint.},
annote = {This is potentially very powerful for us

Place centers anywhere},
author = {Charikar, Moses and Khuller, Samir and Mount, David M and Narasimhan, Giri},
file = {:Users/Walter/Documents/Literature/soda01-outlier.pdf:pdf},
isbn = {0-89871-490-7},
journal = {Proceedings of the Twelfth Annual Symposium on Discrete Algorithms},
pages = {642--651},
title = {{Algorithms for Facility Location Problems with Outliers}},
url = {https://www.cs.umd.edu/{~}mount/Papers/soda01-outlier.pdf{\%}0Ahttp://dl.acm.org/citation.cfm?id=365555},
year = {2001}
}
@book{Garcia2015,
abstract = {In this chapter, we consider instance selection as an important focusing task in the data reduction phase of knowledge discovery and data mining. First of all, we define a broader perspective on concepts and topics related with instance selection (Sect. 8.1). Due to the fact that instance selection has been distinguished over the years as two type of tasks, depending on the data mining method applied later, we clearly separate it into two processes: training set selection and prototype selection. Theses trends are explained in Sect. 8.2. Thereafter, and focusing on prototype selection, we present a unifying framework that covers existing properties obtaining as a result a complete taxonomy (Sect. 8.3). The description of the operation as the most well known and some recent instance and/or prototype selection methods are provided in Sect. 8.4. Advanced and recent approaches that incorporate novel solutions based of hybridizations with other types of data reduction techniques or similar solutions are collected in Sect. 8.5. Finally, we summarize example evaluation results for prototype selection in an exhaustive experimental comparative analysis in Sect. 8.6. {\textcopyright} Springer International Publishing Switzerland 2015.},
address = {Cham},
author = {Garc{\'{i}}a, Salvador and Luengo, Julian and Herrera, Francisco},
booktitle = {Intelligent Systems Reference Library},
isbn = {978-3-319-10246-7},
issn = {18684408},
pages = {195--243},
publisher = {Springer International Publishing},
series = {Intelligent Systems Reference Library},
title = {{Data Preprocessing in Data Mining}},
volume = {72},
year = {2015}
}
@inproceedings{Horwood2016,
author = {Horwood, J T and Aristoff, J M},
booktitle = {Proceedings of the 26tgh AAS/AIAA Space Flight Mechanics Meeting},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Horwood, Aristoff - 2016 - Aas 16-447 athena a data-driven anomaly detection and space object classification tool for ssa.pdf:pdf},
isbn = {9780877036333},
issn = {00653438},
number = {February},
pages = {1--20},
title = {{ATHENA: a Data-Driven Anomaly Detection and Space Object Classification Tool for SSA}},
volume = {2016},
year = {2016}
}
@inproceedings{Bennette2017,
abstract = {{\textcopyright} 2017 IEEE. Characterizing Resident Space Objects is necessary for the maintenance of Space Situational Awareness. Previous studies have shown that machine learning classifiers can be used to automatically characterize these objects using non-resolved photometric data in a simulated environment. In this study, we discuss the curation of a real dataset of Resident Space Objects, feature extraction for these objects, and show that a classification model can achieve adequate performance on real data.},
author = {Bennette, W.D. Walter D and Zeliff, Kayla and Raquepas, Joseph},
booktitle = {2017 IEEE Symposium Series on Computational Intelligence},
doi = {10.1109/SSCI.2017.8280966},
isbn = {9781538627259},
title = {{Classification of Objects in Geosynchronous Earth Orbit Via Light Curve Analysis}},
volume = {2018-Janua},
year = {2017}
}
@article{Elkan2003,
abstract = {We propose and evaluate a family of methods for converting classifier learning algorithms and classification theory into cost-sensitive algorithms and theory. The proposed conversion is based on cost-proportionate weighting of the training examples, which can be realized either by feeding the weights to the classification algorithm (as often done in boosting), or by careful subsampling. We give some theoretical performance guarantees on the proposed methods, as well as empirical evidence that they are practical alternatives to existing approaches. In particular, we propose costing, a method based on cost-proportionate rejection sampling and ensemble aggregation, which achieves excellent predictive performance on two publicly available datasets, while drastically reducing the computation required by other methods.},
author = {Elkan, Charles},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Elkan - 2003 - The Foundations of Cost-Sensitive Learning.pdf:pdf},
journal = {Third IEEE International Conference on Data Mining},
pages = {435--442},
title = {{The Foundations of Cost-Sensitive Learning}},
year = {2003}
}
@inproceedings{Endou2002,
abstract = {In machine learning, decision trees (DTs) are usually considered comprehensible because a reasoning process can be given for each conclusion. When the data set is large, however, the DTs obtained may become very large, and they are no longer comprehensible. To increase the comprehensibility of DTs, we have proposed several methods. For example, we have tried to evolve DTs using genetic programming (GP), with tree size as the secondary fitness measure; we have tried to initialize GP using results obtained by C4.5; and we have also tried to introduce the divide-and-conquer concept in GP, but all results obtained are still not good enough. Up to now we have tried to design good DTs from given fixed data. In this paper, we look at the problem from a different point of view. The basic idea is to evolve a small data set that can cover the domain knowledge as good as possible. From this data set, a small but good DT can be designed. The validity of the new algorithm is verified through several experiments},
author = {Endou, Taichirou and Zhao, Qiangfu},
booktitle = {Proceedings of the 2002 Congress on Evolutionary Computation},
isbn = {0780372824},
pages = {1221--1225},
title = {{Generation of Comprehensible Decision Trees Through Evolution of Training Data}},
year = {2002}
}
@article{Geifman2018,
abstract = {We consider the problem of uncertainty estimation in the context of (non-Bayesian) deep neural classification. All current methods are based on extracting uncertainty signals from a trained network optimized to solve the classification problem at hand. We demonstrate that such techniques tend to misestimate instances whose predictions are supposed to be highly confident. This deficiency is an artifact of the training process with SGD-like optimizers. Based on this observation, we develop an uncertainty estimation algorithm that "peels away" highly confident points sequentially and estimates their confidence using earlier snapshots of the trained model, before their uncertainty estimates are jittered. We present extensive experiments indicating that the proposed algorithm provides uncertainty estimates that are consistently better than the best known methods.},
archivePrefix = {arXiv},
arxivId = {1805.08206},
author = {Geifman, Yonatan and Uziel, Guy and El-Yaniv, Ran},
eprint = {1805.08206},
file = {:Users/Walter/Documents/Literature/1805.08206.pdf:pdf},
title = {{Boosting Uncertainty Estimation for Deep Neural Classifiers}},
url = {http://arxiv.org/abs/1805.08206},
year = {2018}
}
@article{Zheng2003,
abstract = {This paper presents a novel local featureselection approach for text categorization. Itconstructs a feature set for each category by firstselecting a set of terms highly indicative ofmembership as well as another set of termshighly indicative of non-membership, thenunifying the two sets. The size ratio of the twosets was empirically chosen to obtain optimalperformance. This is in contrast with thestandard local feature selection approaches thateither (1) only select the terms most indicative ofmembership; or (2) implicitly but not optimallycombine the terms most indicative ofmembership with non-membership. Theexperimental comparison between the proposedapproach and standard approaches wasconducted on four feature selection metrics: chi-square, correlation coefficient, odds ratio, andGSS coefficient. The results show that theproposed approach improves text categorizationperformance.},
author = {Zheng, Zhaohui and Srihari, Rohini},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Zheng, Srihari - 2003 - Optimally Combining Positive and Negative Features for Text Categorization.pdf:pdf},
journal = {In Proceedings of the ICML},
title = {{Optimally Combining Positive and Negative Features for Text Categorization}},
volume = {3},
year = {2003}
}
@article{Galar2012,
abstract = {Classifier learning with data-sets that suffer from imbalanced class distributions is a challenging problem in data mining community. This issue occurs when the number of examples that represent one class is much lower than the ones of the other classes. Its presence in many real-world applications has brought along a growth of attention from researchers. In machine learning, the ensemble of classifiers are known to increase the accuracy of single classifiers by combining several of them, but neither of these learning techniques alone solve the class imbalance problem, to deal with this issue the ensemble learning algorithms have to be designed specifically. In this paper, our aim is to review the state of the art on ensemble techniques in the framework of imbalanced data-sets, with focus on two-class problems. We propose a taxonomy for ensemble-based methods to address the class imbalance where each proposal can be categorized depending on the inner ensemble methodology in which it is based. In addition, we develop a thorough empirical comparison by the consideration of the most significant published approaches, within the families of the taxonomy proposed, to show whether any of them makes a difference. This comparison has shown the good behavior of the simplest approaches which combine random undersampling techniques with bagging or boosting ensembles. In addition, the positive synergy between sampling techniques and bagging has stood out. Furthermore, our results show empirically that ensemble-based algorithms are worthwhile since they outperform the mere use of preprocessing techniques before learning the classifier, therefore justifying the increase of complexity by means of a significant enhancement of the results.},
author = {Galar, Mikel and Fernandez, Alberto and Barrenechea, Edurne and Bustince, Humberto and Herrera, Francisco},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Galar et al. - 2012 - A review on ensembles for the class imbalance problem bagging-, boosting-, and hybrid-based approaches.pdf:pdf},
journal = {IEEE Transaction on Systems, Man, and Cybernetics, Part C},
number = {4},
pages = {463--484},
title = {{A review on ensembles for the class imbalance problem: bagging-, boosting-, and hybrid-based approaches.}},
volume = {42},
year = {2012}
}
@article{Ganin2018,
abstract = {Advances in deep generative networks have led to impressive results in recent years. Neverthe-less, such models can often waste their capacity on the minutiae of datasets, presumably due to weak inductive biases in their decoders. This is where graphics engines may come in handy since they abstract away low-level details and represent images as high-level programs. Current methods that combine deep learning and renderers are lim-ited by hand-crafted likelihood or distance func-tions, a need for large amounts of supervision, or difficulties in scaling their inference algorithms to richer datasets. To mitigate these issues, we present SPIRAL, an adversarially trained agent that generates a program which is executed by a graphics engine to interpret and sample images. The goal of this agent is to fool a discriminator network that distinguishes between real and ren-dered data, trained with a distributed reinforce-ment learning setup without any supervision. A surprising finding is that using the discrimina-tor's output as a reward signal is the key to allow the agent to make meaningful progress at match-ing the desired output rendering. To the best of our knowledge, this is the first demonstration of an end-to-end, unsupervised and adversarial in-verse graphics agent on challenging real world (MNIST, OMNIGLOT, CELEBA) and synthetic 3D datasets. A video of the agent can be found at https://youtu.be/iSyvwAwa7vk.},
author = {Ganin, Yaroslav and Kulkarni, Tejas and Babuschkin, Igor and {Ali Eslami}, S M and Vinyals, Oriol},
file = {:Users/Walter/Documents/Literature/SPIRAL.pdf:pdf},
title = {{Synthesizing Programs for Images using Reinforced Adversarial Learning}},
year = {2018}
}
@article{Schapire1998,
abstract = {One of the surprising recurring phenomena observed in experiments with boosting is that the test error of the generated classifier usually does not increase as its size becomes very large, and often is observed to decrease even after the training error reaches zero. In this paper, we show that this phenomenon is related to the distribution of margins of the training examples with respect to the generated voting classification rule, where the margin of an example is simply the difference between the number of correct votes and the maximum number of votes received by any incorrect label. We show that techniques used in the analysis of Vapnik's support vector classifiers and of neural networks with small weights can be applied to voting methods to relate the margin distribution to the test error. We also show theoretically and experimentally that boosting is especially effective at increasing the margins of the training examples. Finally, we compare our explanation to those based on the bias-variance decomposition.},
author = {Schapire, Robert E. and Freund, Yoav and Bartlett, Peter and Lee, Wee Sun},
doi = {10.1214/aos/1024691352},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Schapire et al. - 1998 - Boosting the margin A new explanation for the effectiveness of voting methods.pdf:pdf},
isbn = {1558604863},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Bagging,Boosting,Decision trees,Ensemble methods,Error-correcting,Markov chain,Monte Carlo,Neural networks,Output coding},
number = {5},
pages = {1651--1686},
title = {{Boosting the margin: A new explanation for the effectiveness of voting methods}},
volume = {26},
year = {1998}
}
@article{Carrizosa2013,
abstract = {Data mining techniques often ask for the resolution of optimization problems. Supervised classification, and, in particular, support vector machines, can be seen as a paradigmatic instance. In this paper, some links between mathematical optimization methods and supervised classification are emphasized. It is shown that many different areas of mathematical optimization play a central role in off-the-shelf supervised classification methods. Moreover, mathematical optimization turns out to be extremely useful to address important issues in classification, such as identifying relevant variables, improving the interpretability of classifiers or dealing with vagueness/noise in the data. ?? 2012 Elsevier Ltd. All rights reserved.},
author = {Carrizosa, Emilio and {Romero Morales}, Dolores},
issn = {03050548},
journal = {Computers and Operations Research},
keywords = {Cost efficiency,Data mining,Interpretability,Mathematical optimization,Support vector machines},
number = {1},
pages = {150--165},
publisher = {Elsevier},
title = {{Supervised classification and mathematical optimization}},
volume = {40},
year = {2013}
}
@article{Liu2016a,
abstract = {We propose coupled generative adversarial network (CoGAN) for learning a joint distribution of multi-domain images. In contrast to the existing approaches, which require tuples of corresponding images in different domains in the training set, CoGAN can learn a joint distribution without any tuple of corresponding images. It can learn a joint distribution with just samples drawn from the marginal distributions. This is achieved by enforcing a weight-sharing constraint that limits the network capacity and favors a joint distribution solution over a product of marginal distributions one. We apply CoGAN to several joint distribution learning tasks, including learning a joint distribution of color and depth images, and learning a joint distribution of face images with different attributes. For each task it successfully learns the joint distribution without any tuple of corresponding images. We also demonstrate its applications to domain adaptation and image transformation.},
archivePrefix = {arXiv},
arxivId = {1606.07536},
author = {Liu, Ming-Yu and Tuzel, Oncel},
doi = {arXiv:1606.07536},
eprint = {1606.07536},
file = {:Users/Walter/Documents/Literature/1606.07536.pdf:pdf},
isbn = {10495258},
number = {Nips},
title = {{Coupled Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1606.07536},
year = {2016}
}
@article{Oh2015,
abstract = {Motivated by vision-based reinforcement learning (RL) problems, in particular Atari games from the recent benchmark Aracade Learning Environment (ALE), we consider spatio-temporal prediction problems where future (image-)frames are dependent on control variables or actions as well as previous frames. While not composed of natural scenes, frames in Atari games are high-dimensional in size, can involve tens of objects with one or more objects being controlled by the actions directly and many other objects being influenced indirectly, can involve entry and departure of objects, and can involve deep partial observability. We propose and evaluate two deep neural network architectures that consist of encoding, action-conditional transformation, and decoding layers based on convolutional neural networks and recurrent neural networks. Experimental results show that the proposed architectures are able to generate visually-realistic frames that are also useful for control over approximately 100-step action-conditional futures in some games. To the best of our knowledge, this paper is the first to make and evaluate long-term predictions on high-dimensional video conditioned by control inputs.},
archivePrefix = {arXiv},
arxivId = {1507.08750},
author = {Oh, Junhyuk and Guo, Xiaoxiao and Lee, Honglak and Lewis, Richard and Singh, Satinder},
eprint = {1507.08750},
file = {:Users/Walter/Documents/Literature/1507.08750v2.pdf:pdf},
issn = {10495258},
journal = {Nips},
pages = {9},
title = {{Action-Conditional Video Prediction using Deep Networks in Atari Games}},
url = {http://arxiv.org/abs/1507.08750},
year = {2015}
}
@article{Duin2000,
author = {Duin, Robert P W and Tax, David M J},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Duin, Tax - 2000 - Classifier Combining Rules.pdf:pdf},
number = {15},
pages = {16--29},
title = {{Classifier Combining Rules}},
volume = {31},
year = {2000}
}
@article{Kononenko1991,
abstract = {In the past few years many systems for learning decision rules from examples were developed. As different systems allow different types of answers when classifying new instances, it is difficult to appropriately evaluate the systems' classification power in comparison with other classification systems or in comparison with human experts. Classification accuracy is usually used as a measure of classification performance. This measure is, however, known to have several defects. A fair evaluation criterion should exclude the influence of the class probabilities which may enable a completely uninformed classifier to trivially achieve high classification accuracy. In this paper a method for evaluating the information score of a classifier's answers is proposed. It excludes the influence of prior probabilities, deals with various types of imperfect or probabilistic answers and can be used also for comparing the performance in different domains.},
annote = {Assessing Accuracy
I think this is a very good measure! neat concept and informative

- good antectode for why new measures of accuracy
- good method outlined for assessing accuracy
- why ROC isn't always appropriate
- good reason for making an informative classifier},
author = {Kononenko, Igor and Bratko, Ivan},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Kononenko, Bratko - 1991 - Information-based evaluation criterion for classifier's performance.pdf:pdf},
isbn = {0885-6125},
issn = {08856125},
journal = {Machine Learning},
keywords = {Classifier,evaluation criteria,information theory,machine learning},
pages = {67--80},
title = {{Information-based evaluation criterion for classifier's performance}},
volume = {6},
year = {1991}
}
@article{Krzyzak1992,
author = {Krzyzak, L and C.Suen},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Krzyzak, C.Suen - 1992 - Methods of combining multiple classifiers and their application to handwriting recognition.pdf:pdf},
journal = {IEEE Transactions on Systems Man and Cybernetics},
number = {3},
pages = {418},
title = {{Methods of combining multiple classifiers and their application to handwriting recognition.}},
volume = {22},
year = {1992}
}
@article{Liu2002,
abstract = {The digital technologies and computer advances with the booming internet uses have led to massive data collection (corporate data, data warehouses, webs, just to name a few) and information (or misinformation) explosion. Szalay and Gray described this phenomenon as “drowning in data” (Szalay and Gray, 1999). They reported that each year the detectors at the CERN particle collider in Switzerland record 1 petabyte of data; and researchers in areas of science from astronomy to the human genome are facing the same problems and choking on information. A very natural question is “now that we have gathered so much data, what do we do with it?” Raw data is rarely of direct use and manual analysis simply cannot keep pace with the fast growth of data. Data mining and knowledge discovery (KDD), as a new emerging field comprising disciplines such as databases, statistics, machine learning, comes to the rescue. KDD attempts to turn raw data into nuggets and create special edges in this ever competitive world for science discovery and business intelligence. The KDD process is defined in Fayyad et al. (1996) as the nontrivial process of identifying valid, novel, potentially useful, and ultimately understandable patterns in data. Data Mining processes include data selection, preprocessing, data mining, interpretation and evaluation.},
author = {Liu, Huan and Motoda, Hiroshi},
doi = {10.1023/A:1014056429969},
issn = {13845810},
journal = {Data Mining and Knowledge Discovery},
number = {2},
pages = {115--130},
title = {{On issues of instance selection}},
volume = {6},
year = {2002}
}
@article{Giorgino2009,
abstract = {Dynamic time warping is a popular technique for comparing time series, providing both a distance measure that is insensitive to local compression and stretches and the warping which optimally deforms one of the two input series onto the other. A variety of algorithms and constraints have been discussed in the literature. The dtw package provides an unification of them; it allows R users to compute time series alignments mixing freely a variety of continuity constraints, restriction windows, endpoints, local distance definitions, and so on. The package also provides functions for visualizing alignments and constraints using several classic diagram types.},
author = {Giorgino, Toni},
doi = {10.18637/jss.v031.i07},
isbn = {1548-7660},
issn = {1548-7660},
journal = {Journal of Statistical Software},
keywords = {alignment,dynamic programming,dynamic time warping,timeseries},
number = {7},
pages = {1--24},
title = {{Computing and Visualizing Dynamic Time Warping Alignments in R : The dtw Package}},
url = {http://www.jstatsoft.org/v31/i07{\%}5Cnhttp://www.jstatsoft.org/v31/i07/},
volume = {31},
year = {2009}
}
@inproceedings{Chen2016,
abstract = {This paper describes InfoGAN, an information-theoretic extension to the Generative Adversarial Network that is able to learn disentangled representations in a completely unsupervised manner. InfoGAN is a generative adversarial network that also maximizes the mutual information between a small subset of the latent variables and the observation. We derive a lower bound to the mutual information objective that can be optimized efficiently, and show that our training procedure can be interpreted as a variation of the Wake-Sleep algorithm. Specifically, InfoGAN successfully disentangles writing styles from digit shapes on the MNIST dataset, pose from lighting of 3D rendered images, and background digits from the central digit on the SVHN dataset. It also discovers visual concepts that include hair styles, presence/absence of eyeglasses, and emotions on the CelebA face dataset. Experiments show that InfoGAN learns interpretable representations that are competitive with representations learned by existing fully supervised methods.},
archivePrefix = {arXiv},
arxivId = {1606.03657},
author = {Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
booktitle = {Advances in Neural Information Processing Systems},
doi = {10.1007/978-3-319-16817-3},
eprint = {1606.03657},
file = {:Users/Walter/Documents/Literature/1606.03657.pdf:pdf},
isbn = {978-3-319-16816-6},
issn = {978-3-319-16807-4},
pmid = {23459267},
title = {{InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets}},
url = {http://arxiv.org/abs/1606.03657},
year = {2016}
}
@article{Hu2015,
abstract = {BACKGROUND: The size of medical datasets is usually very large, which directly affects the computational cost of the data mining process. Instance selection is a data preprocessing step in the knowledge discovery process, which can be employed to reduce storage requirements while also maintaining the mining quality. This process aims to filter out outliers (or noisy data) from a given (training) dataset. However, when the dataset is very large in size, more time is required to accomplish the instance selection task. OBJECTIVE: In this paper, we introduce an efficient data preprocessing approach (EDP), which is composed of two steps. The first step is based on training a model over a small amount of training data after preforming instance selection. The model is then used to identify the rest of the large amount of training data. METHODS: Experiments are conducted based on two medical datasets for breast cancer and protein homology prediction problems that contain over 100000 data samples. In addition, three well-known instance selection algorithms are used, IB3, DROP3, and genetic algorithms. On the other hand, three popular classification techniques are used to construct the learning models for comparison, namely the CART decision tree, k-nearest neighbor (k-NN), and support vector machine (SVM). RESULTS: The results show that our proposed approach not only reduces the computational cost by nearly a factor of two or three over three other state-of-the-art algorithms, but also maintains the final classification accuracy. CONCLUSIONS: To perform instance selection over large scale medical datasets, it requires a large computational cost to directly execute existing instance selection algorithms. Our proposed EDP approach solves this problem by training a learning model to recognize good and noisy data. To consider both computational complexity and final classification accuracy, the proposed EDP has been demonstrated its efficiency and effectiveness in the large scale instance selection problem.},
annote = {not necessarily text, still must read},
author = {Hu, Ya Han and Lin, Wei Chao and Tsai, Chih Fong and Ke, Shih Wen and Chen, Chih Wen},
doi = {10.3233/THC-140887},
file = {:Users/Walter/Documents/Literature/ContentServer.pdf:pdf},
issn = {09287329},
journal = {Technology and Health Care},
keywords = {Data preprocessing,breast cancer,instance selection,medical data mining,protein homology},
number = {2},
pages = {153--160},
title = {{An efficient data preprocessing approach for large scale medical data mining}},
volume = {23},
year = {2015}
}
@article{Riquelme2002,
abstract = {This paper presents a new approach to finding representative patterns for dataset editing. The algorithm patterns by ordered projections (POP), has some interesting characteristics: Important reduction of the number of instances from the dataset; lower computational cost (??(mn log n)) with respect to other typical algorithms due to the absence of distance calculations; conservation of the decision boundaries, especially from the point of view of the application of axis-parallel classifiers. POP works well in practice with both continuous and discrete attributes. The performance of POP is analysed in two ways: Percentage of reduction and classification. POP has been compared to IB2, ENN and SHRINK concerning the percentage of reduction and the computational cost. In addition, we have analysed the accuracy of k-NN and C4.5 after applying the reduction techniques. An extensive empirical study using datasets with continuous and discrete attributes from the UCI repository shows that POP is a valuable preprocessing method for the later application of any axis-parallel learning algorithm. ?? 2002 Pattern Recognition Society. Published by Elsevier Science Ltd. All rights reserved.},
author = {Riquelme, Jos?? C. and Aguilar-Ruiz, Jes??s S. and Toro, Miguel},
doi = {10.1016/S0031-3203(02)00119-X},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Axis-parallel classifiers,Data mining,Pattern analysis,Preprocessing techniques},
number = {4},
pages = {1009--1018},
title = {{Finding representative patterns with ordered projections}},
volume = {36},
year = {2002}
}
@article{Ribeiro2016a,
abstract = {At the core of interpretable machine learning is the question of whether humans are able to make accurate predictions about a model's behavior. Assumed in this question are three properties of the interpretable output: coverage, precision, and effort. Coverage refers to how often humans think they can predict the model's behavior, precision to how accurate humans are in those predictions, and effort is either the up-front effort required in interpreting the model, or the effort required to make predictions about a model's behavior. In this work, we propose anchor-LIME (aLIME), a model-agnostic technique that produces high-precision rule-based explanations for which the coverage boundaries are very clear. We compare aLIME to linear LIME with simulated experiments, and demonstrate the flexibility of aLIME with qualitative examples from a variety of domains and tasks.},
annote = {Neat idea, concept of anchors},
archivePrefix = {arXiv},
arxivId = {1611.05817},
author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
eprint = {1611.05817},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Ribeiro, Singh, Guestrin - 2016 - Nothing Else Matters Model-Agnostic Explanations By Identifying Prediction Invariance.pdf:pdf},
number = {Nips},
title = {{Nothing Else Matters: Model-Agnostic Explanations By Identifying Prediction Invariance}},
url = {http://arxiv.org/abs/1611.05817},
year = {2016}
}
@article{Wang2015,
abstract = {We consider learning representations (features) in the setting in which we have access to mul-tiple unlabeled views of the data for representa-tion learning while only one view is available at test time. Previous work on this problem has pro-posed several techniques based on deep neural networks, typically involving either autoencoder-like networks with a reconstruction objective or paired feedforward networks with a correlation-based objective. We analyze several techniques based on prior work, as well as new variants, and compare them experimentally on visual, speech, and language domains. To our knowledge this is the first head-to-head comparison of a vari-ety of such techniques on multiple tasks. We find an advantage for correlation-based represen-tation learning, while the best results on most tasks are obtained with our new variant, deep canonically correlated autoencoders (DCCAE).},
author = {Wang, W and Arora, R and Livescu, K and Bilmes, J},
file = {:Users/Walter/Documents/Literature/wangb15.pdf:pdf},
isbn = {9781510810587},
journal = {32st Int. Conf. Machine Learning ( {\ldots}},
keywords = {multi-view learning, representation learning, deep},
title = {{On deep multi-view representation learning}},
url = {http://www.jmlr.org/proceedings/papers/v37/wangb15.pdf},
volume = {37},
year = {2015}
}
@article{LeCun1998,
abstract = {Multilayer neural networks trained with the back-propagation
algorithm constitute the best example of a successful gradient based
learning technique. Given an appropriate network architecture,
gradient-based learning algorithms can be used to synthesize a complex
decision surface that can classify high-dimensional patterns, such as
handwritten characters, with minimal preprocessing. This paper reviews
various methods applied to handwritten character recognition and
compares them on a standard handwritten digit recognition task.
Convolutional neural networks, which are specifically designed to deal
with the variability of 2D shapes, are shown to outperform all other
techniques. Real-life document recognition systems are composed of
multiple modules including field extraction, segmentation recognition,
and language modeling. A new learning paradigm, called graph transformer
networks (GTN), allows such multimodule systems to be trained globally
using gradient-based methods so as to minimize an overall performance
measure. Two systems for online handwriting recognition are described.
Experiments demonstrate the advantage of global training, and the
flexibility of graph transformer networks. A graph transformer network
for reading a bank cheque is also described. It uses convolutional
neural network character recognizers combined with global training
techniques to provide record accuracy on business and personal cheques.
It is deployed commercially and reads several million cheques per day
},
archivePrefix = {arXiv},
arxivId = {1102.0183},
author = {LeCun, Yann and Bottou, L{\'{e}}on and Bengio, Yoshua and Haffner, Patrick},
doi = {10.1109/5.726791},
eprint = {1102.0183},
isbn = {0018-9219},
issn = {00189219},
journal = {Proceedings of the IEEE},
keywords = {Convolutional neural networks,Document recognition,Finite state transducers,Gradient-based learning,Graph transformer networks,Machine learning,Neural networks,Optical character recognition (OCR)},
number = {11},
pages = {2278--2323},
pmid = {15823584},
title = {{Gradient-based learning applied to document recognition}},
volume = {86},
year = {1998}
}
@misc{Linares2016,
address = {Heidelberg, Germany},
annote = {Great, they show that they can determine what an object is based on simulated data. The simulation hopefully doesn't generate identical data for different satelites?},
archivePrefix = {arXiv},
arxivId = {1608.04369},
author = {Linares, Richard and Furfaro, Roberto},
booktitle = {19th International Conference on Information Fusion},
doi = {10.1093/mnras/stw2672},
eprint = {1608.04369},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Linares, Furfaro - 2016 - Space Object Classification Using Deep Convolutional Neural Networks.pdf:pdf},
isbn = {9780996452748},
title = {{Space Object Classification Using Deep Convolutional Neural Networks}},
year = {2016}
}
@article{Jaakkola1999,
abstract = {Generative probability models such as hidden Markov models provide$\backslash$na principled way of treating missing information and dealing with$\backslash$nvariable length sequences. On the other hand, discriminative $\backslash$n$\backslash$nmethods such as support vector machines enable us to construct flexible$\backslash$ndecision boundaries and often result in classification performance$\backslash$nsuperior to that of the model based approaches. An ideal classifier$\backslash$nshould combine these two complementary approaches. In this paper,$\backslash$nwe develop a natural way of achieving this combination by deriving$\backslash$nkernel functions for use in discriminative methods such as support$\backslash$nvector machines from generative probability models. We provide a$\backslash$ntheoretical justication for this combination as well as demonstrate$\backslash$na substantial improvement in the classifcation performance in the$\backslash$ncontext of DNA and protein sequence analysis.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Jaakkola, T.S. and Haussler, D.},
doi = {10.1038/217994a0},
eprint = {arXiv:1011.1669v3},
file = {:Users/Walter/Documents/Literature/1520-exploiting-generative-models-in-discriminative-classifiers.pdf:pdf},
isbn = {0262112450},
issn = {10495258},
journal = {Advances in neural information processing systems},
pages = {487--493},
pmid = {15003161},
title = {{Exploiting generative models in discriminative classifiers}},
year = {1999}
}
@article{Hall2009,
author = {Hall, Mark and Holmes, Geoffrey and Pfahringer, Bernhard and Reutemann, Peter and Witten, Ian H.},
journal = {Sigkdd Explorations},
number = {1},
title = {{The WEKA Data Mining Software: An Update}},
volume = {11},
year = {2009}
}
@article{Shao2014,
abstract = {Regular machine learning and data mining techniques study the training data for future inferences under a major assumption that the future data are within the same feature space or have the same distribution as the training data. However, due to the limited availability of human labeled training data, training data that stay in the same feature space or have the same distribution as the future data cannot be guaranteed to be sufficient enough to avoid the over-fitting problem. In real-world applications, apart from data in the target domain, related data in a different domain can also be included to expand the availability of our prior knowledge about the target future data. Transfer learning addresses such cross-domain learning problems by extracting useful information from data in a related domain and transferring them for being used in target tasks. In recent years, with transfer learning being applied to visual categorization, some typical problems, e.g., view divergence in action recognition tasks and concept drifting in image classification tasks, can be efficiently solved. In this paper, we survey state-of-the-art transfer learning algorithms in visual categorization applications, such as object recognition, image classification, and human action recognition.},
author = {Shao, L and Zhu, F and Li, X},
doi = {10.1109/TNNLS.2014.2330900},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Shao, Zhu, Li - 2014 - Transfer Learning for Visual Categorization A Survey.pdf:pdf},
isbn = {2162-237X},
issn = {2162237X},
journal = {Tnnls},
keywords = {Action recognition,Adaptation models,Knowledge transfer,Learning systems,Testing,Training,Training data,Visualization,image classification,machine learning,object recognition,survey,transfer learning,visual categorization.},
number = {99},
pages = {1},
pmid = {25014970},
title = {{Transfer Learning for Visual Categorization: A Survey}},
volume = {PP},
year = {2014}
}
@inproceedings{Chatterjee2016,
abstract = {We extend the traditional worst-case, minimax analysis of stochastic convex optimization by introducing a localized form of minimax complexity for individual functions. Our main result gives function-specific lower and upper bounds on the number of stochastic subgradient evaluations needed to optimize either the function or its hardest local alternative'' to a given numerical precision. The bounds are expressed in terms of a localized and computational analogue of the modulus of continuity that is central to statistical minimax analysis. We show how the computational modulus of continuity can be explicitly calculated in concrete cases, and relates to the curvature of the function at the optimum. We also prove a superefficiency result that demonstrates it is a meaningful benchmark, acting as a computational analogue of the Fisher information in statistical estimation. The nature and practical implications of the results are demonstrated in simulations.},
archivePrefix = {arXiv},
arxivId = {1605.07596},
author = {sabyasachi Chatterjee and Duchi, John C. and Lafferty, John and Zhu, Yuancheng},
booktitle = {NIPS},
eprint = {1605.07596},
file = {:Users/Walter/Documents/Literature/6601-local-minimax-complexity-of-stochastic-convex-optimization.pdf:pdf},
number = {Nips},
pages = {3423--3431},
title = {{Local Minimax Complexity of Stochastic Convex Optimization}},
year = {2016}
}
@phdthesis{Mosely2013,
author = {Mosely, Lawrence},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Mosely - 2013 - A balanced approach to the multi-class imbalance problem.pdf:pdf},
title = {{A balanced approach to the multi-class imbalance problem}},
year = {2013}
}
@incollection{Desrosiers1993,
author = {Desrosiers, Jacques},
booktitle = {Time},
publisher = {Springer},
title = {{Chapter 1 A PRIMER IN COLUMN GENERATION}},
volume = {3},
year = {2005}
}
@phdthesis{Choularton2009,
author = {{Stephen Choularton}},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Stephen Choularton - 2009 - Early Stage Detection of Speech Recognition Errors.pdf:pdf},
school = {Macquarie University},
title = {{Early Stage Detection of Speech Recognition Errors}},
year = {2009}
}
@article{Bucher1988,
abstract = {An iterative Monte-Carlo simulation procedure for structural analysis is suggested This proposed new approach utilizes results from simulation to adapt the importance sampling density to the specific problem. Considerable reduction of the statistical error of the estimated failure probability is achieved. Most important, problems connected with optimization procedures commonly used in structural reliability are avoided. This makes the suggested procedure especially attractive for systems reliability analyses.},
author = {Bucher, Christian G.},
doi = {10.1016/0167-4730(88)90020-3},
isbn = {0167-4730},
issn = {01674730},
journal = {Structural Safety},
number = {2},
pages = {119--126},
title = {{Adaptive sampling — an iterative fast Monte Carlo procedure}},
volume = {5},
year = {1988}
}
@article{Ezawa1996,
abstract = {This paper discusses issues related to Bayesiannetwork model learning for unbalanced binaryclassification tasks. In general, the primary focusof current research on Bayesian network learningsystems (e.g., K2 and its variants) is on thecreation of the Bayesian network structure that fitsthe database best. It turns out that when appliedwith a specific purpose in mind, such asclassification, the performance of these networkmodels may be very poor. We demonstrate thatBayesian network models should be created tomeet the specific goal or purpose intended for themodel.We first present a goal-oriented algorithm forconstructing Bayesian networks for predictinguncollectibles in telecommunications risk-management datasets. Second, we argue anddemonstrate that current Bayesian networklearning methods may fail to perform satisfactorilyin real life applications since they do not learnmodels tailored to a specific goal or purpose.Third, we discuss the performance of “goaloriented” K2 and its variant.},
annote = {Helpful because it makes bayseian networks goal oriented and discusses tailoring model to task

- large dataset with a small minority class
- goal orient by the selection of the bayseian structure

References to consider:

Cowell et. al. 1993 - want to tailor classifier task},
author = {Ezawa, K and Singh, M and Norton, S},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Ezawa, Singh, Norton - 1996 - Learning Goal-Oriented Bayesian Networks for Telecommunications Risk Management.pdf:pdf},
journal = {13th International Conference on Machine Learning},
pages = {139--147},
title = {{Learning Goal-Oriented Bayesian Networks for Telecommunications Risk Management}},
year = {1996}
}
@article{Matthews1975a,
annote = {Matthew's Correlation Coefficient},
author = {Matthews, Brian},
journal = {Biochimica et Biophysica Acta (BBA)-Protein Structure},
number = {2},
pages = {442--451},
title = {{Comparison of the predicted and observed secondary structure of T4 phage lysozyme}},
volume = {405},
year = {1975}
}
@article{Pazzani1994,
abstract = {We explore algorithms for learning classification procedures that attempt to minimize the cost of misclassifying examples. First, we consider inductive learning of classification rules. The Reduced Cost Ordering algorithm, a new method for creating a decision list (i.e., an ordered set of rules) is described and compared to a variety of inductive learning approaches. Next, we describe approaches that attempt to minimize costs while avoiding overfitting, and introduce the Clause Prefix method for pruning decision lists. Finally, we consider reducing misclassification costs when a prior domain theory is available.},
annote = {Cost sensitive learning introduction

- misclassification COST not number of misclassifications
- cost sensitive decision tree called I-gain
- only slightly mentions weighting instances},
author = {Pazzani, Michael and Merz, Christoper and Murphy, Patrick and Ali, Kamal and Hume, Timothy and Brunk, Clifford},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Pazzani et al. - 1994 - Reducing Misclassification Costs.pdf:pdf},
journal = {Icml},
pages = {217--225},
title = {{Reducing Misclassification Costs.}},
url = {http://pdf.aminer.org/000/335/445/reducing{\_}misclassification{\_}costs.pdf},
year = {1994}
}
@article{Provost1995,
annote = {Not helpful},
author = {Provost, Foster John and Buchanan, Bruce G.},
issn = {08856125},
journal = {Machine Learning},
keywords = {bias selection,inductive bias,inductive learning,inductive policy,pragmatics},
pages = {35--61},
title = {{Inductive policy: The pragmatics of bias selection}},
volume = {20},
year = {1995}
}
@inproceedings{Bennette2016,
author = {Bennette, Walter D},
booktitle = {The First Workshop of Mission-Critical Big Data Analytics},
title = {{Classifier Inspired Scaling for Training Set Selection}},
year = {2016}
}
@article{Friedman2003,
abstract = {Learning a function of many arguments is viewed from the perspective of high–dimensional numerical quadrature. It is shown that many of the popular ensemble learning procedures can be cast in this framework. In particular randomized methods, including bagging and random forests, are seen to correspond to random Monte Carlo integration methods each based on particular importance sampling strategies. Non random boosting methods are seen to correspond to deterministic quasi Monte Carlo integration techniques. This view helps explain some of their properties and suggests modifications to them that can substantially improve their accuracy while dramatically improving computational performance.},
author = {Friedman, Jerome H and Popescu, Bogdan E},
doi = {10.1109/LPT.2009.2020494},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Friedman, Popescu - 2003 - Importance Sampled Learning Ensembles.pdf:pdf},
issn = {1041-1135},
journal = {Computing},
keywords = {adaboost,and phrases,bagging,boosting,dient boosting,gra-,learning ensembles,mart,random forests},
number = {2},
pages = {1--32},
title = {{Importance Sampled Learning Ensembles}},
volume = {94305},
year = {2003}
}
@article{Sung1995,
abstract = {This paper presents an example-based learning approach for locating vertical frontal views of human faces in complex scenes. The technique models the distribution of human face patterns by means of a few view-based “face” and “non-face” prototype clusters. A 2-Value metric is proposed for computing distance features between test patterns and the distribution-based face model during classification. We show empirically that the prototypes we choose for our distribution-based model, and the metric we adopt for computing distance feature vectors, are both critical for the success of our system},
annote = {Largely unhelpful but sweet star trek example
Windowing and bootstrapping imbalanced},
author = {Sung, K K and Sung, K K and Poggio, T and Poggio, T},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Sung et al. - 1995 - Learning human face detection in cluttered scenes.pdf:pdf},
journal = {Springer Berlin / Heidelberg},
number = {1},
pages = {432 439},
title = {{Learning human face detection in cluttered scenes}},
url = {http://www.springerlink.com/index/p3337m2w73962837.pdf},
volume = {970},
year = {1995}
}
@book{Witten2005,
author = {Witten, Ian H and Frank, Eibe},
publisher = {Morgan Kaufmann},
title = {{Data Mining: Practical machine learning tools and techniques}},
year = {2005}
}
@article{Zadrozny2003,
abstract = {In many data mining domains, misclassification costs are different for different examples, in the same way that class membership probabilities are example-dependent. In these domains, both costs and probabilities are unknown for test examples, so both cost estimators and probability estimators must be learned. After discussing how to make optimal decisions given cost and probability estimates, we present decision tree and naive Bayesian learning methods for obtaining well-calibrated probability estimates. We then explain how to obtain unbiased estimators for example-dependent costs, taking into account the difficulty that in general, probabilities and costs are not independent random variables, and the training examples for which costs are known are not representative of all examples. The latter problem is called sample selection bias in econometrics. Our solution to it is based on Nobel prize-winning work due to the economist James Heckman. We show that the methods we propose perform better than MetaCost and all other known methods, in a comprehensive experimental comparison that uses the well-known, large, and challenging dataset from the KDD'98 data mining contest.},
author = {Zadrozny, Bianca and Elkan, Charles},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Zadrozny, Elkan - 2003 - Learning and Making Decisions When Costs and Probabilities are Both Unknown.pdf:pdf},
journal = {Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining},
pages = {204--213},
title = {{Learning and Making Decisions When Costs and Probabilities are Both Unknown}},
year = {2003}
}
@article{Tsai2013,
abstract = {Automatic text classification is usually based on models constructed through learning from training examples. However, as the size of text document repositories grows rapidly, the storage requirements and computational cost of model learning is becoming ever higher. Instance selection is one solution to overcoming this limitation. The aim is to reduce the amount of data by filtering out noisy data from a given training dataset. A number of instance selection algorithms have been proposed in the literature, such as ENN, IB3, ICF, and DROP3. However, all of these methods have been developed for the k-nearest neighbor (k-NN) classifier. In addition, their performance has not been examined over the text classification domain where the dimensionality of the dataset is usually very high. The support vector machines (SVM) are core text classification techniques. In this study, a novel instance selection method, called Support Vector Oriented Instance Selection (SVOIS), is proposed. First of all, a regression plane in the original feature space is identified by utilizing a threshold distance between the given training instances and their class centers. Then, another threshold distance, between the identified data (forming the regression plane) and the regression plane, is used to decide on the support vectors for the selected instances. The experimental results based on the TechTC-100 dataset show the superior performance of SVOIS over other state-of-the-art algorithms. In particular, using SVOIS to select text documents allows the k-NN and SVM classifiers perform better than without instance selection. ?? 2013 Elsevier Ltd. All rights reserved.},
annote = {- slow execution to classify?
- lots to store of model?
- what about a wrapper with a different faster model?

give parameters for method

- TWO PAPERS I MUST READ

- I need to implemnt ICF and ID3},
author = {Tsai, Chih Fong and Chang, Che Wei},
doi = {10.1016/j.is.2013.05.001},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Tsai, Chang - 2013 - SVOIS Support vector oriented instance selection for text classification.pdf:pdf},
isbn = {0306-4379},
issn = {03064379},
journal = {Information Systems},
keywords = {Data reduction,Instance selection,Machine learning,Support vector machines,Text classification},
number = {8},
pages = {1070--1083},
publisher = {Elsevier},
title = {{SVOIS: Support vector oriented instance selection for text classification}},
url = {http://dx.doi.org/10.1016/j.is.2013.05.001},
volume = {38},
year = {2013}
}
@article{Hagel2014,
author = {Hagel, Kayla Von and Joglekar, Shreyas and Pankow, Mark and Ferguson, Scott M},
doi = {doi:10.2514/6.2014-3161},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Hagel et al. - 2014 - Exploring Composite Panels Using Multi-Objective Optimization and Varying Load Conditions.pdf:pdf},
journal = {15th AIAA/ISSMO Multidisciplinary Analysis and Optimization Conference},
pages = {1--17},
title = {{Exploring Composite Panels Using Multi-Objective Optimization and Varying Load Conditions}},
url = {http://dx.doi.org/10.2514/6.2014-3161},
year = {2014}
}
@article{Kim2014,
abstract = {We present the Bayesian Case Model (BCM), a general framework for Bayesian case-based reasoning (CBR) and prototype classification and clustering. BCM brings the intuitive power of CBR to a Bayesian generative framework. The BCM learns prototypes, the “quintessential” observations that best represent clusters in a dataset, by performing joint inference on cluster labels, prototypes and important features. Simultaneously, BCM pursues sparsity by learning subspaces, the sets of features that play important roles in the characterization of the prototypes. The prototype and subspace representation provides quantitative benefits in interpretability while preserving classification accuracy. Human subject experiments verify statistically significant improvements to participants' understanding when using explanations produced by BCM, compared to those given by prior art.},
archivePrefix = {arXiv},
arxivId = {1503.01161},
author = {Kim, Been and Rudin, Cynthia and Shah, Julie},
eprint = {1503.01161},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Kim, Rudin, Shah - 2014 - The Bayesian Case Model A Generative Approach for Case-Based Reasoning and Prototype Classification.pdf:pdf},
issn = {10495258},
journal = {Neural Information Processing Systems},
pages = {1--9},
title = {{The Bayesian Case Model: A Generative Approach for Case-Based Reasoning and Prototype Classification}},
year = {2014}
}
@article{Antonelli2012,
abstract = {When dealing with datasets that are characterized by a large number of instances, multiobjective evolutionary learning (MOEL) of fuzzy rule-based systems (FRBSs) suffers from high computational costs, mainly because of the fitness evaluation. The use of a reduced set of representative instances in place of the overall training set (TS) would considerably lessen the computational effort. Even though a large number of papers have proposed instance selection approaches, mainly in classification problems, how this selection should be performed, especially in the context of regression, is still an open issue. In this paper, we tackle the instance selection problem in the framework of MOEL of FRBSs through a coevolutionary approach. In the execution of the MOEL, periodically, a single-objective genetic algorithm (SOGA) evolves a population of reduced TSs. The SOGA aims to maximize a purposely defined index which measures how much the Pareto fronts computed by using, respectively, the reduced TS and the overall TS are close to each other: The closer the fronts, the more the reduced TS is representative of the overall TS. During the execution of the MOEL, the rule base and the membership function parameters of the fuzzy sets are concurrently learned by maximizing the accuracy and minimizing the complexity. We tested our approach on 12 large datasets. We adopted reduced TSs composed of 5{\%}, 10{\%}, and 20{\%} of the overall TS. Using nonparametric statistical tests, we verified that with 10{\%} and 20{\%} of the overall TS, the Pareto front approximations that are generated by our coevolutionary approach are comparable with the ones generated by applying the MOEL with the overall TS, although the coevolution allows us to save up to 86.36{\%} of the execution time. In addition, the analysis of the behavior of three representative solutions on the test set highlights that the use of the reduced TSs does not affect the generalization capabilities of the generated FRBSs. {\textcopyright} 2012 IEEE.},
author = {Antonelli, Michela and Ducange, Pietro and Marcelloni, Francesco},
doi = {10.1109/TFUZZ.2011.2173582},
issn = {10636706},
journal = {IEEE Transactions on Fuzzy Systems},
keywords = {Large datasets,multiobjective evolutionary fuzzy systems (MOEFS),regression problems,training set selection},
number = {2},
pages = {276--290},
title = {{Genetic training instance selection in multiobjective evolutionary fuzzy systems: A coevolutionary approach}},
volume = {20},
year = {2012}
}
@article{Gu2008,
abstract = {The majority of machine learning algorithms previously designed usually assume that their training sets are well-balanced, and implicitly assume that all misclassification errors cost equally. But data in real-world is usually imbalanced. The class imbalance problem is pervasive and ubiquitous, causing trouble to a large segment of the data mining community. The tradition machine learning algorithms have bad performance when they learn from imbalanced data sets. Thus, machine learning on imbalanced data sets becomes an urgent problem. The importance of imbalanced data sets and their broad application domains in data mining are introduced, and then methods to deal with the class imbalance problem are discussed and their effectiveness are compared. Last but not least, the existing evaluation measures of class imbalance problem are systematically analyzed.},
author = {Gu, Qiong Gu Qiong and Cai, Zhihua Cai Zhihua and Zhu, Li Zhu Li and Huang, Bo Huang Bo},
isbn = {978-0-7695-3489-3},
journal = {2008 International Conference on Advanced Computer Theory and Engineering},
keywords = {Cost-Sensitive Learning,Data Mining,Imbalanced data sets,over-sampling,under-sampling},
pages = {1020--1024},
title = {{Data Mining on Imbalanced Data Sets}},
year = {2008}
}
@article{Silva2016,
abstract = {Given the growing amount of data produced from within different areas of knowledge, data mining methods currently have to face challenging datasets with greater numbers of instances and attributes. However, the processing capacity of data mining algorithms is struggling under this growth. One alternative for tackling the problem is to perform instance selection on the data in order to reduce its size, as a preprocessing step for data mining algorithms. This study presents e-MGD, a method for instance selection as an extension of the Markov Geometric Diffusion method, which is a linear complexity method used in computergraphics for the simplification of triangular meshes. The original method was extended so that it was capable of reducing datasets commonly found in the field of data mining. For this purpose, two essential points of adjustment were required. Firstly, it was necessary to build a geometric structure from the data and secondly, to adjust the method so that it could deal with types of attributes encountered within these datasets. These adjustments however, did not influence the complexity of the final e-MGD, since it remained linear, which enabled it to be applied to datasets with a greater number of instances and features. One distinct characteristic of the proposed extension is that it focuses on preserving dataset information rather than improving classification accuracy, as in the case of most instance selection methods. In order to assess the performance of the method, we compared it with a number of classical and contemporary instance selection methods using medium to large datasets, plus a further set of very large datasets. The results demonstrated a good performance in terms of classification accuracy when compared to results from other methods, indicating that the e-MGD is a good alternative for instance selection.},
author = {Silva, Du{\'{i}}lio A N S and Souza, Leandro C. and Motta, Gustavo H M B},
doi = {10.1016/j.datak.2015.11.002},
issn = {0169023X},
journal = {Data and Knowledge Engineering},
keywords = {Data mining,Instance selection,Large datasets,Markov geometric diffusion},
pages = {24--41},
publisher = {Elsevier B.V.},
title = {{An instance selection method for large datasets based on Markov Geometric Diffusion}},
url = {http://dx.doi.org/10.1016/j.datak.2015.11.002},
volume = {101},
year = {2016}
}
@incollection{reeves2001using,
author = {Reeves, Colin R and Bush, Daniel R},
booktitle = {Instance selection and construction for data mining},
pages = {339--356},
publisher = {Springer},
title = {{Using genetic algorithms for training data selection in RBF networks}},
year = {2001}
}
@article{Chan1998,
abstract = {Very large databases with skewed class distribu-tions and non-unlform cost per error are not un-common in real-world data mining tasks. We de-vised a multi-classifier meta-learning approach toaddress these three issues. Our empirical resultsfrom a credit card fraud detection task indicatethat the approach can significantly reduce loss dueto illegitimate transactions.},
annote = {VERY IMPORTANT
Interesting cost sensitve/sampling approach
(cost sensitive in that use cost to determine the correct sampling percentage)

- Seems they control the number of non fraudulant instances to include in the training data, effectively weighting it. 
- have a way of parallizing},
author = {Chan, Philip K and Stolfo, Salvatore J},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Chan, Stolfo - 1998 - Toward Scalable Learning with Non-uniform Class and Cost Distributions A Case Study in Credit Card Fraud Detectio.pdf:pdf},
journal = {Knowledge and Data Discovery},
pages = {164--168},
title = {{Toward Scalable Learning with Non-uniform Class and Cost Distributions : A Case Study in Credit Card Fraud Detection}},
volume = {1998},
year = {1998}
}
@article{Cano2007a,
abstract = {The generation of predictive models is a frequent task in data mining with the objective of generating highly precise and interpretable models. The data reduction is an interesting preprocessing approach that can allow us to obtain predictive models with these characteristics in large size data sets. In this paper, we analyze the rule classification model based on decision trees using a training selected set via evolutionary stratified instance selection. This method faces the scaling problem that appears in the evaluation of large size data sets, and the trade off interpretability-precision of the generated models. {\textcopyright} 2006 Elsevier B.V. All rights reserved.},
author = {Cano, J and Herrera, Francisco and Lozano, Manuel},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Cano, Herrera, Lozano - 2007 - Evolutionary stratified training set selection for extracting classification rules with trade off prec(2).pdf:pdf},
journal = {Data and Knowledge Engineering},
keywords = {Decision trees,Evolutionary algorithms,Interpretability,Precision,Rule classification,Training set selection},
pages = {90--108},
title = {{Evolutionary stratified training set selection for extracting classification rules with trade off precision-interpretability}},
volume = {60},
year = {2007}
}
@article{Walker1990,
author = {Walker, Paul A},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Walker - 1990 - Modelling Wildlife Distributions Using a Geographic Information System Kangaroos in Relation to Climate.pdf:pdf},
journal = {Journal of Biogeography},
keywords = {australia,climate,gis,kangaroos,simple},
number = {3},
pages = {279--289},
title = {{Modelling Wildlife Distributions Using a Geographic Information System : Kangaroos in Relation to Climate}},
volume = {17},
year = {1990}
}
@article{Kittler1996,
abstract = {We develop a common theoretical framework for combining$\backslash$nclassifiers which use distinct pattern representations and show that$\backslash$nmany existing schemes can be considered as special cases of compound$\backslash$nclassification where all the pattern representations are used jointly to$\backslash$nmake a decision. An experimental comparison of various classifier$\backslash$ncombination schemes demonstrates that the combination rule developed$\backslash$nunder the most restrictive assumptions-the sum rule-outperforms other$\backslash$nclassifier combinations schemes. A sensitivity analysis of the various$\backslash$nschemes to estimation errors is carried out to show that this finding$\backslash$ncan be justified theoretically},
author = {Kittler, J. and Hater, M. and Duin, R. P W},
doi = {10.1109/ICPR.1996.547205},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Kittler, Hater, Duin - 1996 - Combining classifiers.pdf:pdf},
isbn = {081867282X},
issn = {10514651},
journal = {Proceedings - International Conference on Pattern Recognition},
number = {3},
pages = {897--901},
pmid = {20470429},
title = {{Combining classifiers}},
volume = {2},
year = {1996}
}
@incollection{Prati2004,
abstract = {Several works point out class imbalance as an obstacle on applying machine learning algorithms to real world domains. However, in some cases, learning algorithms perform well on several imbalanced domains. Thus, it does not seem fair to directly correlate class imbalance to the loss of performance of learning algorithms. In this work, we develop a systematic study aiming to question whether class imbalances are truly to blame for the loss of performance of learning systems or whether the class imbalances are not a problem by themselves. Our experiments suggest that the problem is not directly caused by class imbalances, but is also related to the degree of overlapping among the classes.},
address = {Berlin Heidelberg},
author = {Prati, Ronaldo C and Batista, Gustavo E A P A and Monard, Maria C},
booktitle = {MICAI 2004: Advance in Artificial Intelligence},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Prati, Batista, Monard - 2004 - Class Imbalances versus Class Overlapping An Analysis of a Learning System Behavior.pdf:pdf},
pages = {312--321},
publisher = {Springer},
title = {{Class Imbalances versus Class Overlapping : An Analysis of a Learning System Behavior}},
year = {2004}
}
@article{Garcia2006,
abstract = {Unbalanced data in a classification problem appears when there are many more instances of some classes than others. Several solutions were proposed to solve this problem at data level by under-sampling. The aim of this work is to propose evolutionary prototype selection algorithms that tackle the problem of unbalanced data by using a new fitness function. The results obtained show that a balancing of data performed by evolutionary under-sampling outperforms previously proposed under-sampling methods in classification accuracy, obtaining reduced subsets and getting a good balance on data.},
author = {Garc{\'{i}}a, Salvador and Cano, Jose Ramon and Fernandez, Alberto and Herrera, Francisco},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Garc{\'{i}}a et al. - 2006 - A Proposal of Evolutionary Prototype Selection for Class Imbalance Problems.pdf:pdf},
pages = {1415--1423},
title = {{A Proposal of Evolutionary Prototype Selection for Class Imbalance Problems}},
year = {2006}
}
@misc{Wickham2009,
author = {Wickham, Hadley},
publisher = {Springer New York},
title = {ggplot2: elegant graphics for data analysis},
url = {http://had.co.nz/ggplot2/book},
year = {2009}
}
@inproceedings{Vandewiele2016,
abstract = {Models obtained by decision tree induction techniques excel in being interpretable.However, they can be prone to overfitting, which results in a low predictive performance. Ensemble techniques are able to achieve a higher accuracy. However, this comes at a cost of losing interpretability of the resulting model. This makes ensemble techniques impractical in applications where decision support, instead of decision making, is crucial. To bridge this gap, we present the GENESIM algorithm that transforms an ensemble of decision trees to a single decision tree with an enhanced predictive performance by using a genetic algorithm. We compared GENESIM to prevalent decision tree induction and ensemble techniques using twelve publicly available data sets. The results show that GENESIM achieves a better predictive performance on most of these data sets than decision tree induction techniques and a predictive performance in the same order of magnitude as the ensemble techniques. Moreover, the resulting model of GENESIM has a very low complexity, making it very interpretable, in contrast to ensemble techniques.},
archivePrefix = {arXiv},
arxivId = {1611.05722},
author = {Vandewiele, Gilles and Janssens, Olivier and Ongenae, Femke and {De Turck}, Filip and {Van Hoecke}, Sofie},
booktitle = {NIPS},
eprint = {1611.05722},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Vandewiele et al. - 2016 - GENESIM genetic extraction of a single, interpretable model.pdf:pdf},
title = {{GENESIM: genetic extraction of a single, interpretable model}},
url = {http://arxiv.org/abs/1611.05722},
year = {2016}
}
@article{Bansal2014,
abstract = {Today's computer vision systems are not perfect. They fail frequently. Even worse, they fail abruptly and seemingly inexplicably. We argue that making our systems more transparent via an explicit hu- man understandable characterization of their failure modes is desirable. We propose characterizing the failure modes of a vision system using semantic attributes. For example, a face recognition system may say “If the test image is blurry, or the face is not frontal, or the person to be recognized is a young white woman with heavy make up, I am likely to fail.” This information can be used at training time by researchers to design better features, models or collect more focused training data. It can also be used by a downstream machine or human user at test time to know when to ignore the output of the system, in turn making it more reliable. To generate such a “specification sheet”, we discrimina- tively cluster incorrectly classified images in the semantic attribute space using L1-regularized weighted logistic regression.We show that our spec- ification sheets can predict oncoming failures for face and animal species recognition better than several strong baselines. We also show that lay people can easily follow our specification sheets.},
annote = {Create human interpretable attributes to images. Assign attributes to specific images (0 absent, 1 present). aAnd try to cluster misclassified images.


"as researchers, we can design vision solutions more effectively if we systematically understand the failure modes of our systems."

"Identification of recurring failure modes via manual inspection of instances where the system fails is not feasible given the scale of the data involved in realistic applications" - does not scale well, not able to do systematically

"At test time, our characterization of failure modes can be used to automat-
ically detect oncoming failure."

"This helps gain operator trust in applications involving semi-autonomous systems."

"because vi-
sion systems often suffer from systematic failure modes"

"Of course, similar to other sophisticated systems, vision systems also suffer from arbitrary non-systematic mistakes."},
author = {Bansal, Aayush and Farhadi, Ali and Parikh, Devi},
file = {:Users/Walter/Documents/Literature/24fa18de2a3b57bb588e474d2b29c2107d57.pdf:pdf},
isbn = {9783319105987},
issn = {16113349},
journal = {European Conference on Computer Vision},
pages = {366--381},
title = {{Towards transparent systems: Semantic characterization of failure modes}},
year = {2014}
}
@inproceedings{Venkateswaran2016,
abstract = {{\textcopyright} 2016 SPIE. This work addresses the problem of identifying the set of nodes in a power network critical to system operation. Formally, the CNA problem is the problem of identifying a minimum cardinality set of nodes to target in a power network in order to reduce throughput by a given factor. Since the defender may reroute flows in an attempt to restore throughput, the attack must anticipate and defeat this possibility. We develop here an algorithm to solve this problem. In our approach we model the problem as a bi-level optimization problem where the master problem attempts different attack combinations and the sub-problem responds with the best routing. The optimization problems that result from such a framework are mixed integer programs (MIPs), which we solve in our implementation using IBM CPLEX. The algorithm has been tested on several benchmark networks and appears to perform well. We have also developed variants that can be used for determining optimal restoration configuration post damage on large networks (4000 nodes, 8000 links) and for modeling propagation of failures after the initial attack. We report on computational experiments with these variants as well.},
author = {Venkateswaran, V. and Bennette, W.},
booktitle = {SPIE Defense+ Security. International Society for Optics and Photonics},
doi = {10.1117/12.2223499},
isbn = {9781510600911},
issn = {1996756X},
keywords = {Bi-Level optimization,Power network vulnerability,cascading failures,network restoration},
title = {{Critical node analysis (CNA) of electrical infrastructure networks}},
volume = {9850},
year = {2016}
}
@article{Dickinson2010,
abstract = {Citizen science, the involvement of volunteers in research, has increased the scale of ecological field studies with continent-wide, centralized monitoring efforts and, more rarely, tapping of volunteers to conduct large, coordinated, field experiments. The unique benefit for the field of ecology lies in understanding processes occurring at broad geographic scales and on private lands, which are impossible to sample extensively with traditional field research models. Citizen science produces large, longitudinal data sets, whose potential for error and bias is poorly understood. Because it does not usually aim to uncover mechanisms underlying ecological patterns, citizen science is best viewed as complementary to more localized, hypothesis-driven research. In the process of addressing the impacts of current, global “experiments” altering habitat and climate, large-scale citizen science has led to new, quantitative approaches to emerging questions about the distribution and abundance of organisms across space and time.},
author = {Dickinson, Janis L. and Zuckerberg, Benjamin and Bonter, David N.},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Dickinson, Zuckerberg, Bonter - 2010 - Citizen Science as an Ecological Research Tool Challenges and Benefits.pdf:pdf},
issn = {1543-592X},
journal = {Annual Review of Ecology, Evolution, and Systematics},
keywords = {biodiversity,climate change,crowdsourcing,ecology,fragmentation,geographical,habitat loss,invasive species,macroecology,monitoring,observer,population ecology,quality,sampling bias,sampling error,spatial ecology},
month = {dec},
number = {1},
pages = {149--172},
title = {{Citizen Science as an Ecological Research Tool: Challenges and Benefits}},
url = {http://www.annualreviews.org/doi/abs/10.1146/annurev-ecolsys-102209-144636},
volume = {41},
year = {2010}
}
@article{Kim2015,
abstract = {We present the Mind the Gap Model (MGM), an approach for interpretable fea- ture extraction and selection. By placing interpretability criteria directly into the model, we allowfor the model to both optimize parameters related to interpretabil- ity and to directly report a global set of distinguishable dimensions to assist with further data exploration and hypothesis generation. MGM extracts distinguishing features on real-world datasets of animal features, recipes ingredients, and dis- ease co-occurrence. It also maintains or improves performance when compared to related approaches. We perform a user study with domain experts to show the MGM's ability to help with dataset exploration.},
author = {Kim, Been and Shah, Julie and Doshi-Velez, Finale},
file = {:Users/Walter/Documents/Literature/5957-mind-the-gap-a-generative-approach-to-interpretable-feature-selection-and-extraction.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
pages = {1--9},
title = {{Mind the Gap : A Generative Approach to Interpretable Feature Selection and Extraction}},
year = {2015}
}
@article{Cano2006,
annote = {For Knn but has a good stratification strategy.},
author = {Cano, Jos{\'{e}} Ram{\'{o}}n and Herrera, Francisco and Lozano, Manuel},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Cano, Herrera, Lozano - 2006 - On the combination of evolutionary algorithms and stratified strategies for training set selection in dat.pdf:pdf},
issn = {15684946},
journal = {Applied Soft Computing},
keywords = {data mining,evolutionary algorithms,instance selection,stratification,training set selection},
pages = {323--332},
title = {{On the combination of evolutionary algorithms and stratified strategies for training set selection in data mining}},
volume = {6},
year = {2006}
}
@article{Belhumeur1997,
author = {Belhumeur, Peter N. and Hespanha, Joao P. and Kriegman, David J.},
file = {:Users/Walter/Documents/Literature/fisherface-pami97.pdf:pdf},
journal = {IEEE Transactions on Patern Analysis and Machine Intelligence},
number = {7},
pages = {711--720},
title = {{Eigenfaces vs . Fisherfaces : Recognition Using Class Specific Linear Projection}},
volume = {19},
year = {1997}
}
@article{Avati2017,
abstract = {Improving the quality of end-of-life care for hospitalized patients is a priority for healthcare organizations. Studies have shown that physicians tend to over-estimate prognoses, which in combination with treatment inertia results in a mismatch between patients wishes and actual care at the end of life. We describe a method to address this problem using Deep Learning and Electronic Health Record (EHR) data, which is currently being piloted, with Institutional Review Board approval, at an academic medical center. The EHR data of admitted patients are automatically evaluated by an algorithm, which brings patients who are likely to benefit from palliative care services to the attention of the Palliative Care team. The algorithm is a Deep Neural Network trained on the EHR data from previous years, to predict all-cause 3-12 month mortality of patients as a proxy for patients that could benefit from palliative care. Our predictions enable the Palliative Care team to take a proactive approach in reaching out to such patients, rather than relying on referrals from treating physicians, or conduct time consuming chart reviews of all patients. We also present a novel interpretation technique which we use to provide explanations of the model's predictions.},
archivePrefix = {arXiv},
arxivId = {1711.06402},
author = {Avati, Anand and Jung, Kenneth and Harman, Stephanie and Downing, Lance and Ng, Andrew and Shah, Nigam H.},
eprint = {1711.06402},
file = {:Users/Walter/Documents/Literature/1711.06402.pdf:pdf},
isbn = {9781509016297},
title = {{Improving Palliative Care with Deep Learning}},
url = {http://arxiv.org/abs/1711.06402},
year = {2017}
}
@article{Lenguas2011,
author = {Lenguas, Ley De and Delgado, Susy},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Lenguas, Delgado - 2011 - 4-5 6 8.pdf:pdf},
pages = {1--60},
title = {4-5 6 8},
year = {2011}
}
@article{Kuncheva1995,
abstract = {A genetic algorithm is applied for selecting a reference set for the k-Nearest Neighbors rule. The performance has been evaluated on a medical data set by the rotation method. The results are commented together with those obtained with the standard k-NN, random selection, Wilson's technique, and the MULTIEDIT algorithm. ?? 1995.},
author = {Kuncheva, Ludmila I.},
isbn = {0167-8655},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Editing techniques,Genetic algorithms,k-NN rule},
number = {8},
pages = {809--814},
title = {{Editing for the k-nearest neighbors rule by a genetic algorithm}},
volume = {16},
year = {1995}
}
@article{Kaczaek2016,
abstract = {{\textless}p{\textgreater}The objective of this research is to detect points that describe a road surface in an unclassified point cloud of the airborne laser scanning (ALS). For this purpose we use the Random Forest learning algorithm. The proposed methodology consists of two stages: preparation of features and supervised point cloud classification. In this approach we consider ALS points, representing only the last echo. For these points RGB, intensity, the normal vectors, their mean values and the standard deviations are provided. Moreover, local and global height variations are taken into account as components of a feature vector. The feature vectors are calculated on a basis of the 3D Delaunay triangulation. The proposed methodology was tested on point clouds with the average point density of 12 pts/m2 that represent large urban scene. The significance level of 15{\%} was set up for a decision tree of the learning algorithm. As a result of the Random Forest classification we received two subsets of ALS points. One of those groups represents points belonging to the road network. After the classification evaluation we achieved from 90{\%} of the overall classification accuracy. Finally, the ALS points representing roads were merged and simplified into road network polylines using morphological operations.{\textless}/p{\textgreater}},
author = {Kacza{\l}ek, B. and Borkowski, A.},
doi = {10.5194/isprsarchives-XLI-B3-255-2016},
file = {:Users/Walter/Documents/Literature/URBAN{\_}ROAD{\_}DETECTION{\_}IN{\_}AIRBONE{\_}LASER{\_}SCANNING{\_}POI.pdf:pdf},
issn = {2194-9034},
journal = {ISPRS - International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
keywords = {als,classification,detection,random forest,road},
number = {July},
pages = {255--259},
title = {{Urban Road Detection in Airbone Laser Scanning Point Cloud Using Random Forest Algorithm}},
url = {http://www.int-arch-photogramm-remote-sens-spatial-inf-sci.net/XLI-B3/255/2016/isprs-archives-XLI-B3-255-2016.pdf},
volume = {XLI-B3},
year = {2016}
}
@article{Elhamifar2013,
annote = {Perhaps a good candidate for batch selection},
author = {Elhamifar, Ehsan and Sapiro, Guillermo and Yang, Allen Y and Sastry, S Shankar},
doi = {10.1109/ICCV.2013.33},
file = {:Users/Walter/Documents/Literature/ElhamifarConvex2013.pdf:pdf},
isbn = {9781479928392},
journal = {IEEE International Conference on Computer Vision, (ICCV)},
keywords = {active learning},
pages = {4321--4328},
title = {{A Convex Optimization Framework for Active Learning}},
year = {2013}
}
@article{Ducoffe2018,
abstract = {We propose a new active learning strategy designed for deep neural networks. The goal is to minimize the number of data annotation queried from an oracle during training. Previous active learning strategies scalable for deep networks were mostly based on uncertain sample selection. In this work, we focus on examples lying close to the decision boundary. Based on theoretical works on margin theory for active learning, we know that such examples may help to considerably decrease the number of annotations. While measuring the exact distance to the decision boundaries is intractable, we propose to rely on adversarial examples. We do not consider anymore them as a threat instead we exploit the information they provide on the distribution of the input space in order to approximate the distance to decision boundaries. We demonstrate empirically that adversarial active queries yield faster convergence of CNNs trained on MNIST, the Shoe-Bag and the Quick-Draw datasets.},
archivePrefix = {arXiv},
arxivId = {1802.09841},
author = {Ducoffe, Melanie and Precioso, Frederic},
eprint = {1802.09841},
file = {:Users/Walter/Documents/Literature/1802.09841.pdf:pdf},
title = {{Adversarial Active Learning for Deep Networks: a Margin Based Approach}},
url = {http://arxiv.org/abs/1802.09841},
year = {2018}
}
@article{Provost1999,
annote = {Progressive sampling - trying not to have to learn from all of the data

- no recognition that naive bayes accuracy decreases as samples are added as I believe I have observed, but they are speaking of large datasets, maybe that is different

- perhaps a nice idea for idenitfying required peak in random subsets for instance selection},
author = {Provost, Foster and Jensen, David and Oates, Tim},
journal = {Proceeding KDD '99 Proceedings of the fifth ACM SIGKDD international conference on Knowledge discovery and data mining},
pages = {23--32},
title = {{Efficient Progressive Sampling}},
year = {1999}
}
@article{Lakkaraju2016a,
abstract = {One of the most important obstacles to deploying predictive models is the fact that humans do not understand and trust them. Knowing which variables are important in a model's prediction and how they are combined can be very powerful in helping people understand and trust automatic decision making systems. Here we propose interpretable decision sets, a framework for building predictive models that are highly accurate, yet also highly interpretable. Decision sets are sets of independent if-then rules. Because each rule can be applied independently, decision sets are simple, concise, and easily interpretable. We formalize decision set learning through an objective function that simultaneously optimizes accuracy and interpretability of the rules. In particular, our approach learns short, accurate, and non-overlapping rules that cover the whole feature space and pay attention to small but important classes. Moreover, we prove that our objective is a non-monotone submodular function, which we efficiently optimize to find a near-optimal set of rules. Experiments show that interpretable decision sets are as accurate at classification as state-of-the-art machine learning techniques. They are also three times smaller on average than rule-based models learned by other methods. Finally, results of a user study show that people are able to answer multiple-choice questions about the decision boundaries of interpretable decision sets and write descriptions of classes based on them faster and more accurately than with other rule-based models that were designed for interpretability. Overall, our framework provides a new approach to interpretable machine learning that balances accuracy, interpretability, and computational efficiency.},
annote = {{\#}{\#}{\#} Background quotes

- "For classification models, this requirement means that humans can understand where the de- cision boundaries between classes are and why particular labels are predicted for different data points"

- "Interpretable models are needed in many domains because they bridge the gap between domain experts and data scientists."

- "Learning interpretable models is challenging because interpretabil-
ity and accuracy are generally two competing objectives, one fa- voring simplicity and generalization, the other favoring nuance and exception."

{\#}{\#}{\#} Background information

- Decison trees and lists are favored for interpretable models because they are stated in terms of input features and do not rely on any latent variables or representations

- "There can be no better judges than humans to evaluate this notion of interpretability"

{\#}{\#}{\#} Their method 

- Decision sets, which are if then rules, but not in a hierarchy
- Perform association rule mining to get candidates for sets
- define an optimization problem and solve with Smooth Local Search

{\#}{\#}{\#} Evaluation

- Evaluate interpretability by the conciseness, coverage, and overlap of a rule set
- Use a user study where humans answer multiple choice questions about the decision boundaries using lists then sets
- Count words used to describe a class and time to write description
- 47 students in a data mining class

{\#}{\#}{\#} Unique paper aspects 

- "We show that exactly optimizing the objective is an NP-hard problem. However, we also show that eh objective ha s a particular structure which allows for provably near-optimal solutions"

- conduct a user study


{\#}{\#}{\#} Follow up

{\#}{\#}{\#}{\#} Interpretability motivation

- S. Guillaume. Designing fuzzy inference systems from data: An interpretability-oriented review. IEEE Transactions on Fuzzy Systems, 9(3):426–443, 2001. 

- W. Revelle and T. Rocklin. Very simple structure: An alternative procedure for estimating the optimal number of interpretable factors. Multivariate Behavioral Research, 14(4):403–414, 1979.

- H. Schielzeth. Simple means to improve the interpretability of regression coefficients. Methods in Ecology and Evolution, 1(2):103–113, 2010.

- G. Ridgeway, D. Madigan, T. Richardson, and J. O'Kane. Interpretable boosted naive Bayes classification. In KDD, 1998.

- D. Nauck and R. Kruse. Obtaining interpretable fuzzy classification rules from medical data. A.I. in Medicine, 16(2):149–169, 1999

{\#}{\#}{\#}{\#} Other methods

- J. Bien and R. Tibshirani. Classification by set cover: The prototype vector machine. arXiv:0908.2284 [stat.ML], 2009.

- H. Schielzeth. Simple means to improve the interpretability of regression coefficients. Methods in Ecology and Evolution, 1(2):103–113, 2010

- R. Tibshirani. Regression shrinkage and selection via the lasso. J. of the Royal Statistical Society. Series B, 58(1):267–288, 1996.

- T. J. Hastie and R. J. Tibshirani. Generalized Additive Models. Chapman and Hall/CRC, 1990.

- Y. Lou, R. Caruana, and J. Gehrke. Intelligible models for classification and regression. In KDD, 2012

- Y. Lou, R. Caruana, J. Gehrke, and G. Hooker. Accurate intelligible models with pairwise interactions. In KDD, 2013.

- B. Ustun and C. Rudin. Supersparse linear integer models for optimized medical scoring systems. Machine Learning, 102(3):1–43, 2015

- S.Wood. Generalized Additive Models: An Introduction with R. Chapman and Hall/CRC, 2006

- ***B. Kim, C. Rudin, and J. Shah. The Bayesian case model: A generative approach for case-based reasoning and prototype classification. In NIPS, 2014.***

- B. Kim, J. Shah, and F. Doshi-Velez. Mind the gap: A generative approach tointerpretable feature selection and extraction. In NIPS, 2015},
author = {Lakkaraju, H and Bach, S H and Jure, L},
doi = {10.1145/2939672.2939874},
file = {:Users/Walter/Documents/Literature/lakkaraju-kdd16.pdf:pdf},
isbn = {2154817X (Linking)},
journal = {Kdd},
keywords = {Joint,KDD Stanford University,Prediction important obstacles,Stanford University jure,University bach cs,art,average rule,balances accuracy interpretability,choice questions boundaries,edu Recent,edu Stanford University,fact,interpretability rules,interpretable accurate classification,interpretable formalize,interpretable sets,interpretable write descriptions,learning,methods,objective non monotone,optimal rules Experiments,overlapping rules,predictive models,simple concise,small important classes,space pay attention,stanford edu Stanford,state,submodular function,systems,techniques,variables important model},
pages = {1675--1684},
pmid = {27853627},
title = {{Interpretable Decision Sets: A Joint Framework for Description and Prediction}},
url = {https://www.ncbi.nlm.nih.gov/pubmed/27853627},
volume = {2016},
year = {2016}
}
@article{Kannan2018,
abstract = {In this paper, we develop improved techniques for defending against adversarial examples at scale. First, we implement the state of the art version of adversarial training at unprecedented scale on ImageNet and investigate whether it remains effective in this setting - an important open scientific question (Athalye et al., 2018). Next, we introduce enhanced defenses using a technique we call logit pairing, a method that encourages logits for pairs of examples to be similar. When applied to clean examples and their adversarial counterparts, logit pairing improves accuracy on adversarial examples over vanilla adversarial training; we also find that logit pairing on clean examples only is competitive with adversarial training in terms of accuracy on two datasets. Finally, we show that adversarial logit pairing achieves the state of the art defense on ImageNet against PGD white box attacks, with an accuracy improvement from 1.5{\%} to 27.9{\%}. Adversarial logit pairing also successfully damages the current state of the art defense against black box attacks on ImageNet (Tramer et al., 2018), dropping its accuracy from 66.6{\%} to 47.1{\%}. With this new accuracy drop, adversarial logit pairing ties with Tramer et al.(2018) for the state of the art on black box attacks on ImageNet.},
archivePrefix = {arXiv},
arxivId = {1803.06373},
author = {Kannan, Harini and Kurakin, Alexey and Goodfellow, Ian},
eprint = {1803.06373},
file = {:Users/Walter/Documents/Literature/1803.06373.pdf:pdf},
title = {{Adversarial Logit Pairing}},
url = {http://arxiv.org/abs/1803.06373},
year = {2018}
}
@article{Yosinski2014,
abstract = {Deep neural networks (DNNs) have recently been achieving state-of-the-art performance on a variety of pattern-recognition tasks, most notably visual classification problems. Given that DNNs are now able to classify objects in images with near-human-level performance, questions naturally arise as to what differences remain between computer and human vision. A recent study revealed that changing an image (e.g. of a lion) in a way imperceptible to humans can cause a DNN to label the image as something else entirely (e.g. mislabeling a lion a library). Here we show a related result: it is easy to produce images that are completely unrecognizable to humans, but that state-of-the-art DNNs believe to be recognizable objects with 99.99{\%} confidence (e.g. labeling with certainty that white noise static is a lion). Specifically, we take convolutional neural networks trained to perform well on either the ImageNet or MNIST datasets and then find images with evolutionary algorithms or gradient ascent that DNNs label with high confidence as belonging to each dataset class. It is possible to produce images totally unrecognizable to human eyes that DNNs believe with near certainty are familiar objects, which we call "fooling images" (more generally, fooling examples). Our results shed light on interesting differences between human vision and current DNNs, and raise questions about the generality of DNN computer vision.},
annote = {From Duplicate 3 (Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images - Yosinski, Jason; Clune, Jeff; Nguyen, Anh; Yosinski, Jason; Clune, Jeff)

Retraining does not seem to solve, 

point out costly mistakes can be made for facial recognition, 

gaming search engines (text classification)},
archivePrefix = {arXiv},
arxivId = {1412.1897},
author = {Yosinski, Jason and Clune, Jeff and Nguyen, Anh and Yosinski, Jason and Clune, Jeff},
doi = {10.1109/CVPR.2015.7298640},
eprint = {1412.1897},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Yosinski et al. - 2014 - Deep Neural Networks are Easily Fooled High Confidence Predictions for Unrecognizable Images.pdf:pdf;:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Yosinski et al. - 2014 - Deep Neural Networks are Easily Fooled High Confidence Predictions for Unrecognizable Images(2).pdf:pdf},
isbn = {9781467369640},
title = {{Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images}},
url = {http://arxiv.org/abs/1412.1897},
year = {2014}
}
@article{Kohavi1995,
abstract = {We review accuracy estimation methods and compare the two most common methods crossvalidation and bootstrap Recent experimental results on artificial data and theoretical re cults m restricted settings have shown that for selecting a good classifier from a set of classifiers (model selection), ten-fold cross-validation may be better than the more expensive kap one-out cross-validation We report on a largescale experimentover half a million runs of C4 5 and aNaive-Bayes algorithmloestimale the effects of different parameters on these al gonthms on real-world datascts For crossvalidation we vary the number of folds and whether the folds arc stratified or not, for bootstrap, we vary the number of bootstrap samples Our results indicate that for real-word datasets similar to ours, The best method lo use for model selection is ten fold stratified cross validation even if computation power allows using more folds.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Kohavi, Ron},
doi = {10.1067/mod.2000.109031},
eprint = {arXiv:1011.1669v3},
file = {:Users/Walter/Documents/Literature/d781305750b37acb35fa187febd8db67bfcc.pdf:pdf},
isbn = {1-55860-363-8},
issn = {10450823},
journal = {International Joint Conference on Artificial Intelligence},
number = {12},
pages = {1137--1143},
pmid = {11029742},
title = {{A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection}},
volume = {14},
year = {1995}
}
@article{Manton2015,
abstract = {Reproducing kernel Hilbert spaces are elucidated without assuming prior familiarity with Hilbert spaces. Compared with extant pedagogic material, greater care is placed on motivating the definition of reproducing kernel Hilbert spaces and explaining when and why these spaces are efficacious. The novel viewpoint is that reproducing kernel Hilbert space theory studies extrinsic geometry, associating with each geometric configuration a canonical overdetermined coordinate system. This coordinate system varies continuously with changing geometric configurations, making it well-suited for studying problems whose solutions also vary continuously with changing geometry. This primer can also serve as an introduction to infinite-dimensional linear algebra because reproducing kernel Hilbert spaces have more properties in common with Euclidean spaces than do more general Hilbert spaces.},
archivePrefix = {arXiv},
arxivId = {arXiv:1408.0952v2},
author = {Manton, Jonathan H and Amblard, Pierre-Olivier},
doi = {10.1561/XXXXXXXXXX},
eprint = {arXiv:1408.0952v2},
file = {:Users/Walter/Documents/Literature/hilbertSpaces.pdf:pdf},
issn = {1935-8237},
journal = {Foundations and Trends in Signal Processing2},
number = {1-2},
pages = {1--126},
title = {{A Primer on Reproducing Kernel Hilbert Spaces}},
volume = {8},
year = {2015}
}
@article{Wilson1972,
abstract = {The convergence properties of a nearest neighbor rule that uses an editing procedure to reduce the number of preclassified samples and to improve the performance of the rule are developed. Editing of the preclassified samples using the three-nearest neighbor rule followed by classification using the single-nearest neighbor rule with the remaining preclassified samples appears to produce a decision procedure whose risk approaches the Bayes' risk quite closely in many problems with only a few preclassified samples. The asymptotic risk of the nearest neighbor rules and the nearest neighbor rules using edited preclassified samples is calculated for several problems.},
annote = {When using ENN, they sugest using k = 3 to edit down instances, then use k = 1 when actually classifying},
author = {Wilson, Dennis L.},
isbn = {0018-9472},
issn = {0018-9472},
journal = {IEEE Transactions on Systems, Man, and Cybernetics},
number = {3},
pages = {408--421},
title = {{Asymptotic Properties of Nearest Neighbor Rules Using Edited Data}},
volume = {2},
year = {1972}
}
@article{Wickham2014,
abstract = {In this paper we present the R package gRain for propagation in graphical indepen- dence networks (for which Bayesian networks is a special instance). The paper includes a description of the theory behind the computations. The main part of the paper is an illustration of how to use the package. The paper also illustrates how to turn a graphical model and data into an independence network.},
archivePrefix = {arXiv},
arxivId = {arXiv:1501.0228},
author = {Wickham, Hadley},
doi = {10.18637/jss.v059.i10},
eprint = {arXiv:1501.0228},
file = {:Users/Walter/Documents/Literature/tidyData.pdf:pdf},
isbn = {9780387781662},
issn = {1548-7660},
journal = {Journal of Statistical Software},
keywords = {conditional independence,conditional probability,data cleaning,data tidying,dence,directed acyclical graph,evi-,expert system,graph,graphical model,junction tree,maximum cardinality search,message passing,r,relational databases,running intersection property,triangulation},
number = {10},
pages = {1--23},
pmid = {18291371},
title = {{Tidy data}},
volume = {46},
year = {2014}
}
@article{Rebbapragada2009,
abstract = {Catalogs of periodic variable stars contain large numbers of periodic light-curves (photometric time series data from the astrophysics domain). Separating anomalous objects from well-known classes is an important step towards the discovery of new classes of astronomical objects. Most anomaly detection methods for time series data assume either a single continuous time series or a set of time series whose periods are aligned. Light-curve data precludes the use of these methods as the periods of any given pair of light-curves may be out of sync. One may use an existing anomaly detection method if, prior to similarity calculation, one performs the costly act of aligning two light-curves, an operation that scales poorly to massive data sets. This paper presents PCAD, an unsupervised anomaly detection method for large sets of unsynchronized periodic time-series data, that outputs a ranked list of both global and local anomalies. It calculates its anomaly score for each light-curve in relation to a set of centroids produced by a modified k-means clustering algorithm. Our method is able to scale to large data sets through the use of sampling. We validate our method on both light-curve data and other time series data sets. We demonstrate its effectiveness at finding known anomalies, and discuss the effect of sample size and number of centroids on our results. We compare our method to naive solutions and existing time series anomaly detection methods for unphased data, and show that PCAD's reported anomalies are comparable to or better than all other methods. Finally, astrophysicists on our team have verified that PCAD finds true anomalies that might be indicative of novel astrophysical phenomena.},
annote = {1. Phase shift looks at covariances to match up light curves as best as possible
2. It is expensive to do!
3. PCAD does no rearranging
4.},
archivePrefix = {arXiv},
arxivId = {0905.3428},
author = {Rebbapragada, Umaa and Protopapas, Pavlos and Brodley, Carla E. and Alcock, Charles},
doi = {10.1007/s10994-008-5093-3},
eprint = {0905.3428},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Rebbapragada et al. - 2009 - Finding anomalous periodic time series An application to catalogs of periodic variable stars.pdf:pdf},
issn = {08856125},
journal = {Machine Learning},
keywords = {Anomaly detection,Time series data},
number = {3},
pages = {281--313},
title = {{Finding anomalous periodic time series : An application to catalogs of periodic variable stars}},
volume = {74},
year = {2009}
}
@article{Si2013,
abstract = {This paper presents a framework for unsupervised learning of a hierarchical reconfigurable image template--the AND-OR Template (AOT) for visual objects. The AOT includes: 1) hierarchical composition as "AND" nodes, 2) deformation and articulation of parts as geometric "OR" nodes, and 3) multiple ways of composition as structural "OR" nodes. The terminal nodes are hybrid image templates (HIT) [17] that are fully generative to the pixels. We show that both the structures and parameters of the AOT model can be learned in an unsupervised way from images using an information projection principle. The learning algorithm consists of two steps: 1) a recursive block pursuit procedure to learn the hierarchical dictionary of primitives, parts, and objects, and 2) a graph compression procedure to minimize model structure for better generalizability. We investigate the factors that influence how well the learning algorithm can identify the underlying AOT. And we propose a number of ways to evaluate the performance of the learned AOTs through both synthesized examples and real-world images. Our model advances the state of the art for object detection by improving the accuracy of template matching.},
author = {Si, Zhangzhang and Zhu, Song Chun},
doi = {10.1109/TPAMI.2013.35},
file = {:Users/Walter/Documents/Literature/31e5f9edcacc182fbc2f9923456ca7b27128.pdf:pdf},
isbn = {2012010032},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Deformable templates,image grammar,information projection,object recognition},
number = {9},
pages = {2189--2205},
pmid = {23868779},
title = {{Learning and-or templates for object recognition and detection}},
volume = {35},
year = {2013}
}
@article{Li2004,
abstract = {This paper is a comparative study of feature selection methods in$\backslash$nstatistical learning of text categorization . The focus is on aggres-$\backslash$nsive dimensionality reduction. Five meth- ods were evaluated, including$\backslash$nterm selection based on document frequency (DF), informa- tion gain$\backslash$n...},
author = {Li, T. and Zhang, C. and Ogihara, M.},
doi = {10.1093/bioinformatics/bth267},
isbn = {1558604863},
issn = {1367-4803},
journal = {Bioinformatics},
number = {15},
pages = {2429--2437},
pmid = {15087314},
title = {{A comparative study of feature selection and multiclass classification methods for tissue classification based on gene expression}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.32.9956{\&}rep=rep1{\&}type=pdf{\%}5Cnpapers2://publication/uuid/23DB36B5-2348-44C4-B831-DBDD6EC7702D{\%}5Cnhttp://bioinformatics.oxfordjournals.org/cgi/doi/10.1093/bioinformatics/bth267},
volume = {20},
year = {2004}
}
@article{Berndt1994,
abstract = {Knowledge discovery in databases presents many interesting challenges within the context of providing computer tools for exploring large data archives. Electronic data repositories are growing qulckiy and contain data from commercial, scientific, and other domains. Much of this data is inherently temporal, such as stock prices or NASA telemetry data. Detecting patterns in such data streams or time series is an important knowledge discovery task. This paper describes some primary experiments with a dynamic programming approach to the problem. The pattern detection algorithm is based on the dynamic time warping technique used in the speech recognition field. Keywords: dynamic programming, dynamic time warping, knowledge discovery, pattern analysis, time series.},
author = {Berndt, Donald and Clifford, James},
file = {:Users/Walter/Documents/Literature/WS94-03-031.pdf:pdf},
isbn = {0-929280-73-3},
journal = {Workshop on Knowledge Knowledge Discovery in Databases},
keywords = {dynamic programming,dynamic time warping,knowledge discovery,pat,tern analysis,time series},
pages = {359--370},
title = {{Using dynamic time warping to find patterns in time series}},
volume = {398},
year = {1994}
}
@article{Assael2017,
annote = {Get very good accuracy but dataset has very predictable structure. I suspect this will not generalize well.},
archivePrefix = {arXiv},
arxivId = {1611.01599},
author = {Assael, Yannis M and Shillingford, Brendan and Whiteson, Shimon and Freitas, Nando De},
eprint = {1611.01599},
file = {:Users/Walter/Documents/Literature/1611.01599.pdf:pdf},
pages = {1--12},
title = {{LipNet: Sentence-level Lipreading}},
year = {2017}
}
@article{Jankowski2004,
abstract = {Several methods were proposed to reduce the number of instances (vectors) in the learning set. Some of them extract only bad vectors while others try to remove as many instances as possible without significant degradation of the reduced dataset for learning. Several strategies to shrink training sets are compared here using different neural and machine learning classification algorithms. In part II [ibid. 580-585 (2004; Zbl 1058.68559)] results on benchmarks databases are presented.},
author = {Jankowski, Norbert and Grochowski, Marek},
doi = {10.1007/b98109},
isbn = {3-540-22123-9},
issn = {03029743},
journal = {Artificial intelligence and soft computing - ICAISC 2004. 7th international conference, Zakopane, Poland, June 7-11, 2004. Proceedings.},
pages = {598--603},
title = {{Comparison of Instances Seletion Algorithms I .}},
year = {2004}
}
@inproceedings{Goodfellow2016,
abstract = {This report summarizes the tutorial presented by the author at NIPS 2016 on generative adversarial networks (GANs). The tutorial describes: (1) Why generative modeling is a topic worth studying, (2) how generative models work, and how GANs compare to other generative models, (3) the details of how GANs work, (4) research frontiers in GANs, and (5) state-of-the-art image models that combine GANs with other methods. Finally, the tutorial contains three exercises for readers to complete, and the solutions to these exercises.},
archivePrefix = {arXiv},
arxivId = {1701.00160},
author = {Goodfellow, Ian},
booktitle = {NIPS},
doi = {10.1001/jamainternmed.2016.8245},
eprint = {1701.00160},
file = {:Users/Walter/Documents/Literature/GANSTutorial.pdf:pdf},
isbn = {1581138285},
issn = {0253-0465},
pmid = {15040217},
title = {{NIPS 2016 Tutorial: Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1701.00160},
year = {2016}
}
@article{Zhang2016,
abstract = {We present a novel subset scan method to detect if a probabilistic binary classifier has statistically significant bias -- over or under predicting the risk -- for some subgroup, and identify the characteristics of this subgroup. This form of model checking and goodness-of-fit test provides a way to interpretably detect the presence of classifier bias and poor classifier fit, not just in one or two dimensions of features of a priori interest, but in the space of all possible feature subgroups. We use subset scan and parametric bootstrap methods to efficiently address the difficulty of assessing the exponentially many possible subgroups. We also suggest several useful extensions of this method for increasing interpretability of predictive models and prediction performance.},
archivePrefix = {arXiv},
arxivId = {1611.08292},
author = {Zhang, Zhe and Neill, Daniel B.},
eprint = {1611.08292},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Zhang, Neill - 2016 - Identifying Significant Predictive Bias in Classifiers.pdf:pdf},
number = {Nips},
pages = {1--7},
title = {{Identifying Significant Predictive Bias in Classifiers}},
url = {http://arxiv.org/abs/1611.08292},
year = {2016}
}
@article{Chan2015,
abstract = {We present Listen, Attend and Spell (LAS), a neural network that learns to transcribe speech utterances to characters. Unlike traditional DNN-HMM models, this model learns all the components of a speech recognizer jointly. Our system has two components: a listener and a speller. The listener is a pyramidal recurrent network encoder that accepts filter bank spectra as inputs. The speller is an attention-based recurrent network decoder that emits characters as outputs. The network produces character sequences without making any independence assumptions between the characters. This is the key improvement of LAS over previous end-to-end CTC models. On a subset of the Google voice search task, LAS achieves a word error rate (WER) of 14.1{\%} without a dictionary or a language model, and 10.3{\%} with language model rescoring over the top 32 beams. By comparison, the state-of-the-art CLDNN-HMM model achieves a WER of 8.0{\%}.},
archivePrefix = {arXiv},
arxivId = {1508.01211},
author = {Chan, William and Jaitly, Navdeep and Le, Quoc V. and Vinyals, Oriol},
doi = {10.1109/72.279181},
eprint = {1508.01211},
file = {:Users/Walter/Documents/Literature/44926.pdf:pdf},
isbn = {1045-9227 VO - 5},
issn = {19410093},
journal = {arXiv preprint},
pages = {1--16},
pmid = {18267787},
title = {{Listen, attend and spell}},
url = {http://arxiv.org/abs/1508.01211},
year = {2015}
}
@article{Zhu2017,
abstract = {Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain {\$}X{\$} to a target domain {\$}Y{\$} in the absence of paired examples. Our goal is to learn a mapping {\$}G: X \backslashrightarrow Y{\$} such that the distribution of images from {\$}G(X){\$} is indistinguishable from the distribution {\$}Y{\$} using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping {\$}F: Y \backslashrightarrow X{\$} and introduce a cycle consistency loss to push {\$}F(G(X)) \backslashapprox X{\$} (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.},
archivePrefix = {arXiv},
arxivId = {1703.10593},
author = {Zhu, Jun Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
doi = {10.1109/ICCV.2017.244},
eprint = {1703.10593},
file = {:Users/Walter/Documents/Literature/1703.10593.pdf:pdf},
isbn = {9781538610329},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {2242--2251},
title = {{Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks}},
volume = {2017-Octob},
year = {2017}
}
@article{Gordon1989,
abstract = {During incremental concept learning from examples, tentative hypotheses are formed and then modified to form new hypotheses. When there is a choice among hypotheses, bias is used to express a preference. Bias may be expressed by the choice of hypothesis language, it may be implemented as an evaluation function for selecting among hypotheses already generated, or it may consist of screening potential hypotheses prior to hypothesis generation. This paper describes the use of the third method. Bias is represented explicitly both as assumptions that reduce the space of potential hypotheses and as procedures for testing these assumptions. There are advantages gained by using explicit assumptions. One advantage is that the assumptions are meta-level hypotheses that are used to generate future, as well as to select between current, inductive hypotheses. By testing these meta-level hypotheses, a system gains the power to anticipate the form of future hypotheses. Furthermore, rigorous testing of these meta-level hypotheses before using them to generate inductive hypotheses avoids consistency checks of the inductive hypotheses. A second advantage of using explicit assumptions is that bias can be tested using a variety of learning methods.},
annote = {Not very helpful
Talks a little about different costs of misclassification},
author = {Gordon, D and Perlis, D},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Gordon, Perlis - 1989 - Explicitly biased generalization.pdf:pdf},
issn = {0824-7935},
journal = {Computational Intelligence},
number = {2},
pages = {67--81},
title = {{Explicitly biased generalization}},
url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1467-8640.1989.tb00317.x/abstract},
volume = {5},
year = {1989}
}
@article{Aghasi,
abstract = {Model reduction is a highly desirable process for deep neural networks. While large networks are theoretically capable of learning arbitrarily complex models, overfitting and model redundancy negatively affects the prediction accuracy and model variance. Net-Trim is a layer-wise convex framework to prune (sparsify) deep neural networks. The method is applicable to neural networks operating with the rectified linear unit (ReLU) as the nonlinear activation. The basic idea is to retrain the network layer by layer keeping the layer inputs and outputs close to the originally trained model, while seeking a sparse transform matrix. We present both the parallel and cascade versions of the algorithm. While the former enjoys computational distributability, the latter is capable of achieving simpler models. In both cases, we mathematically show a consistency between the retrained model and the initial trained network. We also derive the general sufficient conditions for the recovery of a sparse transform matrix. In the case of standard Gaussian training samples of dimension N being fed to a layer, and s being the maximum number of nonzero terms across all columns of the transform matrix, we show that O(s log N) samples are enough to accurately learn the layer model.},
archivePrefix = {arXiv},
arxivId = {1611.05162},
author = {Aghasi, Alireza and Nguyen, Nam and Romberg, Justin},
eprint = {1611.05162},
file = {:Users/Walter/Documents/Literature/deepLearningLP.pdf:pdf},
title = {{Net-Trim: A Layer-wise Convex Pruning of Deep Neural Networks}}
}
@article{Settles2010,
abstract = {The key idea behind active learning is that a machine learning algorithm can achieve greater accuracy with fewer training labels if it is allowed to choose the data from which it learns. An active learner may pose queries, usually in the form of unlabeled data instances to be labeled by an oracle (e.g., a human annotator). Active learning is well-motivated in many modern machine learning problems, where unlabeled data may be abundant or easily obtained, but labels are difficult, time-consuming, or expensive to obtain. This report provides a general introduction to active learning and a survey of the literature. This includes a discussion of the scenarios in which queries can be formulated, and an overview of the query strategy frameworks proposed in the literature to date. An analysis of the empirical and theoretical evidence for successful active learning, a summary of problem setting variants and practical issues, and a discussion of related topics in machine learning research are also presented.},
annote = {We may be looking at membership query synthesis},
archivePrefix = {arXiv},
arxivId = {1206.5533},
author = {Settles, Burr},
doi = {10.1.1.167.4245},
eprint = {1206.5533},
file = {:Users/Walter/Documents/Literature/Active{\_}learning{\_}literature{\_}survey.pdf:pdf},
isbn = {978-1-4673-8391-2},
issn = {00483931},
journal = {Machine Learning},
number = {2},
pages = {201--221},
pmid = {15003161},
title = {{Active Learning Literature Survey}},
volume = {15},
year = {2010}
}
@article{Bennette2017a,
abstract = {{\textcopyright} Springer International Publishing AG 2017. Instance based classifiers, such as k-Nearest Neighbors, predict the class value of a new observation based on some distance or similarity measure between the new instance and the stored training data. However, due to the required distance calculations, classifying new instances becomes computationally expensive as the number of training observations increases. Therefore, instance selection techniques have been proposed to improve instance based classifiers by reducing the number of training instances that must be stored to achieve adequate classification rates. Although other methods exist, an evolutionary algorithm has been used for instance selection with some of the best results in regard to data reduction and preservation of classification accuracy. Unfortunately, the performance of the evolutionary algorithm for instance selection comes at the cost of longer computation times in comparison to classic instance selection techniques. In this work we introduce a new stopping criterion for the evolutionary algorithm which depends on the convergence of its fitness function. Experimentation shows that the new criterion results in less computation time while achieving comparable performance.},
author = {Bennette, Walter D. W.D.},
doi = {10.1007/978-3-319-46562-3_26},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Bennette - 2017 - A data driven stopping criterion for evolutionary instance selection.pdf:pdf},
isbn = {9783319465616},
issn = {21945357},
journal = {Advances in Intelligent Systems and Computing},
number = {May 2016},
pages = {407--420},
title = {{A data driven stopping criterion for evolutionary instance selection}},
volume = {513},
year = {2017}
}
@article{Brodley1995,
author = {Brodley, Carla E.},
doi = {10.1023/A:1022686102325},
issn = {15730565},
journal = {Machine Learning},
keywords = {Inductive bias,automatic algorithm selection,decision trees,hybrid classifiers,learning from examples},
number = {1},
pages = {63--94},
title = {{Recursive Automatic Bias Selection for Classifier Construction}},
volume = {20},
year = {1995}
}
@article{Olvera-Lopez2010,
abstract = {In supervised learning, a training set providing previously known information is used to classify new instances. Commonly, several instances are stored in the training set but some of them are not useful for classifying therefore it is possible to get acceptable classification rates ignoring non useful cases; this process is known as instance selection. Through instance selection the training set is reduced which allows reducing runtimes in the classification and/or training stages of classifiers. This work is focused on presenting a survey of the main instance selection methods reported in the literature.},
author = {Olvera-Lopez, J. Arturo and Carrasco-Ochoa, J. Ariel and Martinez-Trinidad, J. Francisco and Kittler, Josef},
issn = {02692821},
journal = {Artificial Intelligence Review},
keywords = {Data reduction,Instance selection,Pre-processing,Supervised learning},
pages = {133--143},
title = {{A review of instance selection methods}},
volume = {34},
year = {2010}
}
@article{Figueredo2012,
abstract = {One issue in data classification problems is to find an optimal subset of instances to train a classifier. Training sets that represent well the characteristics of each class have better chances to build a successful predictor. There are cases where data are redundant or take large amounts of computing time in the learning process. To overcome this issue, instance selection techniques have been proposed. These techniques remove examples from the data set so that classifiers are built faster and, in some cases, with better accuracy. Some of these techniques are based on nearest neighbors, ordered removal, random sampling and evolutionary methods. The weaknesses of these methods generally involve lack of accuracy, overfitting, lack of robustness when the data set size increases and high complexity. This work proposes a simple and fast immune-inspired suppressive algorithm for instance selection, called SeleSup. According to self-regulation mechanisms, those cells unable to neutralize danger tend to disappear from the organism. Therefore, by analogy, data not relevant to the learning of a classifier are eliminated from the training process. The proposed method was compared with three important instance selection algorithms on a number of data sets. The experiments showed that our mechanism substantially reduces the data set size and is accurate and robust, specially on larger data sets. {\textcopyright} 2012 Springer-Verlag.},
author = {Figueredo, Grazziela P. and Ebecken, Nelson F F and Augusto, Douglas A. and Barbosa, Helio J C},
doi = {10.1007/s12293-012-0081-3},
file = {:Users/Walter/Documents/Literature/SeleSup.pdf:pdf},
isbn = {1229301200813},
issn = {18659284},
journal = {Memetic Computing},
keywords = {Artificial immune systems,Data classification,Data reduction,Instance selection},
number = {2},
pages = {135--147},
title = {{An immune-inspired instance selection mechanism for supervised classification}},
volume = {4},
year = {2012}
}
@phdthesis{Bennette2014,
abstract = {Aspects of a classifier's training dataset can often make building a helpful and high accuracy classifier difficult. Instance selection addresses some of the issues in a dataset by selecting a subset of the data in such a way that learning from the reduced dataset leads to a better classifier. This work introduces an integer programming formulation of instance selection that relies on column generation techniques to obtain a good solution to the problem. Experimental results show that instance selection improves the usefulness of some classifiers by optimizing the training data so that that the training dataset has easier to learn boundaries between class values. Also included in this paper are two case studies from the Surveillance, Epidemiology, and End Results (SEER) database that further confirm the benefit of instance selection. Overall, results indicate that performing instance selection for a classifier is a competitive classification approach. However, it should be noted that instance selection might overfit classifiers that have already achieved a good fit to the dataset.},
author = {Bennette, Walter Dean},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Bennette - 2014 - Instance selection for model-based classifiers.pdf:pdf},
school = {Iowa State University},
title = {{Instance selection for model-based classifiers}},
year = {2014}
}
@article{Achar2009,
abstract = {Sequence of time-ordered events arise in a variety of applications$\backslash$nlike customer transaction databases, alarm sequences in telecommunication$\backslash$nnetworks, fault logs in manufacturing plant data, web interaction$\backslash$nlogs, etc. A popular framework for temporal pattern extraction from$\backslash$nsuch data is the frequent episode discovery paradigm. An episode$\backslash$nis a set of nodes with a partial order prescribed on it, with each$\backslash$nnode associated with an event type. Efficient algorithms exist for$\backslash$nepisode discovery when the associated partial order is total(serial$\backslash$nepisode) or trivial(parallel episode). In this paper, we propose$\backslash$nefficient algorithms for discovering frequent episodes with general$\backslash$npartial orders. The algorithms generalize the existing apriori-based$\backslash$ndiscovery algorithms for serial and parallel episodes. There is an$\backslash$ninherent combinatorial explosion in frequent partial order mining.$\backslash$nWe point out that frequency alone is not a sufficient measure of$\backslash$ninterestingness for general episodes. We present post-processing$\backslash$ntechniques to prune an explosive number of uninteresting patterns$\backslash$nfrom the set of frequent partial orders. We demonstrate with simulation$\backslash$nthe efficiency of our algorithms and the effectiveness of the post-processing$\backslash$nfilters. The simulations show utility of partial order mining in$\backslash$nunearthing information that cannot be discovered by serial/parallel$\backslash$nepisode algorithms.},
archivePrefix = {arXiv},
arxivId = {0902.1227},
author = {Achar, Avinash and Laxman, Srivatsan and Raajay, V and Sastry, P S},
doi = {http://arxiv.org/abs/0902.1227},
eprint = {0902.1227},
pages = {1--51},
title = {{{\{}D{\}}iscovering general partial orders in event streams}},
year = {2009}
}
@article{Garcia-Pedrajas2013,
abstract = {In current research, an enormous amount of information is constantly being produced, which poses a challenge for data mining algorithms. Many of the problems in extremely active research areas, such as bioinformatics, security and intrusion detection, or text mining, share the following two features: large data sets and class-imbalanced distribution of samples. Although many methods have been proposed for dealing with class-imbalanced data sets, most of these methods are not scalable to the very large data sets common to those research fields. In this paper, we propose a new approach to dealing with the class-imbalance problem that is scalable to data sets with many millions of instances and hundreds of features. This proposal is based on the divide-and-conquer principle combined with application of the selection process to balanced subsets of the whole data set. This divide-and-conquer principle allows the execution of the algorithm in linear time. Furthermore, the proposed method is easy to implement using a parallel environment and can work without loading the whole data set into memory. Using 40 class-imbalanced medium-sized data sets, we will demonstrate our method's ability to improve the results of state-of-the-art instance selection methods for class-imbalanced data sets. Using three very large data sets, we will show the scalability of our proposal to millions of instances and hundreds of features.},
author = {Garc{\'{i}}a-Pedrajas, Nicol{\'{a}}s and Peŕez-Rodr{\'{i}}guez, Javier and {De Haro-Garci{\'{a}}}, Aida},
issn = {21682267},
journal = {IEEE Transactions on Cybernetics},
keywords = {Class-imbalance problem,Instance selection,Instance-based learning,Very large problems},
number = {1},
pages = {332--346},
pmid = {22868583},
title = {{OligoIS: Scalable instance selection for class-imbalanced data sets}},
volume = {43},
year = {2013}
}
@article{Muhlenbein1993,
abstract = {In this paper a new genetic algorithm called the Breeder Genetic Algorithm (BGA) is introduced. The BGA is based on artificial selection similar to that used by human breeders. A predictive model for the BGA is presented that is derived from quantitative genetics. The model is used to predict the behavior of the BGA for simple test functions. Different mutation schemes are compared by computing the expected progress to the solution. The numerical performance of the BGA is demonstrated on a test suite of multimodal functions. The number of function evaluations needed to locate the optimum scales only as n ln(n) where n is the number of parameters. Results up to n = 1000 are reported.},
author = {M{\"{u}}hlenbein, Heinz and Schlierkamp-Voosen, Dirk},
doi = {10.1162/evco.1993.1.1.25},
file = {:Users/Walter/Documents/Literature/breeder93.pdf:pdf},
isbn = {1063-6560},
issn = {1063-6560},
journal = {Evolutionary Computation},
number = {1},
pages = {25--49},
title = {{Predictive Models for the Breeder Genetic Algorithm I. Continuous Parameter Optimization}},
url = {http://www.mitpressjournals.org/doi/10.1162/evco.1993.1.1.25},
volume = {1},
year = {1993}
}
@article{Provost2000a,
annote = {Good discussion talking about learning from imbalanced data

References:

Provost {\&} Fawcett 1997
Provost {\&} Fawcett 1998
Provost {\&} Fawcett 2000
Provost {\&} Buchanan 1995
Weis 2000
Provost, Jensen, and Oates 1999},
author = {Provost, Foster},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Provost - 2000 - Machine Learning from Imbalanced Data Sets 101 Extended Abstract.pdf:pdf},
title = {{Machine Learning from Imbalanced Data Sets 101 Extended Abstract}},
year = {2000}
}
@inproceedings{Yi1998,
author = {Yi, B and Jagadish, K and Faloutsos, H},
booktitle = {ICDE},
pages = {23--27},
title = {{Efficient retrieval of similar time sequences under time warping}},
year = {1998}
}
@article{Sickle1997,
author = {Sickle, John Van},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Sickle - 1997 - Using Mean Similarity Dendrograms to Evaluate Classifications.pdf:pdf},
journal = {Journal of Agricultural, Bilogical, and Environmental Statistics},
number = {4},
pages = {370--388},
title = {{Using Mean Similarity Dendrograms to Evaluate Classifications}},
volume = {2},
year = {1997}
}
@article{Clark2016,
author = {Clark, Christopher and Divvala, Santosh},
doi = {10.1145/2910896.2910904},
isbn = {9781450342292},
journal = {Proceedings of the 16th ACM/IEEE-CS on Joint Conference on Digital Libraries - JCDL '16},
keywords = {academic search engine,figure usage analysis,scalable figure extraction,section,title extraction},
pages = {143--152},
title = {{PDFFigures 2.0: Mining Figures from Research Papers}},
url = {http://dl.acm.org/citation.cfm?doid=2910896.2910904},
year = {2016}
}
@article{Linares2014,
abstract = {{\textcopyright} 2014 International Society of Information Fusion. In recent years there has been an increase in the number of inactive and debris objects in space. The characterization of the uncertainty in the knowledge of these Space Objects (SOs) is very important in developing an understanding of the space debris fields and any present or future threat they may pose. This work examines classification based on Multiple Model Adaptive Estimation (MMAE) to extract SO characteristics from observations while estimating the probability the observations belong to a given class of objects. Recovering these characteristics and trajectories with sufficient accuracy is shown in this paper, where the characteristics are inherent in unique SO models used in the MMAE filter bank. A number of scenarios are shown to highlight the effectiveness of the proposed classification approach. The performance of this strategy is demonstrated via simulated scenarios.},
author = {Linares, R. and Crassidis, J.L. and Jah, M.K.},
file = {:Users/Walter/Documents/Literature/AD1015362.pdf:pdf},
isbn = {9788490123553},
journal = {FUSION 2014 - 17th International Conference on Information Fusion},
title = {{Space object classification and characterization via Multiple Model Adaptive Estimation}},
year = {2014}
}
@article{Rao2012,
author = {Rao, Rahul and Liptak, David and Cherukuri, Tonya and Yakobson, Boris I. and Maruyama, Benji},
doi = {10.1038/nmat3231},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Rao et al. - 2012 - In situ evidence for chirality-dependent growth rates of individual carbon nanotubes.pdf:pdf},
issn = {1476-1122},
journal = {Nature Materials},
number = {3},
pages = {213--216},
publisher = {Nature Publishing Group},
title = {{In situ evidence for chirality-dependent growth rates of individual carbon nanotubes}},
url = {http://www.nature.com/doifinder/10.1038/nmat3231},
volume = {11},
year = {2012}
}
@article{Lewis1994,
abstract = {Uncertainty sampling methods iteratively request class labels for training instances whose classes are uncertain despite the previous labeled instances. These methods can greatly reduce the number of instances that an expert need label. One problem with this approach is that the classifier best suited for an application may be too expensive to train or use during the selection of instances. We test the use of one classifier (a highly efficient probabilistic one) to select exam- ples for training another (the C4.5 rule induction program). Despite being chosen by this heterogeneous approach, the uncertainty samples yielded classifiers with lower error rates than random samples ten times larger.},
annote = {Not that helpful
argument for using one classifier to select for another

- method to not have to label all of the data
- use heterogeneous approach (use one classifier to select instances for another type of classifier)},
author = {Lewis, D.D. and Catlett, Jason},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Lewis, Catlett - 1994 - Heterogeneous uncertainty sampling for supervised learning.pdf:pdf},
journal = {Proceedings of the eleventh international conference on machine learning},
pages = {148--156},
title = {{Heterogeneous uncertainty sampling for supervised learning}},
url = {http://www.cs.brynmawr.edu/cs372/LeC94.pdf},
year = {1994}
}
@misc{Lichman2013,
author = {Lichman, M},
booktitle = {University of California, Irvine, School of Information and Computer Sciences},
publisher = {University of California, Irvine, School of Information and Computer Sciences},
title = {{{\{}UCI{\}} Machine Learning Repository}},
url = {http://archive.ics.uci.edu/ml},
year = {2013}
}
@article{Romer2006,
abstract = {Data Mining in Sensor Networks. Es werden Assoziationsregeln vorgestellt um bestimmte zeitlich-raeumliche Muster in den Daten zu finden. Als Anwendung wird das Beobachten von Vulkanen genannt. Das Mining wird fuer die Effizienz auf verschiedene Knoten verteilt.},
author = {R{\"{o}}mer, Kay},
isbn = {3540776893},
journal = {Eawms/Dcoss 2006},
number = {5005},
pages = {103--116},
title = {{Distributed Mining of Spatio-Temporal Event Patterns In Sensor Networks}},
volume = {67322},
year = {2006}
}
@article{Tukey1977,
abstract = {Scratching down numbers (stem-and-leaf); Schematic summaries (pictures and numbers); Easy re-expression; Effective comparison (including well-chosen expresion); Plots of relationship; Straightening out plots (using three points); Smoothing sequences; Optional sections for chapter 7; Parallel and wandering schematic plots; Delineations of batches of points; Using two-way analyses; Making two-way analyses; Advances fits; Three-way fits; Looking in two or more ways at batches of points; Counted fractions; Better smoothing; Counts in bin after bin; Product-ratio plots; Shapes of distribution; Mathematical distributions; Postscript.},
author = {Tukey, J W},
isbn = {0201076160},
issn = {1557170X},
journal = {Analysis},
pages = {688},
pmid = {21097328},
title = {{Exploratory Data Analysis}},
volume = {2},
year = {1977}
}
@article{Goodman2016,
abstract = {We summarize the potential impact that the European Union's new General Data Protection Regulation will have on the routine use of machine learning algorithms. Slated to take effect as law across the EU in 2018, it will restrict automated individual decision-making (that is, algorithms that make decisions based on user-level predictors) which "significantly affect" users. The law will also create a "right to explanation," whereby a user can ask for an explanation of an algorithmic decision that was made about them. We argue that while this law will pose large challenges for industry, it highlights opportunities for machine learning researchers to take the lead in designing algorithms and evaluation frameworks which avoid discrimination.},
archivePrefix = {arXiv},
arxivId = {1606.08813},
author = {Goodman, Bryce and Flaxman, Seth},
eprint = {1606.08813},
file = {:Users/Walter/Documents/Literature/euRegulation.pdf:pdf},
journal = {2016 ICML Workshop on Human Interpretability in Machine Learning (WHI 2016)},
keywords = {machine learning},
pages = {26--30},
title = {{EU regulations on algorithmic decision-making and a "right to explanation"}},
url = {http://arxiv.org/abs/1606.08813},
year = {2016}
}
@article{Hickey2003,
abstract = {An r-contour footprint is a set of individualseach of whom has a propensity of at least r ofbelonging to a rare class. The properties offootprints are summarized. An algorithm,REFLEX, is proposed for extracting a footprintfrom an induced decision tree. Results of initialexperiments comparing REFLEX to m-estimation Laplace smoothing show that bothalgorithms deliver broadly similar performancefor different contours. Unlike Laplace, REFLEXdoes not require extensive tuning. When highpropensity rare class disjuncts exist ({\textgreater} 50{\%}),both algorithms perform better on pruned trees.},
author = {Hickey, Ray J},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Hickey - 2003 - Learning Rare Class Footprints the REFLEX Algorithm.pdf:pdf},
journal = {Proceedings of the ICML},
title = {{Learning Rare Class Footprints : the REFLEX Algorithm}},
volume = {3},
year = {2003}
}
@article{Ferraz2016,
abstract = {In forested mountainous areas, the road location and characterization are invaluable inputs for various purposes such as forest management, wood harvesting industry, wildfire protection and fighting. Airborne topographic lidar has become an established technique to characterize the Earth surface. Lidar provides 3D point clouds allowing for fine reconstruction of ground topography while preserving high frequencies of the relief: fine Digital Terrain Models (DTMs) is the key product. This paper addresses the problem of road detection and characterization in forested environments over large scales ({\textgreater}1000km2). For that purpose, an efficient pipeline is proposed, which assumes that main forest roads can be modeled as planar elongated features in the road direction with relief variation in orthogonal direction. DTMs are the only input and no complex 3D point cloud processing methods are involved. First, a restricted but carefully designed set of morphological features is defined as input for a supervised Random Forest classification of potential road patches. Then, a graph is built over these candidate regions: vertices are selected using stochastic geometry tools and edges are created in order to fill gaps in the DTM created by vegetation occlusion. The graph is pruned using morphological criteria derived from the input road model. Finally, once the road is located in 2D, its width and slope are retrieved using an object-based image analysis. We demonstrate that our road model is valid for most forest roads and that roads are correctly retrieved ({\textgreater}80{\%}) with few erroneously detected pathways (10-15{\%}) using fully automatic methods. The full pipeline takes less than 2min per km2 and higher planimetric accuracy than 2D existing topographic databases are achieved. Compared to these databases, additional roads can be detected with the ability of lidar sensors to penetrate the understory. In case of very dense vegetation and insufficient relief in the DTM, gaps may exist in the results resulting in local incompleteness ({\~{}}15{\%}).},
author = {Ferraz, Ant??nio and Mallet, Cl??ment and Chehata, Nesrine},
doi = {10.1016/j.isprsjprs.2015.12.002},
file = {:Users/Walter/Documents/Literature/1-s2.0-S0924271615002609-main.pdf:pdf},
isbn = {978-1-4799-5775-0},
issn = {09242716},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
keywords = {Airborne,Classification,Forests,Large scale mapping,Lidar,Mountainous areas,Road extraction},
number = {February},
pages = {23--36},
title = {{Large-scale road detection in forested mountainous areas using airborne topographic lidar data}},
volume = {112},
year = {2016}
}
@article{Zhang2011,
abstract = {In a real tracking system, track breakages (TBs) can occur due to highly maneuvering targets, low detection probability, or clutter. In [27], a track segment association (TSA) approach was developed for an airborne early warning (AEW) system to improve track continuity by "stitching" broken track segments pertaining to the same target. However, this technique cannot provide satisfactory association performance in tracking with a ground moving target indicator (GMTI) radar ground moving targets employing evasive move-stop-move maneuvers. To avoid detection by a GMTI radar, targets can deliberately stop for some time before moving again. Since a GMTI radar does not detect a target when the radial velocity (along the line-of-sight from the sensor) falls below a certain minimum detectable velocity (MDV), the move-stop-move maneuvers of the targets usually lead to broken tracks as a result. We present a new TSA technique which employs a dummy track to formulate a complete association. By using an interacting multiple model (IMM) estimator with state-dependent mode transition probabilities (IMM-SDP) for track segment prediction (forward and backward), the proposed algorithm can effectively stitch both "regular" broken tracks and broken tracks due to targets' move-stop-move maneuvers. Comparisons are given to show the advantages of the proposed algorithm in broken tracks reduction and track continuity improvement.},
author = {Zhang, Shuo and Bar-Shalom, Yaakov},
doi = {10.1109/TAES.2011.5937272},
issn = {0018-9251},
journal = {IEEE Transactions on Aerospace and Electronic Systems},
number = {3},
pages = {1899--1914},
title = {{Track Segment Association for GMTI Tracks of Evasive Move-Stop-Move Maneuvering Targets}},
url = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp={\&}arnumber=5937272{\&}contentType=Journals+{\&}+Magazines{\&}searchField=Search{\_}All{\&}queryText=Track+Segment+Association+for+GMTI+Tracks+of+Evasive+Move-Stop-Move+Maneuvering+Targets},
volume = {47},
year = {2011}
}
@article{Ertekin2007,
abstract = {The class imbalance problem has been known to hinder the learning performance of classification algorithms. Various real-world classification tasks such as text categorization suffer from this phenomenon. We demonstrate that active learning is capable of solving the problem.},
author = {Ertekin, Seyda and Huang, Jian and Giles, C Lee},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Ertekin, Huang, Giles - 2007 - Active learning for class imbalance problem.pdf:pdf},
isbn = {9781595935977},
journal = {Proceedings of the International ACM SIGIR conference on Research and Development in Information Retrieval},
keywords = {active learning,imbalanced data,support vector machines},
pages = {823--824},
publisher = {Acm},
title = {{Active learning for class imbalance problem}},
year = {2007}
}
@article{LeCun2015,
abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
archivePrefix = {arXiv},
arxivId = {arXiv:1312.6184v5},
author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
doi = {10.1038/nature14539},
eprint = {arXiv:1312.6184v5},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/LeCun, Bengio, Hinton - 2015 - Deep learning.pdf:pdf},
isbn = {9780521835688},
issn = {0028-0836},
journal = {Nature},
number = {7553},
pages = {436--444},
pmid = {10463930},
title = {{Deep learning}},
url = {http://dx.doi.org/10.1038/nature14539},
volume = {521},
year = {2015}
}
@inproceedings{Garcia2007,
abstract = {It has been observed that class imbalance (that is, significant differences in class prior probabilities) may produce an important deterioration of the performance achieved by existing learning and classification systems. This situation is often found in real-world data describing an infrequent but important case. In the present work, we perform a review of the most important research lines on this topic and point out several directions for further investigation.},
author = {Garc{\'{i}}a, V and Mollineda, J S S{\'{a}}nchez R A and Sotoca, R Alejo J M},
booktitle = {II Congreso Espa{\~{n}}ol de Inform{\'{a}}tica},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Garc{\'{i}}a, Mollineda, Sotoca - 2007 - The class imbalance problem in pattern classification and learning.pdf:pdf},
title = {{The class imbalance problem in pattern classification and learning}},
year = {2007}
}
@article{Patnaik2012,
archivePrefix = {arXiv},
arxivId = {arXiv:1205.4477v1},
author = {Patnaik, Debprakash and Ramakrishnan, Naren},
eprint = {arXiv:1205.4477v1},
journal = {Arxiv preprint arXiv: {\ldots}},
pages = {1--25},
title = {{Streaming Algorithms for Pattern Discovery over Dynamically Changing Event Sequences}},
url = {http://arxiv.org/abs/1205.4477},
year = {2012}
}
@article{Provost2001,
annote = {MUCH BETTER READ THAN EARLIER ARTICLE

I WANT TO MIN DISTANCE TO ROCCH, MAX AUC, MAX GLIMMER ACCURACY},
author = {Provost, F and Fawcett, T},
journal = {Machine Learning},
pages = {203--231},
title = {{Robust classification for imprecise environment}},
volume = {42},
year = {2001}
}
@article{Salzberg1997,
abstract = {An important component of many data mining projects is ﬁnding a good classiﬁcation algorithm, a process that requires very careful thought about experimental design. If not done very carefully, comparative studies of classiﬁcation and other types of algorithms can easily result in statistically invalid conclusions. This is especially true when one is using data mining techniques to analyze very large databases, which inevitably contain some statistically unlikely data. This paper describes several phenomena that can, if ignored, invalidate an experimental comparison. These phenomena and the conclusions that follow apply not only to classiﬁcation, but to computational experiments in almost any aspect of data mining. The paper also discusses why comparative analysis is more important in evaluating some types of algorithms than for others, and provides some suggestions about how to avoid the pitfalls suffered by many experimental studies.},
annote = {How to compare classifiers VERY GOOD TO KEEP IN MIND

- some real dangers for comparing algorithms
- could make an interesting lecture
- UCI expand

References:

Wolpert 1992},
author = {Salzberg, Steven},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Salzberg - 1997 - On Comparing Classifiers Pitfalls to Avoid and a Recommended Approach.pdf:pdf},
isbn = {1384-5810},
issn = {13845810},
journal = {Data Mining and Knowledge Discovery},
keywords = {classification,comparative studies,statistical methods},
pages = {317--328},
title = {{On Comparing Classifiers : Pitfalls to Avoid and a Recommended Approach}},
volume = {328},
year = {1997}
}
@article{Provost1998,
annote = {The ROCCH-hybrid combines models for robustness accross different cost and class distributions.

I WANT TO TRY AND MINIMIZE DISTANCE TO ROCCH, ALSO CURIOUS IF MAXIMIZING GLIMMER ACCURACY GETS ME THERE},
author = {Provost, Foster and Fawcett, Tom},
keywords = {classi cation,comparison,cost-sensitive learn-,evaluation,ing,learning,multiple models,skewed distributions,uncertainty},
pages = {203--231},
title = {{Robust Classifcation Systems for Imprecise Environments}},
year = {1998}
}
@article{Jo2004,
abstract = {It is often assumed that class imbalances are responsible for significant losses of performance in standard classifiers. The purpose of this paper is to the question whether class imbalances are truly responsible for this degradation or whether it can be explained in some other way. Our experiments suggest that the problem is not directly caused by class imbalances, but rather, that class imbalances may yield small disjuncts which, in turn, will cause degradation. We argue that, in order to improve classifier performance, it may, then, be more useful to focus on the small disjuncts problem than it is to focus on the class imbalance problem. We experiment with a method that takes the small disjunct problem into consideration, and show that, indeed, it yields a performance superior to the performance obtained using standard or advanced solutions to the class imbalance problem.},
author = {Jo, Taeho and Japkowicz, Nathalie},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Jo, Japkowicz - 2004 - Class Imbalances versus Small Disjuncts.pdf:pdf},
journal = {ACM SIGKDD Explorations Newsletter - Special issue on learning from imbalanced datasets},
number = {1},
pages = {40--49},
title = {{Class Imbalances versus Small Disjuncts}},
volume = {6},
year = {2004}
}
@article{Wilson1997,
abstract = {Instance-based learning techniques typically handle continuous and linear input values well, but often do not handle nominal input attributes appropriately. The Value Difference Metric (VDM) was designed to find reasonable distance values between nominal attribute values, but it largely ignores continuous attributes, requiring discretization to map continuous values into nominal values. This paper proposes three new heterogeneous distance functions, called the Heterogeneous Value Difference Metric (HVDM), the Interpolated Value Difference Metric (IVDM), and the Windowed Value Difference Metric (WVDM). These new distance functions are designed to handle applications with nominal attributes, continuous attributes, or both. In experiments on 48 applications the new distance metrics achieve higher classification accuracy on average than three previous distance functions on those datasets that have both nominal and continuous attributes.},
archivePrefix = {arXiv},
arxivId = {cs/9701101},
author = {Wilson, D. Randall and Martinez, Tony R.},
doi = {10.1613/jair.346},
eprint = {9701101},
issn = {10769757},
journal = {Journal of Artificial Intelligence Research},
pages = {1--34},
primaryClass = {cs},
title = {{Improved heterogeneous distance functions}},
volume = {6},
year = {1997}
}
@article{Wang2012,
annote = {TSS for NN using entropy measure basically

do use an approximate algorithm to find out information loss information, then do the selection on a filter basis},
author = {Wang, Xizhao and Miao, Qing and Zhai, Mengyao and Zhai, Junhai},
isbn = {9781467317146},
journal = {IEEE International Conference on Systems, Man, and Cybernetics},
keywords = {-instances selection,elm,large,sample entropy},
pages = {970--974},
title = {{Instance selection based on sample entropy for efficient data classification with ELM}},
year = {2012}
}
@article{Wilson2000,
abstract = {Instance-based learning algorithms are often faced with the problem of deciding which instances to store for use during generalization. Storing too many instances can result in large memory requirements and slow execution speed, and can cause an oversensitivity to noise. This paper has two main purposes. First, it provides a survey of existing algorithms used to reduce storage requirements in instance-based learning algorithms and other exemplar-based algorithms. Second, it proposes six additional reduction algorithms called DROP1-DROP5 and DEL (three of which were first described in Wilson {\&} Martinez, 1997c, as RT1-RT3) that can be used to remove instances from the concept description. These algorithms and 10 algorithms from the survey are compared on 31 classification tasks. Of those algorithms that provide substantial storage reduction, the DROP algorithms have the highest average generalization accuracy in these experiments, especially in the presence of uniform class noise.},
annote = {Uses HVDM distance, can find that distance metric in here},
author = {Wilson, D. R and Martinez, T. R},
isbn = {0885-6125},
issn = {08856125},
journal = {Machine Learning},
keywords = {classification,instance reduction,instance-based learning,nearest neighbor,pruning},
number = {3},
pages = {257--286},
title = {{Reduction techniques for instance-based learning algorithms}},
volume = {38},
year = {2000}
}
@techreport{Dunn2004,
abstract = {GMTI can help transform Intelligence, Surveillance, and Reconnaissance (ISR), a mission area essential to the U.S. battlefield information advantage, because it can depict vehicular movement of enemy forces in near-real time throughout a large area, regardless of weather. This unprecedented capabil- ity reduces uncertainty, clearing the Clausewitzian “fog of war” that has hung over land battles for centuries, thereby multiplying the capability of the com- bined force and transforming the execution of air-land operations.},
author = {Dunn, Richard J and Bingham, Price T and Fowler, Charles a Bert},
booktitle = {Northrop Grumman},
file = {:Users/Walter/Documents/Literature/Ground-Moving-Target-Indicator (1).pdf:pdf},
number = {February},
pages = {1--32},
title = {{Ground Moving Target Indicator Radar}},
year = {2004}
}
@inproceedings{FenTan2016,
abstract = {Ensembles of decision trees have good prediction accuracy but suffer from a lack of interpretability. We propose a new approach for interpreting tree ensembles by finding prototypes in tree space, utilizing the naturally-learned similarity measure from the tree ensemble. Demonstrating the method on random forests, we show that the method benefits from two unique aspects of tree ensembles by leveraging tree structure to sequentially find prototypes, and utilizing the naturally-learned similarity measure from the tree ensemble. The method provides good prediction accuracy when found prototypes are used in nearest-prototype classifiers, while us-ing fewer prototypes than competitor methods. We are investigating the sensitivity of the method to different prototype-finding procedures and demonstrating it on higher-dimensional data.},
annote = {Find prototypes in tree space (distance metric defined by tree)
Random forests define a distance metric

simple algorithm to follow...},
archivePrefix = {arXiv},
arxivId = {1611.07115},
author = {{Fen Tan}, Hui and Hooker, Giles J and Wells, Martin T},
booktitle = {NIPS},
eprint = {1611.07115},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Fen Tan, Hooker, Wells - 2016 - Tree Space Prototypes Another Look at Making Tree Ensembles Interpretable.pdf:pdf},
title = {{Tree Space Prototypes: Another Look at Making Tree Ensembles Interpretable}},
year = {2016}
}
@article{Wang2007,
abstract = {Computation-intensive design problems are becoming increasingly common in manufacturing industries. The computation burden is often caused by expensive analysis and simulation processes in order to reach a comparable level of accuracy as physical testing data. To address such a challenge, approximation or metamodeling techniques are often used. Metamodeling techniques have been developed from many different disciplines including statistics, mathematics, computer science, and various engineering disciplines. These metamodels are initially developed as “surrogates” of the expensive simulation process in order to improve the overall computation efficiency. They are then found to be a valuable tool to support a wide scope of activities in modern engineering design, especially design optimization. This work reviews the state-of-the-art metamodel-based techniques from a practitioner's perspective according to the role of metamodeling in supporting design optimization, including model approximation, design space exploration, problem formulation, and solving various types of optimization problems. Challenges and future development of metamodeling in support of engineering design is also analyzed and discussed.},
author = {Wang, G. Gary and Shan, S.},
doi = {10.1115/1.2429697},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Wang, Shan - 2007 - Review of Metamodeling Techniques in Support of Engineering Design Optimization.pdf:pdf},
isbn = {0-7918-4255-X},
issn = {10500472},
journal = {Journal of Mechanical Design},
number = {4},
pages = {370},
title = {{Review of Metamodeling Techniques in Support of Engineering Design Optimization}},
volume = {129},
year = {2007}
}
@article{Buckland1994,
author = {Buckland, Michael K and Fredric, Gey},
journal = {JASIS},
number = {1},
pages = {12--19},
title = {{The relationship between recall and precision.}},
volume = {45},
year = {1994}
}
@article{Turney2000,
abstract = {Inductive concept learning is the task of learningto assign cases to a discrete set of classes. Inreal-world applications of concept learning, thereare many different types of cost involved. Themajority of the machine learning literatureignores all types of cost (unless accuracy isinterpreted as a type of cost measure). A fewpapers have investigated the cost ofmisclassification errors. Very few papers haveexamined the many other types of cost. In thispaper, we attempt to create a taxonomy of thedifferent types of cost that are involved ininductive concept learning. This taxonomy mayhelp to organize the literature on cost-sensitivelearning. We hope that it will inspire researchersto investigate all types of cost in inductiveconcept learning in more depth.},
annote = {LOTS of different costs

- idea of active learning sounds neat},
author = {Turney, Peter},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Turney - 2000 - Types of Cost in Inductive Concept Learning.pdf:pdf},
title = {{Types of Cost in Inductive Concept Learning}},
year = {2000}
}
@article{Brenning2005,
abstract = {The predictive power of logistic regression, support vector machines and bootstrap-aggregated classification trees (bagging, double-bagging) is compared using misclassification error rates on independent test data sets. Based on a resampling approach that takes into account spatial autocorrelation, error rates for predicting "present" and "future" landslides are estimated within and outside the training area. In a case study from the Ecuadorian Andes, logistic regression with stepwise backward variable selection yields lowest error rates and demonstrates the best generalization capabilities. The evaluation outside the training area reveals that tree-based methods tend to overfit the data. European Geosciences Union {\textcopyright} 2005 Author(s). This work is licensed under a Creative Commons License.},
author = {Brenning, a.},
doi = {10.5194/nhess-5-853-2005},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Brenning - 2005 - Spatial prediction models for landslide hazards review, comparison and evaluation.pdf:pdf},
isbn = {15618633 (ISSN)},
issn = {16849981},
journal = {Natural Hazards and Earth System Science},
number = {6},
pages = {853--862},
title = {{Spatial prediction models for landslide hazards: review, comparison and evaluation}},
volume = {5},
year = {2005}
}
@article{Paegert2014,
abstract = {We describe a new neural-net-based light curve classifier and provide it with documentation as a ready-to-use tool for the community. While optimized for identification and classification of eclipsing binary stars, the classifier is general purpose, and has been developed for speed in the context of upcoming massive surveys such as the Large Synoptic Survey Telescope. A challenge for classifiers in the context of neural-net training and massive data sets is to minimize the number of parameters required to describe each light curve. We show that a simple and fast geometric representation that encodes the overall light curve shape, together with a chi-square parameter to capture higher-order morphology information results in efficient yet robust light curve classification, especially for eclipsing binaries. Testing the classifier on the ASAS light curve database, we achieve a retrieval rate of 98{\%} and a false-positive rate of 2{\%} for eclipsing binaries. We achieve similarly high retrieval rates for most other periodic variable-star classes, including RR Lyrae, Mira, and delta Scuti. However, the classifier currently has difficulty discriminating between different sub-classes of eclipsing binaries, and suffers a relatively low ({\~{}}60{\%}) retrieval rate for multi-mode delta Cepheid stars. We find that it is imperative to train the classifier's neural network with exemplars that include the full range of light curve quality to which the classifier will be expected to perform; the classifier performs well on noisy light curves only when trained with noisy exemplars. The classifier source code, ancillary programs, a trained neural net, and a guide for use, are provided.},
archivePrefix = {arXiv},
arxivId = {arXiv:1407.0443v1},
author = {Paegert, Martin and Stassun, Keivan G. and Burger, Dan M.},
doi = {10.1088/0004-6256/148/2/31},
eprint = {arXiv:1407.0443v1},
file = {:Users/Walter/Documents/Literature/The EB Factory Project. I. A Fast, Neural-Net-Based, General Purpose Light Curve Classifier Optimized for Eclipsing Binaries.pdf:pdf},
issn = {0004-6256},
journal = {The Astronomical Journal},
keywords = {binaries,color figures,eclipsing,general,online-only material,stars,variables},
number = {2},
pages = {31},
title = {{the Eb Factory Project. I. a Fast, Neural-Net-Based, General Purpose Light Curve Classifier Optimized for Eclipsing Binaries}},
url = {http://stacks.iop.org/1538-3881/148/i=2/a=31?key=crossref.1b5467c4c9144356df62e3ebfdbf3a49},
volume = {148},
year = {2014}
}
@article{Shental2014,
abstract = {Mycotoxins are small (MW approximately 700), toxic chemical products formed as secondary metabolites by a few fungal species that readily colonise crops and contaminate them with toxins in the field or after harvest. Ochratoxins and Aflatoxins are mycotoxins of major significance and hence there has been significant research on broad range of analytical and detection techniques that could be useful and practical. Due to the variety of structures of these toxins, it is impossible to use one standard technique for analysis and/or detection. Practical requirements for high-sensitivity analysis and the need for a specialist laboratory setting create challenges for routine analysis. Several existing analytical techniques, which offer flexible and broad-based methods of analysis and in some cases detection, have been discussed in this manuscript. There are a number of methods used, of which many are lab-based, but to our knowledge there seems to be no single technique that stands out above the rest, although analytical liquid chromatography, commonly linked with mass spectroscopy is likely to be popular. This review manuscript discusses (a) sample pre-treatment methods such as liquid-liquid extraction (LLE), supercritical fluid extraction (SFE), solid phase extraction (SPE), (b) separation methods such as (TLC), high performance liquid chromatography (HPLC), gas chromatography (GC), and capillary electrophoresis (CE) and (c) others such as ELISA. Further currents trends, advantages and disadvantages and future prospects of these methods have been discussed.},
archivePrefix = {arXiv},
arxivId = {1011.1669},
author = {Shental, Noam and Hertz, Tomer and Weinshall, Daphna and Pavel, Misha},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {1011.1669},
file = {:Users/Walter/Documents/Literature/Adjustment{\_}Learning{\_}and{\_}Relevant{\_}Compone.pdf:pdf},
isbn = {9780874216561},
issn = {1098-6596},
journal = {Igarss 2014},
keywords = {Bott},
number = {1},
pages = {1--5},
pmid = {25246403},
title = {{Adjustment learning and relevant component analysis}},
year = {2014}
}
@article{Redmon2016,
abstract = {We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster RCNN with ResNet and SSD while still running significantly faster. Finally we propose a method to jointly train on object detection and classification. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. Our joint training allows YOLO9000 to predict detections for object classes that don't have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. But YOLO can detect more than just 200 classes; it predicts detections for more than 9000 different object categories. And it still runs in real-time.},
archivePrefix = {arXiv},
arxivId = {1612.08242},
author = {Redmon, Joseph and Farhadi, Ali},
eprint = {1612.08242},
file = {:Users/Walter/Documents/Literature/1612.08242v1.pdf:pdf},
title = {{YOLO9000: Better, Faster, Stronger}},
url = {http://arxiv.org/abs/1612.08242},
year = {2016}
}
@article{Wang2017,
abstract = {Recent successes in learning-based image classification, however, heavily rely on the large number of annotated training samples, which may require considerable human efforts. In this paper, we propose a novel active learning framework, which is capable of building a competitive classifier with optimal feature representation via a limited amount of labeled training instances in an incremental learning manner. Our approach advances the existing active learning methods in two aspects. First, we incorporate deep convolutional neural networks into active learning. Through the properly designed framework, the feature representation and the classifier can be simultaneously updated with progressively annotated informative samples. Second, we present a cost-effective sample selection strategy to improve the classification performance with less manual annotations. Unlike traditional methods focusing on only the uncertain samples of low prediction confidence, we especially discover the large amount of high confidence samples from the unlabeled set for feature learning. Specifically, these high confidence samples are automatically selected and iteratively assigned pseudo-labels. We thus call our framework "Cost-Effective Active Learning" (CEAL) standing for the two advantages.Extensive experiments demonstrate that the proposed CEAL framework can achieve promising results on two challenging image classification datasets, i.e., face recognition on CACD database [1] and object categorization on Caltech-256 [2].},
annote = {Perform active learning like normal, but add the most confident samples for the learning process},
archivePrefix = {arXiv},
arxivId = {1701.03551},
author = {Wang, Keze and Zhang, Dongyu and Li, Ya and Zhang, Ruimao and Lin, Liang},
doi = {10.1109/TCSVT.2016.2589879},
eprint = {1701.03551},
file = {:Users/Walter/Documents/Literature/1701.03551.pdf:pdf},
issn = {1051-8215},
pages = {1--10},
title = {{Cost-Effective Active Learning for Deep Image Classification}},
url = {http://arxiv.org/abs/1701.03551{\%}0Ahttp://dx.doi.org/10.1109/TCSVT.2016.2589879},
year = {2017}
}
@article{Weiss2004,
abstract = {Rare objects are often of great interest and great value. Until recently, however, rarity has not received much attention in the context of data mining. Now, as increasingly complex real-world problems are addressed, rarity, and the related problem of imbalanced data, are taking center stage. This article discusses the role that rare classes and rare cases play in data mining. The problems that can result from these two forms of rarity are described in detail, as are methods for addressing these problems. These descriptions utilize examples from existing research, so that this article provides a good survey of the literature on rarity in data mining. This article also demonstrates that rare classes and rare cases are very similar phenomena—both forms of rarity are shown to cause similar problems during data mining and benefit from the same remediation methods.},
author = {Weiss, Gary M},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Weiss - 2004 - Mining with Rarity A Unifying Framework.pdf:pdf},
journal = {ACM SIGKDD Explorations Newsletter 6.1},
keywords = {class imbalance,cost-sensitive learning,inductive bias,pling,rare cases,rare classes,sam-,small disjuncts},
number = {1},
pages = {7--19},
title = {{Mining with Rarity : A Unifying Framework}},
volume = {6},
year = {2004}
}
@inproceedings{B2005,
abstract = {In many practical optimization problems, evaluation of objectives and constraints often involve computationally expensive pro- cedures. To handle such problems, a metamodel-assisted approach is usually used to complete an optimization run in a reasonable amount of time. A metamodel is an approximate mathematical model of an objective or a constrained function which is constructed with a handful of solutions evaluated exactly. However, when comes to solving multi- objective optimization problems involving numerous constraints, it may be too much to metamodel each and every objective and constrained function independently. The cumulative effect of errors from each meta- model may turn out to be detrimental for the accuracy of the overall optimization procedure. In this paper, we propose a taxonomy of various metamodeling methodologies for multi-objective optimization and pro- vide a comparative study by discussing advantages and disadvantages of each method. The first results presented in this paper are obtained using the well-known Kriging metamodeling approach. Based on our proposed taxonomy and an extensive literature search, we also highlight new and promising methods for multi-objective metamodeling algorithms.},
archivePrefix = {arXiv},
arxivId = {9780201398298},
author = {Deb, Kalyanmoy and Hussein, Rayan and Roy, Proteek and Toscano, Gregorio},
booktitle = {International Conference on Evolutionary Multi-Criterion Optimization},
doi = {10.1007/b106458},
eprint = {9780201398298},
file = {:Users/Walter/Documents/Literature/Evolutionary multi-criterion optimization.pdf:pdf},
isbn = {978-3-540-24983-2},
issn = {03029743},
keywords = {Evolutionary multi- objective optimization,Kriging,Metamodel,Surrogate model,Taxonomy},
mendeley-tags = {Evolutionary multi- objective optimization,Kriging,Metamodel,Surrogate model,Taxonomy},
pages = {160--175},
pmid = {4520227},
title = {{Classifying Metamodeling Methods for Evolutionary Multi-objective Optimization: First Results}},
url = {http://link.springer.com/10.1007/b106458},
volume = {3410},
year = {2017}
}
@article{QiangYang2010,
author = {{Qiang Yang}},
doi = {10.1109/TKDE.2009.191},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Qiang Yang - 2010 - a Survey on Transfer Learning.pdf:pdf},
number = {10},
pages = {1--15},
title = {{a Survey on Transfer Learning}},
volume = {1},
year = {2010}
}
@article{Kim2006,
abstract = {In this paper, I propose a genetic algorithm (GA) approach to instance selection in artificial neural networks (ANNs) for financial data mining. ANN has preeminent learning ability, but often exhibit inconsistent and unpredictable performance for noisy data. In addition, it may not be possible to train ANN or the training task cannot be effectively carried out without data reduction when the amount of data is so large. In this paper, the GA optimizes simultaneously the connection weights between layers and a selection task for relevant instances. The globally evolved weights mitigate the well-known limitations of gradient descent algorithm. In addition, genetically selected instances shorten the learning time and enhance prediction performance. This study applies the proposed model to stock market analysis. Experimental results show that the GA approach is a promising method for instance selection in ANN.},
author = {Kim, Soo Young},
isbn = {1399-3054},
issn = {00319317},
journal = {Expert Systems with Applications},
keywords = {artificial neural networks,financial forecasting,genetic algorithms,instance selection},
number = {3},
pages = {519--526},
title = {{Artificial neural networks with evolutionary instance selection for financial forecasting, Expert Syst Appl}},
volume = {30},
year = {2006}
}
@article{Oates1997,
author = {Oates, T. and Jensen, D.},
journal = {ICML '97: Proceedings of the Fourteenth International Conference on Machine Learning},
pages = {254--262},
title = {{The Effects of Training Set Size on Decision Tree Complexity}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.53.7365{\&}rep=rep1{\&}type=pdf},
year = {1997}
}
@article{Tsai2014,
abstract = {Text classification is usually based on constructing a model through learning from training examples to automatically classify text documents. However, as the size of text document repositories grows rapidly, the storage requirement and computational cost of model learning become higher. Instance selection is one solution to solve these limitations whose aim is to reduce the data size by filtering out noisy data from a given training dataset. In this paper, we introduce a novel algorithm for these tasks, namely a biological-based genetic algorithm (BGA). BGA fits a "biological evolution" into the evolutionary process, where the most streamlined process also complies with the reasonable rules. In other words, after long-term evolution, organisms find the most efficient way to allocate resources and evolve. Consequently, we can closely simulate the natural evolution of an algorithm, such that the algorithm will be both efficient and effective. The experimental results based on the TechTC-100 and Reuters-21578 datasets show the outperformance of BGA over five state-of-the-art algorithms. In particular, using BGA to select text documents not only results in the largest dataset reduction rate, but also requires the least computational time. Moreover, BGA can make the k-NN and SVM classifiers provide similar or slightly better classification accuracy than GA. ?? 2014 Elsevier Inc.},
annote = {From Duplicate 4 (Evolutionary instance selection for text classification - Tsai, Chih Fong; Chen, Zong Yao; Ke, Shih Wen)

this is a poorly written paper

propose a new genetic algorithm for TSS, it is not clear how fitness is calculated, suspect it is by creating a classifier

the motivation does not seem to fit, for SVM at least, which they show to be the superior method for the text classification problems

their genetic algorithm seems neat},
author = {Tsai, Chih Fong and Chen, Zong Yao and Ke, Shih Wen and Hu, Ya Han and Lin, Wei Chao and Tsai, Chih Fong and Ke, Shih Wen and Chen, Chih Wen and Ramesh, B and Uysal, Alper Kursat and Gunal, Serkan},
doi = {10.1016/j.jss.2013.12.034},
file = {:Users/Walter/Documents/Literature/1-s2.0-S0164121214000077-main.pdf:pdf;:Users/Walter/Documents/Literature/ContentServer.pdf:pdf;:Users/Walter/Documents/Literature/Support vector machine using efficient instant selection for micro array data sets.pdf:pdf;:Users/Walter/Documents/Literature/The impact of preprocessing on text classification.pdf:pdf},
isbn = {5359777370},
issn = {01641212},
journal = {Journal of Systems and Software},
keywords = {Data preprocessing,Genetic algorithms,Instance selection,Pattern recognition,Text categorization,Text classification,Text preprocessing,breast cancer,classification,data,gene expression data,instance selection,medical data mining,protein homology,support vector machine},
number = {1},
pages = {104--113},
publisher = {Elsevier Inc.},
title = {{Evolutionary instance selection for text classification}},
url = {http://dx.doi.org/10.1016/j.ipm.2013.08.006 http://dx.doi.org/10.1016/j.jss.2013.12.034},
volume = {90},
year = {2014}
}
@article{Andrews1995,
abstract = {It is becoming increasingly apparent that, without some form of explanation capability, the full potential of trained artificial neural networks (ANNs) may not be realised. This survey gives an overview of techniques developed to redress this situation. Specifically, the survey focuses on mechanisms, procedures, and algorithms designed to insert knowledge into ANNs (knowledge initialisation), extract rules from trained ANNs (rule extraction), and utilise ANNs to refine existing rule bases (rule refinement). The survey also introduces a new taxonomy for classifying the various techniques, discusses their modus operandi, and delineates criteria for evaluating their efficacy.},
author = {Andrews, Robert and Diederich, Joachim and Tickle, Alan B.},
doi = {10.1016/0950-7051(96)81920-4},
issn = {09507051},
journal = {Knowledge-based systems},
number = {6},
pages = {373--389},
publisher = {Elsevier},
title = {{"Survey and critique of techniques for extracting rules from trained artificial neural networks.".}},
volume = {8},
year = {1995}
}
@article{Kim2016,
abstract = {Example-based explanations are widely used in the effort to improve the inter-pretability of highly complex distributions. However, prototypes alone are rarely sufficient to represent the gist of the complexity. In order for users to construct better mental models and understand complex data distributions, we also need criticism to explain what are not captured by prototypes. Motivated by the Bayesian model criticism framework, we develop MMD-critic which efficiently learns pro-totypes and criticism, designed to aid human interpretability. A human subject pilot study shows that the MMD-critic selects prototypes and criticism that are useful to facilitate human understanding and reasoning. We also evaluate the prototypes selected by MMD-critic via a nearest prototype classifier, showing competitive performance compared to baselines.},
author = {Kim, Been and Khanna, Rajiv and Koyejo, Oluwasanmi},
file = {:Users/Walter/Documents/Literature/KIM2016NIPS{\_}MMD.pdf:pdf},
issn = {10495258},
journal = {Nips 2016},
number = {Nips},
pages = {2280--2288},
pmid = {172823},
title = {{Examples are not Enough, Learn to Criticize! Criticism for Interpretability (slides)}},
year = {2016}
}
@article{Madigan1999,
author = {Madigan, David and Raftery, Adrian E and Volinsky, Chris T},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Madigan, Raftery, Volinsky - 1999 - Bayesian Model Averaging A Tutorial.pdf:pdf},
journal = {Statistical Science},
keywords = {and phrases,bayesian graphical,bayesian model averaging,learning,markov chain monte carlo,model uncertainty,models},
number = {4},
pages = {382--401},
title = {{Bayesian Model Averaging: A Tutorial}},
volume = {14},
year = {1999}
}
@article{Domingos1999,
abstract = {Research in machine learning, statistics and related fields has produced a wide variety of algorithms for classification. However, most of these algorithms assume that all errors have the same cost, which is seldom the case in KDD prob- lems. Individually making each classification learner costsensitive is laborious, and often non-trivial. In this paper we propose a principled method for making an arbitrary classifier cost-sensitive by wrapping a cost-minimizing procedure around it. This procedure, called MetaCost, treats the underlying classifier as a black box, requiring no knowledge of its functioning or change to it. Unlike stratification, MetaCost is applicable to any number of classes and to arbitrary cost matrices. Empirical trials on a large suite of benchmark databases show that MetaCost almost always produces large cost reductions compared to the cost-blind classifier used (C4.5RULES) and to two forms of stratification. Further tests identify the key components of MetaCost and those that can be varied without substantial loss. Experiments on a larger database indicate that MetaCost scales well.},
annote = {Metacost is a pretty cool way of doing cost sensitive sampling (sort of)

- currently chaning balance of classes in stratification is only way to accomplish cost sensitiveness
- want to incorporate cost of misclassifying one class as another, not just cost of misclassifying class
- pretty cute algormithm that learns bootstrapped models and then relables instances to match the optimal boundary, then relearns the base classifier
- claim DT is defacto
- good references for other methods},
author = {Domingos, Pedro},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Domingos - 1999 - MetaCost A General Method for Making Classifiers Cost-Sensitive.pdf:pdf},
journal = {Proceedings of the Fifth International Conference on Knowledge Discovery and Data Mining},
pages = {155--164},
title = {{MetaCost : A General Method for Making Classifiers Cost-Sensitive}},
year = {1999}
}
@article{Nakhaeizadeh2000,
abstract = {Today there are several efficient algorithms that cope with the popular and computationally expensive task of associ- ation rule mining. Actually, these algorithms are more or less described on their own. In this paper we explain the fundamentals of association rule mining and moreover de- rive a general framework. Based on this we describe to- day's approaches in context by pointing out common aspects and differences. After that we thoroughly investigate their strengths and weaknesses and carry out several runtime ex- periments. It turns out that the runtime behavior of the algorithms is much more similar as to be expected.},
author = {Nakhaeizadeh, Gholamreza and Hipp, Jochen and G{\"{u}}ntzer, Ulrich},
doi = {10.1145/360402.360421},
file = {:Users/Walter/Documents/Literature/Algorithms{\_}for{\_}Association{\_}Rule{\_}Mining{\_}-{\_}A{\_}General.pdf:pdf},
isbn = {1931-0145},
issn = {19310145},
journal = {ACM sigkdd explorations newsletter},
number = {1},
pages = {58--64},
title = {{Algorithms for association rule mining - a general survey and comparison}},
volume = {2},
year = {2000}
}
@article{Davidov2004,
abstract = {Although text categorization is a burgeoning area of IR research, readily available test collections in this field are surprisingly scarce. We describe a methodology and system (named ACCIO) for automatically acquiring labeled datasets for text categorization from the World Wide Web, by capitalizing on the body of knowledge encoded in the structure of existing hierarchical directories such as the Open Directory. We define parameters of categories that make it possible to acquire numerous datasets with desired properties, which in turn allow better control over categorization experiments. In particular, we develop metrics that estimate the difficulty of a dataset by examining the host directory structure. These metrics are shown to be good predictors of categorization accuracy that can be achieved on a dataset, and serve as efficient heuristics for generating datasets subject to user's requirements. A large collection of automatically generated datasets are made available for other researchers to use.},
author = {Davidov, Dmitry and Gabrilovich, E. and Markovitch, Shaul},
doi = {10.1145/1008992.1009036},
file = {:Users/Walter/Documents/Literature/accio.pdf:pdf},
isbn = {1581138814},
journal = {Proceedings of the 27th annual international ACM SIGIR conference on research and development in information retrieval},
pages = {250--257},
title = {{Parameterized generation of labeled datasets for text categorization based on a hierarchical directory}},
year = {2004}
}
@article{Keogh2005,
archivePrefix = {arXiv},
arxivId = {1201.2969},
author = {Keogh, Eamonn and Ratanamahatana, Ca},
doi = {10.1007/s10115-004-0154-9},
eprint = {1201.2969},
file = {:Users/Walter/Documents/Literature/KAIS04.pdf:pdf},
isbn = {9781920682828},
issn = {14451336},
journal = {Knowledge and Information Systems},
keywords = {Data mining,Dynamic time warping,Similarity measures,Time series},
pages = {358--386},
pmid = {15470472},
title = {{Exact indexing of dynamic time warping Eamonn}},
year = {2005}
}
@article{Hernandez-Leal2013,
abstract = {Instance selection algorithms are used for reducing the number of training instances. However, most of them suffer from long runtimes which results in the incapability to be used with large datasets. In this work, we introduce an Instance Ranking per class using Borders (instances near to instances belonging to different classes), using this ranking we propose an instance selection algorithm (IRB). We evaluated the proposed algorithm using k-NN with small and large datasets, comparing it against state of the art instance selection algorithms. In our experiments, for large datasets IRB has the best compromise between time and accuracy. We also tested our algorithm using SVM, LWLR and C4.5 classifiers, in all cases the selection computed by our algorithm obtained the best accuracies in average. ?? 2012 Elsevier Ltd All rights reserved.},
author = {Hernandez-Leal, Pablo and Carrasco-Ochoa, J. Ariel and Mart??nez-Trinidad, J. Fco and Olvera-Lopez, J. Arturo},
doi = {10.1016/j.patcog.2012.07.007},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Border instances,Instance ranking,Instance selection,Supervised classification},
number = {1},
pages = {365--375},
title = {{Instance Rank based on borders for instance selection}},
volume = {46},
year = {2013}
}
@inproceedings{Furafro2016,
author = {Furfaro, Roberto and Linares, Richard and Gaylor, David and Jah, Moriba and Walls, Ramona},
booktitle = {2016 Advanced Maui Optical and Space Surveillance Technologies Conference},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Furfaro et al. - 2016 - Resident Space Object Characterization and Behavior Understanding via Machine Learning and Ontology-based Bayesi.pdf:pdf},
number = {November},
title = {{Resident Space Object Characterization and Behavior Understanding via Machine Learning and Ontology-based Bayesian...}},
year = {2016}
}
@article{Hjelm2017,
abstract = {We introduce a novel approach to training generative adversarial networks, where we train a generator to match a target distribution that converges to the data distribution at the limit of a perfect discriminator. This objective can be interpreted as training a generator to produce samples that lie on the decision boundary of a current discriminator in training at each update, and we call a GAN trained using this algorithm a boundary-seeking GAN (BS-GAN). This approach can be used to train a generator with discrete output when the generator outputs a parametric conditional distribution. We demonstrate the effectiveness of the proposed algorithm with discrete image data. In contrary to the proposed algorithm, we observe that the recently proposed Gumbel-Softmax technique for re-parametrizing the discrete variables does not work for training a GAN with discrete data. Finally, we notice that the proposed boundary-seeking algorithm works even with continuous variables, and demonstrate its effectiveness with two widely used image data sets, SVHN and CelebA.},
annote = {I showed Eric this paper},
archivePrefix = {arXiv},
arxivId = {1702.08431},
author = {Hjelm, R Devon and Jacob, Athul Paul and Che, Tong and Cho, Kyunghyun and Bengio, Yoshua},
eprint = {1702.08431},
file = {:Users/Walter/Documents/Literature/BoundarySeekingGans.pdf:pdf},
title = {{Boundary-Seeking Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1702.08431},
year = {2017}
}
@article{Nushi2016,
abstract = {We study the problem of troubleshooting machine learning systems that rely on analytical pipelines of distinct components. Understanding and fixing errors that arise in such integrative systems is difficult as failures can occur at multiple points in the execution workflow. Moreover, errors can propagate, become amplified or be suppressed, making blame assignment difficult. We propose a human-in-the-loop methodology which leverages human intellect for troubleshooting system failures. The approach simulates potential component fixes through human computation tasks and measures the expected improvements in the holistic behavior of the system. The method provides guidance to designers about how they can best improve the system. We demonstrate the effectiveness of the approach on an automated image captioning system that has been pressed into real-world use.},
archivePrefix = {arXiv},
arxivId = {1611.08309},
author = {Nushi, Besmira and Kamar, Ece and Horvitz, Eric and Kossmann, Donald},
eprint = {1611.08309},
file = {:Users/Walter/Documents/Literature/15032-66359-1-PB.pdf:pdf},
keywords = {Humans and Artificial Intelligence},
pages = {1017--1025},
title = {{On Human Intellect and Machine Failures: Troubleshooting Integrative Machine Learning Systems}},
url = {http://arxiv.org/abs/1611.08309},
year = {2016}
}
@article{Rosenberg,
abstract = {We present V-measure, an external entropy-based cluster evaluation measure. V-measure provides an elegant solution to many problems that affect previously de-fined cluster evaluation measures includ-ing 1) dependence on clustering algorithm or data set, 2) the " problem of matching " , where the clustering of only a portion of data points are evaluated and 3) accurate evalu-ation and combination of two desirable as-pects of clustering, homogeneity and com-pleteness. We compare V-measure to a num-ber of popular cluster evaluation measures and demonstrate that it satisfies several de-sirable properties of clustering solutions, us-ing simulated clustering results. Finally, we use V-measure to evaluate two clustering tasks: document clustering and pitch accent type clustering.},
author = {Rosenberg, Andrew and Hirschberg, Julia},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Rosenberg, Hirschberg - 2007 - V-Measure A conditional entropy-based external cluster evaluation measure.pdf:pdf},
journal = {EMNLP-CoNLL},
pages = {410--420},
title = {{V-Measure: A conditional entropy-based external cluster evaluation measure}},
volume = {7},
year = {2007}
}
@article{Zhang2015,
abstract = {Frequent itemset mining is an essential step in the process of association rule mining. Conventional approaches for mining frequent itemsets in big data era encounter sig-nificant challenges when computing power and memory space are limited. This paper proposes an efficient distrib-uted frequent itemset mining algorithm (DFIMA) which can significantly reduce the amount of candidate itemsets by applying a matrix-based pruning approach. The proposed algorithm has been implemented using Spark to further improve the efficiency of iterative computation. Numeric experiment results using standard benchmark datasets by comparing the proposed algorithm with the existing algo-rithm, parallel FP-growth, show that DFIMA has better efficiency and scalability. In addition, a case study has been carried out to validate the feasibility of DFIMA.},
author = {Zhang, Feng and Liu, Min and Gui, Feng and Shen, Weiming and Shami, Abdallah and Ma, Yunlong},
doi = {10.1007/s10586-015-0477-1},
isbn = {1058601504},
issn = {15737543},
journal = {Cluster Computing},
keywords = {Big data,Distributed data mining algorithm,Frequent itemset mining,Spark},
number = {4},
pages = {1493--1501},
publisher = {Springer US},
title = {{A distributed frequent itemset mining algorithm using spark for big data analytics}},
volume = {18},
year = {2015}
}
@article{Zhang2017,
abstract = {Neural style transfer has drawn broad attention in recent years. However, most existing methods aim to explicitly model the transformation between different styles, and the learned model is thus not generalizable to new styles. We here attempt to separate the representations for styles and contents, and propose a generalized style transfer network consisting of style encoder, content encoder, mixer and decoder. The style encoder and content encoder are used to extract the style and content factors from the style reference images and content reference images, respectively. The mixer employs a bilinear model to integrate the above two factors and finally feeds it into a decoder to generate images with target style and content. To separate the style features and content features, we leverage the conditional dependence of styles and contents given an image. During training, the encoder network learns to extract styles and contents from two sets of reference images in limited size, one with shared style and the other with shared content. This learning framework allows simultaneous style transfer among multiple styles and can be deemed as a special `multi-task' learning scenario. The encoders are expected to capture the underlying features for different styles and contents which is generalizable to new styles and contents. For validation, we applied the proposed algorithm to the Chinese Typeface transfer problem. Extensive experiment results on character generation have demonstrated the effectiveness and robustness of our method.},
archivePrefix = {arXiv},
arxivId = {1711.06454},
author = {Zhang, Yexun and Zhang, Ya and Cai, Wenbin},
eprint = {1711.06454},
file = {:Users/Walter/Documents/Literature/1711.06454.pdf:pdf},
title = {{Separating Style and Content for Generalized Style Transfer}},
url = {http://arxiv.org/abs/1711.06454},
year = {2017}
}
@article{Baldi2000,
abstract = {We provide a unified overview of methods that currently are widely used to assess the accuracy of prediction algorithms, from raw percentages, quadratic error measures and other distances, and correlation coefficients, and to information theoretic measures such as relative entropy and mutual information. We briefly discuss the advantages and disadvantages of each approach. For classification tasks, we derive new learning algorithms for the design of prediction systems by directly optimising the correlation coefficient. We observe and prove several results relating sensitivity and specificity of optimal systems. While the principles are general, we illustrate the applicability on specific problems such as protein secondary structure and signal peptide prediction. Contact: pfbaldi@ics.uci.edu},
annote = {Ton of measurements protrayed by protein classification

- good bash on uninfomrative classifiers},
author = {Baldi, Pierre and Brunak, S{\o}ren and Chauvin, Yves and Andersen, Claus A F and Nielsen, Henrik},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Baldi et al. - 2000 - Assessing the accuracy of prediction algorithms for classification an overview.pdf:pdf},
journal = {Bioinformatics Review},
number = {5},
pages = {412--424},
title = {{Assessing the accuracy of prediction algorithms for classification: an overview}},
volume = {16},
year = {2000}
}
@article{Breiman1999,
abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Freund and Schapire[1996]), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
archivePrefix = {arXiv},
arxivId = {http://dx.doi.org/10.1023{\%}2FA{\%}3A1010933404324},
author = {Breiman, Leo (University of California)},
eprint = {/dx.doi.org/10.1023{\%}2FA{\%}3A1010933404324},
isbn = {9781424444427},
issn = {0885-6125},
journal = {Machine Learning},
number = {5},
pages = {1--35},
pmid = {20142443},
primaryClass = {http:},
title = {{Random forest}},
volume = {45},
year = {1999}
}
@article{Hand2009,
abstract = {The area under the ROC curve (AUC) is a very widely used measure of performance for classification and diagnostic rules. It has the appealing property of being objective, requiring no subjective input from the user. On the other hand, the AUC has disadvantages, some of which are well known. For example, the AUC can give potentially misleading results if ROC curves cross. However, the AUC also has a much more serious deficiency, and one which appears not to have been previously recognised. This is that it is fundamentally incoherent in terms of misclassification costs: the AUC uses different misclassification cost distributions for different classifiers. This means that using the AUC is equivalent to using different metrics to evaluate different classification rules. It is equivalent to saying that, using one classifier, misclassifying a class 1 point is p times as serious as misclassifying a class 0 point, but, using another classifier, misclassifying a class 1 point is P times as serious, where p≠P. This is nonsensical because the relative severities of different kinds of misclassifications of individual points is a property of the problem, not the classifiers which happen to have been chosen. This property is explored in detail, and a simple valid alternative to the AUC is proposed.},
annote = {




Summary






























AUC is used to assess the performance of classification rules, especialy in medicine, radiology, credit scoring etc. 








It is known that they are innefective for comparison when the curves being compared cross.








It is going to be shown that comparing classifiers with AUC is stupid. 






},
author = {Hand, David J.},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Hand - 2009 - Measuring classifier performance a coherent alternative to the area under the ROC curve.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {AUC,Classification,Cost,Error rate,Loss,Misclassification rate,ROC curves,Sensitivity,Specificity,auc,classification,cost,error rate,loss,misclassification,rate,roc curves,sensitivity,specificity},
mendeley-tags = {AUC,Classification,Cost,Error rate,Loss,Misclassification rate,ROC curves,Sensitivity,Specificity},
month = {jun},
number = {1},
pages = {103--123},
title = {{Measuring classifier performance: a coherent alternative to the area under the ROC curve}},
url = {http://link.springer.com/10.1007/s10994-009-5119-5},
volume = {77},
year = {2009}
}
@article{Iverson1998,
author = {Iverson, Louis and Prasad, Anantha},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Iverson, Prasad - 1998 - Predicting Abundance of 80 Tree Species Following Climate Change in the Eastern United States.pdf:pdf},
journal = {Ecological Monographs},
keywords = {climate change,envelope analysis,forest inventory,geographic information systems,gis,global change,landscape ecology,predictive vegetation mapping,regression tree analysis,rta,species-environment relationships,tree species,tree species distribution,tree species migration},
number = {4},
pages = {465--485},
title = {{Predicting Abundance of 80 Tree Species Following Climate Change in the Eastern United States.}},
volume = {68},
year = {1998}
}
@article{Cano2003,
abstract = {Evolutionary algorithms are adaptive methods based on natural evolution that may be used for search and optimization. As data reduction in knowledge discovery in databases (KDDs) can be viewed as a search problem, it could be solved using evolutionary algorithms (EAs). In this paper, we have carried out an empirical study of the performance of four representative EA models in which we have taken into account two different instance selection perspectives, the prototype selection and the training set selection for data reduction in KDD. This paper includes a comparison between these algorithms and other nonevolutionary instance selection algorithms. The results show that the evolutionary instance selection algorithms consistently outperform the nonevolutionary ones, the main advantages being: better instance reduction rates, higher classification accuracy, and models that are easier to interpret.},
author = {Cano, J.R. and Herrera, F. and Lozano, M.},
isbn = {1089-778X},
issn = {1089-778X},
journal = {IEEE Transactions on Evolutionary Computation},
number = {6},
pages = {561--575},
title = {{Using evolutionary algorithms as instance selection for data reduction in KDD: an experimental study}},
volume = {7},
year = {2003}
}
@article{Czarnowski2012,
abstract = {Instance selection in the supervised machine learning, often referred to as the data reduction, aims at deciding which instances from the training set should be retained for further use during the learning process. Instance selection can result in increased capabilities and generalization properties of the learning model, shorter time of the learning process, or it can help in scaling up to large data sources. The paper proposes a cluster-based instance selection approach with the learning process executed by the team of agents and discusses its four variants. The basic assumption is that instance selection is carried out after the training data have been grouped into clusters. To validate the proposed approach and to investigate the influence of the clustering method used on the quality of the classification, the computational experiment has been carried out.},
author = {Czarnowski, Ireneusz},
isbn = {0219-1377},
issn = {02191377},
journal = {Knowledge and Information Systems},
keywords = {Data mining,Instance selection,Machine learning,Multi-agent system},
number = {1},
pages = {113--133},
title = {{Cluster-based instance selection for machine classification}},
volume = {30},
year = {2012}
}
@article{Goldstein2013,
abstract = {This article presents Individual Conditional Expectation (ICE) plots, a tool for visualizing the model estimated by any supervised learning algorithm. Classical partial dependence plots (PDPs) help visualize the average partial relationship between the predicted response and one or more features. In the presence of substantial interaction effects, the partial response relationship can be heterogeneous. Thus, an average curve, such as the PDP, can obfuscate the complexity of the modeled relationship. Accordingly, ICE plots refine the partial dependence plot by graphing the functional relationship between the predicted response and the feature for individual observations. Specifically, ICE plots highlight the variation in the fitted values across the range of a covariate, suggesting where and to what extent heterogeneities might exist. In addition to providing a plotting suite for exploratory analysis, we include a visual test for additive structure in the data generating model. Through simulated examples and real data sets, we demonstrate how ICE plots can shed light on estimated models in ways PDPs cannot. Procedures outlined are available in the R package ICEbox.},
archivePrefix = {arXiv},
arxivId = {1309.6392},
author = {Goldstein, Alex and Kapelner, Adam and Bleich, Justin and Pitkin, Emil},
doi = {10.1080/10618600.2014.907095},
eprint = {1309.6392},
file = {:Users/Walter/Documents/Literature/1309.6392.pdf:pdf},
issn = {1061-8600},
journal = {Journal of Computational and Graphical Statistics},
pages = {1--22},
title = {{Peeking Inside the Black Box: Visualizing Statistical Learning with Plots of Individual Conditional Expectation}},
url = {https://www.ncbi.nlm.nih.gov/pubmed/27853627},
year = {2013}
}
@article{Skalak1994,
abstract = {With the goal of reducing computational costs without sacrificing accuracy,we describe two al- gorithms to find sets of prototypes for nearest neighbor classification. Here, the term “proto- types” refers to the reference instances used in a nearest neighbor computation—the instances with respect to which similarity is assessed in order to assign a class to a new data item. Both algorithms rely on stochastic techniques to search the space of sets of prototypes and are simple to implement. The first is a Monte Carlo sampling algorithm; the second applies random mutation hill climbing. On four datasets we show that only three or four prototypes sufficed to give pre- dictive accuracy equal or superior to a basic near- est neighbor algorithm whose run-time storage costswere approximately 10 to 200 times greater. We briefly investigate how randommutation hill climbing may be applied to select features and prototypes simultaneously. Finally, we explain the performance of the sampling algorithm on these datasets in terms of a statistical measure of the extent of clustering displayed by the target classes.},
author = {Skalak, David B},
doi = {10.1016/B978-1-55860-335-6.50043-X},
isbn = {9781558603356},
journal = {Baseline},
keywords = {air force o ce,in part by the,instance-based learning,of scienti c research,this work was supported,under contract 90-0359},
pages = {1--15},
title = {{Prototype and feature selection by sampling and random mutation hill climbing algorithms}},
year = {1994}
}
@article{Cortez2012,
annote = {Sensitivity analysis sampling methods

- Reference for better understanding and trust of DM through increased interpretability},
author = {Cortez, Paulo and Embrechts, Mark J},
file = {:Users/Walter/Documents/Literature/nsensitivity2.pdf:pdf},
journal = {Information Sciences},
keywords = {classification,dsi,edu,email addresses,embrem,input importance,mark,paulo cortez,pcortez,pt,regression,rpi,sensitivity analysis,supervised data mining,uminho,visualization},
title = {{Using sensitivity analysis and visualization techniques to open black box data mining models}},
year = {2012}
}
@article{Ferreira2013,
abstract = {As increasing volumes of urban data are captured and become available, new opportunities arise for data-driven analysis that can lead to improvements in the lives of citizens through evidence-based decision making and policies. In this paper, we focus on a particularly important urban data set: taxi trips. Taxis are valuable sensors and information associated with taxi trips can provide unprecedented insight into many different aspects of city life, from economic activity and human behavior to mobility patterns. But analyzing these data presents many challenges. The data are complex, containing geographical and temporal components in addition to multiple variables associated with each trip. Consequently, it is hard to specify exploratory queries and to perform comparative analyses (e.g., compare different regions over time). This problem is compounded due to the size of the data-there are on average 500,000 taxi trips each day in NYC. We propose a new model that allows users to visually query taxi trips. Besides standard analytics queries, the model supports origin-destination queries that enable the study of mobility across the city. We show that this model is able to express a wide range of spatio-temporal queries, and it is also flexible in that not only can queries be composed but also different aggregations and visual representations can be applied, allowing users to explore and compare results. We have built a scalable system that implements this model which supports interactive response times; makes use of an adaptive level-of-detail rendering strategy to generate clutter-free visualization for large results; and shows hidden details to the users in a summary through the use of overlay heat maps. We present a series of case studies motivated by traffic engineers and economists that show how our model and system enable domain experts to perform tasks that were previously unattainable for them.},
author = {Ferreira, Nivan and Poco, Jorge and Vo, Huy T. and Freire, Juliana and Silva, Claudio T.},
doi = {10.1109/TVCG.2013.226},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Ferreira et al. - 2013 - Visual exploration of big spatio-temporal urban data A study of New York city taxi trips.pdf:pdf},
isbn = {1077-2626},
issn = {10772626},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {NYC taxis,Spatio-temporal queries,urban data,visual exploration},
number = {12},
pages = {2149--2158},
pmid = {24051781},
title = {{Visual exploration of big spatio-temporal urban data: A study of New York city taxi trips}},
volume = {19},
year = {2013}
}
@article{Alcala-Fdez2011a,
abstract = {This work is related to the KEEL (Knowledge Extraction based on Evolutionary Learning) tool, an open source software that supports data management and a designer of experiments. KEEL pays special attention to the implementation of evolutionary learning and soft computing based techniques for Data Mining problems including regression, classification, clustering, pattern mining and so on. The aim of this paper is to present three new aspects of KEEL: KEELdataset, a data set repository which includes the data set partitions in the KEEL format and shows some results of algorithms in these data sets; some guidelines for including new algorithms in KEEL, helping the researchers to make their methods easily accessible to other authors and to compare the results of many approaches already included within the KEEL software; and a module of statistical procedures developed in order to provide to the researcher a suitable tool to contrast the results obtained in any experimental study.Acase of study is given to illustrate a complete case of application within this experimental analysis framework. {\textcopyright} 2011 Old City Publishing, Inc.},
author = {Alcal{\'{a}}-Fdez, J. and Fern{\'{a}}ndez, Alberto and Luengo, Julian and Derrac, Joaqu{\'{i}}n and Garc{\'{i}}a, S. and S{\'{a}}nchez, L. and Herrera, Francisco},
issn = {15423980},
journal = {Journal of Multiple-Valued Logic and Soft Computing},
keywords = {Data mining,Data set repository,Evolutionary algorithms,Java,Knowledge extraction,Machine learning,data mining,data set repository,evolutionary algorithms,java},
number = {2-3},
pages = {255--287},
title = {{KEEL data-mining software tool: Data set repository, integration of algorithms and experimental analysis framework}},
volume = {17},
year = {2011}
}
@article{Choudhury2015,
author = {Choudhury, S.R. and Mitra, P. and Giles, C.L.},
doi = {10.1145/2682571.2797085},
isbn = {9781450333078},
journal = {2015 ACM Symposium on Document Engineering, DocEng 2015},
keywords = {document analysis,figure extraction,pdf},
pages = {47--50},
title = {{Automatic Extraction of Figures from Scholarly Documents}},
year = {2015}
}
@article{Tramer2017,
abstract = {Adversarial examples are maliciously perturbed inputs designed to mislead machine learning (ML) models at test-time. Adversarial examples are known to transfer across models: a same perturbed input is often misclassified by different models despite being generated to mislead a specific architecture. This phenomenon enables simple yet powerful black-box attacks against deployed ML systems. In this work, we propose novel methods for estimating the previously unknown dimensionality of the space of adversarial inputs. We find that adversarial examples span a contiguous subspace of large dimensionality and that a significant fraction of this space is shared between different models, thus enabling transferability. The dimensionality of the transferred adversarial subspace implies that the decision boundaries learned by different models are eerily close in the input domain, when moving away from data points in adversarial directions. A first quantitative analysis of the similarity of different models' decision boundaries reveals that these boundaries are actually close in arbitrary directions, whether adversarial or benign. We conclude with a formal study of the limits of transferability. We show (1) sufficient conditions on the data distribution that imply transferability for simple model classes and (2) examples of tasks for which transferability fails to hold. This suggests the existence of defenses making models robust to transferability attacks---even when the model is not robust to its own adversarial examples.},
archivePrefix = {arXiv},
arxivId = {1704.03453},
author = {Tram{\`{e}}r, Florian and Papernot, Nicolas and Goodfellow, Ian and Boneh, Dan and McDaniel, Patrick},
eprint = {1704.03453},
file = {:Users/Walter/Documents/Literature/1704.03453.pdf:pdf},
pages = {1--16},
title = {{The Space of Transferable Adversarial Examples}},
url = {http://arxiv.org/abs/1704.03453},
year = {2017}
}
@article{Attenberg2015,
author = {Attenberg, Joshua and Ipeirotis, Panos and Provost, Foster},
doi = {10.1145/2700832},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Attenberg, Ipeirotis, Provost - 2015 - Beat the Machine.pdf:pdf},
issn = {19361955},
journal = {Journal of Data and Information Quality},
number = {1},
pages = {1--17},
title = {{Beat the Machine}},
url = {http://dl.acm.org/citation.cfm?doid=2742852.2700832},
volume = {6},
year = {2015}
}
@article{Arvanitidis2017,
abstract = {Deep generative models provide a systematic way to learn nonlinear data distributions, through a set of latent variables and a nonlinear "generator" function that maps latent points into the input space. The nonlinearity of the generator imply that the latent space gives a distorted view of the input space. Under mild conditions, we show that this distortion can be characterized by a stochastic Riemannian metric, and demonstrate that distances and interpolants are significantly improved under this metric. This in turn improves probability distributions, sampling algorithms and clustering in the latent space. Our geometric analysis further reveals that current generators provide poor variance estimates and we propose a new generator architecture with vastly improved variance estimates. Results are demonstrated on convolutional and fully connected variational autoencoders, but the formalism easily generalize to other deep generative models.},
archivePrefix = {arXiv},
arxivId = {1710.11379},
author = {Arvanitidis, Georgios and Hansen, Lars Kai and Hauberg, S{\o}ren},
doi = {10.1155/2018/8195208},
eprint = {1710.11379},
file = {:Users/Walter/Documents/Literature/1710.11379.pdf:pdf},
isbn = {9783901882760},
issn = {16879139},
number = {2},
pages = {1--15},
title = {{Latent Space Oddity: on the Curvature of Deep Generative Models}},
url = {http://arxiv.org/abs/1710.11379},
year = {2017}
}
@misc{eBird2003,
address = {Ithaca, New York},
author = {of Ornithology, Cornell Lab},
title = {{eBird Basic Dataset}},
year = {2013}
}
@article{Sun2007,
abstract = {Classification of data with imbalanced class distribution has posed a significant drawback of the performance attainable by most standard classifier learning algorithms, which assume a relatively balanced class distribution and equal misclassification costs. The significant difficulty and frequent occurrence of the class imbalance problem indicate the need for extra research efforts. The objective of this paper is to investigate meta-techniques applicable to most classifier learning algorithms, with the aim to advance the classification of imbalanced data. The AdaBoost algorithm is reported as a successful meta-technique for improving classification accuracy. The insight gained from a comprehensive analysis of the AdaBoost algorithm in terms of its advantages and shortcomings in tacking the class imbalance problem leads to the exploration of three cost-sensitive boosting algorithms, which are developed by introducing cost items into the learning framework of AdaBoost. Further analysis shows that one of the proposed algorithms tallies with the stagewise additive modelling in statistics to minimize the cost exponential loss. These boosting algorithms are also studied with respect to their weighting strategies towards different types of samples, and their effectiveness in identifying rare cases through experiments on several real world medical data sets, where the class imbalance problem prevails. {\textcopyright} 2007 Pattern Recognition Society.},
author = {Sun, Yanmin and Kamel, Mohamed S. and Wong, Andrew K C and Wang, Yang},
doi = {10.1016/j.patcog.2007.04.009},
isbn = {00313203},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {AdaBoost,Class imbalance problem,Classification,Cost-sensitive learning},
pages = {3358--3378},
pmid = {1796429094669450594},
title = {{Cost-sensitive boosting for classification of imbalanced data}},
volume = {40},
year = {2007}
}
@article{Ribeiro2016b,
abstract = {Understanding why machine learning models behave the way they do empowers both system designers and end-users in many ways: in model selection, feature engineering, in order to trust and act upon the predictions, and in more intuitive user interfaces. Thus, interpretability has become a vital concern in machine learning, and work in the area of interpretable models has found re-newed interest. In some applications, such models are as accurate as non-interpretable ones, and thus are preferred for their transparency. Even when they are not accurate, they may still be preferred when interpretability is of paramount importance. However, restricting machine learning to inter-pretable models is often a severe limitation. In this paper we argue for explaining machine learning predictions using model-agnostic approaches. By treating the machine learning models as black-box functions, these approaches provide crucial flexibility in the choice of models, explanations, and representations, improving debugging, com-parison, and interfaces for a variety of users and models. We also outline the main challenges for such methods, and review a recently-introduced model-agnostic explanation approach (LIME) that addresses these challenges.},
archivePrefix = {arXiv},
arxivId = {1606.05386},
author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
eprint = {1606.05386},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Ribeiro, Singh, Guestrin - 2016 - Model-Agnostic Interpretability of Machine Learning.pdf:pdf},
journal = {ICML Workshop on Human Interpretability in Machine Learning},
keywords = {interpretability, machine learning, comprehensibil},
number = {Whi},
title = {{Model-Agnostic Interpretability of Machine Learning}},
year = {2016}
}
@article{Kozak2015,
author = {Kozak, Mark P and Blount, Michael and Moscicki, Stacey and Cheung, Derrick},
file = {:Users/Walter/Documents/Winter2016/VentureFund2016/FA8750-12-C-0034 Final Rpt.pdf:pdf},
title = {{New Improved Models Based on Live Environment ( NIMBLE ) Final Report}},
year = {2015}
}
@article{Shen2004,
author = {Shen, Xiaoqiao and Lin, Yaping},
doi = {10.1109/ISIMP.2004.1434022},
isbn = {0-7803-8687-6},
journal = {Proceedings of 2004 International Symposium on Intelligent Multimedia, Video and Speech Processing, 2004.},
pages = {149--152},
title = {{Gene expression data classification using SVM-KNN classifier}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1434022},
year = {2004}
}
@article{Everson2006,
abstract = {The receiver operating characteristic (ROC) has become a standard tool for the analysis and comparison of classifiers when the costs of misclassification are unknown. There has been relatively little work, however, examining ROC for more than two classes. Here we discuss and present an extension to the standard two-class ROC for multi-class problems. We define the ROC surface for the Q-class problem in terms of a multi-objective optimisation problem in which the goal is to simultaneously minimise the Q(Q - 1) misclassification rates, when the misclassification costs and parameters governing the classifier's behaviour are unknown. We present an evolutionary algorithm to locate the Pareto front-the optimal trade-off surface between misclassifications of different types. The use of the Pareto optimal surface to compare classifiers is discussed and we present a straightforward multi-class analogue of the Gini coefficient. The performance of the evolutionary algorithm is illustrated on a synthetic three class problem, for both k-nearest neighbour and multi-layer perceptron classifiers. ?? 2005 Elsevier B.V. All rights reserved.},
author = {Everson, Richard M. and Fieldsend, Jonathan E.},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Evolutionary computation,Gini coefficient,Multiple objectives,Pareto optimality,Receiver operating characteristic},
pages = {918--927},
title = {{Multi-class ROC analysis from a multi-objective optimisation perspective}},
volume = {27},
year = {2006}
}
@article{Ting2002,
abstract = {We introduce an instance-weighting method to induce cost-sensitive trees. It is a generalization of the standard tree induction process where only the initial instance weights determine the type of tree to be induced-minimum error trees or minimum high cost error trees. We demonstrate that it can be easily adapted to an existing tree learning algorithm. Previous research provides insufficient evidence to support the idea that the greedy divide-and-conquer algorithm can effectively induce a truly cost-sensitive tree directly from the training data. We provide this empirical evidence in this paper. The algorithm incorporating the instance-weighting method is found to be better than the original algorithm in in of total misclassification costs, the number of high cost errors, and tree size two-class data sets. The instance-weighting method is simpler and more effective in implementation than a previous method based on altered priors},
annote = {Look I haven't read this},
author = {Ting, Kai Ming},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Ting - 2002 - An Instance-Weighting Method to Induce Cost-Sensitive Trees.pdf:pdf},
journal = {IEEE Transactions on Knowledge and Data Engineering},
number = {3},
pages = {659--665},
title = {{An Instance-Weighting Method to Induce Cost-Sensitive Trees}},
volume = {14},
year = {2002}
}
@incollection{Japkowicz2001,
abstract = {In a concept learning problem, imbalances in the distribution of the data can occur either between the two classes or within a single class. Yet, although both types of imbalances are known to affect negatively the performance of standard classifiers, methods for dealing with the class imbalance problem usually focus on rectifying the between-class imbalance problem, neglecting to address the imbalance occurring within each class. The purpose of this paper is to extend the simplest proposed approach for dealing with the between-class imbalance problem—random re—sampling in order to deal simultaneously with the two problems. Although re-sampling is not necessarily the best way to deal with problems of imbalance, the results reported in this paper suggest that addressing both problems simultaneously is beneficial and should be done by more sophisticated techniques as well.},
address = {Berlin Heidelberg},
annote = {- They make artificial data may be a good source},
author = {Japkowicz, Nathalie},
booktitle = {Advances in Artificial Intelligence},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Japkowicz - 2001 - Concept-Learning in the Presence of Between-Class and Within-Class Imbalances.pdf:pdf},
pages = {67--77},
publisher = {Springer},
title = {{Concept-Learning in the Presence of Between-Class and Within-Class Imbalances}},
year = {2001}
}
@article{Andrienko2003,
abstract = {Current software tools for visualization of spatio-temporal data, on the one hand, utilize the opportunities provided by modern computer technologies, on the other hand, incorporate the legacy from the conventional cartography. We have considered existing visualization-based techniques for exploratory analysis of spatio-temporal data from two perspectives: (1) what types of spatio-temporal data they are applicable to; (2) what exploratory tasks they can potentially support. The technique investigation has been based on an operational typology of spatio-temporal data and analytical tasks we specially devised for this purpose. The result of the study is a structured inventory of existing exploratory techniques related to the types of data and tasks they are appropriate for. This result is potentially helpful for data analysts - users of geovisualization tools: it provides guidelines for selection of proper exploratory techniques depending on the characteristics of data to analyze and the goals of analysis. At the same time the inventory as well as the suggested typology of tasks could be useful for tool designers and developers of various domain-specific geovisualization applications. The designers can, on the one hand, see what task types are insufficiently supported by the existing tools and direct their creative activities towards filling the gaps, on the other hand, use the techniques described as basic elements for building new, more sophisticated ones. The application developers can, on the one hand, use the task and data typology in the analysis of potential user needs, on the other hand, appropriately select and combine existing tools in order to satisfy these needs. ?? 2003 Elsevier Ltd. All rights reserved.},
author = {Andrienko, Natalia and Andrienko, Gennady and Gatalsky, Peter},
doi = {10.1016/S1045-926X(03)00046-6},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Andrienko, Andrienko, Gatalsky - 2003 - Exploratory spatio-temporal visualization An analytical review.pdf:pdf},
isbn = {1045-926X},
issn = {1045926X},
journal = {Journal of Visual Languages and Computing},
number = {6},
pages = {503--541},
title = {{Exploratory spatio-temporal visualization: An analytical review}},
volume = {14},
year = {2003}
}
@article{KubatM.Matwin1997,
abstract = {Adding examples of the majority class to the training set can have a detrimental effect on the learner's behavior: noisy or otherwise unreliable examples from the majority class can overwhelm the minority class. The paper discusses criteria to evaluate the utility of classifiers induced from such imbalanced training sets, gives explanation of the poor behavior of some learners under these circumstances, and suggests as a solution a simple technique called one-sided selection of...},
annote = {Sampling and review of problem

- Sources for imbalanced datasets
- Argument for new measures
- Good example of ROC curve and tradeoff between error rates for a classifier 
- Good argument for imbalanced difficulty with k-NN
- Good argument for imbalanced difficulty with Bayseian
- Good argument for imbalanced difficulty with decision tree
- Argue that the learner always keep the minority instances as they are too rare to sacrafice
- discusses issues that the majority may exhibit
- have an experimental procedure to consider

References to consider:

Fisher 1936 - classification 
Tomek 1976 - one sided sampling
Lewis and Gale 1994 - F measure 
Kononenko and Bratko 1991 - information based measure
Swetts 1998 - ROC curve and biasing classifiers 
Kubat, Holte, and Matwin - Gmean
DeRouin et al. 1991 - neural network imbalanced
Pazzani et al 1994 - weighting training instances 
Gordon and Perlis 1989 - misclassification costs 
Catlett 1991 - windowing and bootstrapping imbalanced
Sung and Poggio 1995 - windowing and bootstrapping imbalanced
Lewis and Catlett 1994 - heterogeneous sampling imbalanced
ezawa et al. 1996 - attribute selection for imbalance
Dietterich, Lathrop and Lozano-Perez 1997 - scarcity of data

Selection literature
=============

Hart 1968
Gates 1972
Tomek 1976
Aha, Kibler, and Albert 1991
Zhang 1992
Skalak 1994
Lewis and Gale 1994
Floyd and Warmuth 1995},
author = {{Kubat, M., Matwin}, S.},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Kubat, M., Matwin - 1997 - Addressing the curse of imbalanced training sets one-sided selection.pdf:pdf},
isbn = {1-55860-486-3},
journal = {Icml},
pages = {179--186},
pmid = {297143},
title = {{Addressing the curse of imbalanced training sets: one-sided selection}},
volume = {97},
year = {1997}
}
@inproceedings{Domingos1998,
annote = {When to implement a machine learning algorithm},
author = {Domingos, P},
booktitle = {Proceedings of the {\{}AAAI-98/ICML-98{\}} {\{}W{\}}orkshop on the {\{}M{\}}ethodology of {\{}A{\}}pplying {\{}M{\}}achine {\{}L{\}}earning},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Domingos - 1998 - How to Get a Free Lunch A Simple Cost Model for Machine Learning Applications.pdf:pdf},
pages = {1--7},
title = {{How to Get a Free Lunch: A Simple Cost Model for Machine Learning Applications}},
year = {1998}
}
@article{Giannella2003,
abstract = {Although frequent-pattern mining has been widely studied and used, it is challenging to extend it to data streams. Compared to mining from a static transaction data set, the streaming case has far more information to track and far greater complexity to manage. Infrequent items can become frequent later on and hence cannot be ignored. The storage structure needs to be dynamically adjusted to reflect the evolution of itemset frequencies over time.},
author = {Giannella, Chris and Han, Jiawei and Pei, Jian and Yan, Xifeng and Yu, Philip S},
doi = {10.1.1.14.2327},
journal = {Next generation data mining},
pages = {191--212},
title = {{Mining Frequent Patterns in Data Streams at Multiple Time Granularities}},
url = {http://web.engr.illinois.edu/{~}hanj/pdf/fpstm03.pdf},
year = {2003}
}
@article{Stock2017,
abstract = {ConvNets and Imagenet have driven the recent success of deep learning for image classification. However, the marked slowdown in performance improvement, the recent studies on the lack of robustness of neural networks to adversarial examples and their tendency to exhibit undesirable biases (e.g racial biases) questioned the reliability and the sustained development of these methods. This work investigates these questions from the perspective of the end-user by using human subject studies and explanations. We experimentally demonstrate that the accuracy and robustness of ConvNets measured on Imagenet are underestimated. We show that explanations can mitigate the impact of misclassified adversarial examples from the perspective of the end-user and we introduce a novel tool for uncovering the undesirable biases learned by a model. These contributions also show that explanations are a promising tool for improving our understanding of ConvNets' predictions and for designing more reliable models},
annote = {A lot going on in this paper

- What is a systematic way to uncover bias?
- In my mind "bias" == "failure state"},
archivePrefix = {arXiv},
arxivId = {1711.11443},
author = {Stock, Pierre and Cisse, Moustapha},
eprint = {1711.11443},
file = {:Users/Walter/Documents/Literature/1711.11443.pdf:pdf},
title = {{ConvNets and ImageNet Beyond Accuracy: Explanations, Bias Detection, Adversarial Examples and Model Criticism}},
url = {http://arxiv.org/abs/1711.11443},
year = {2017}
}
@article{Jurafsky2009,
abstract = {This book takes an empirical approach to language processing, based on applying statistical and other machine-learning algorithms to large corpora. Methodology boxes are included in each chapter. Each chapter is built around one or more worked examples to demonstrate the main idea of the chapter. Covers the fundamental algorithms of various fields, whether originally proposed for spoken or written language to demonstrate how the same algorithm can be used for speech recognition and word-sense disambiguation. Emphasis on web and other practical applications. Emphasis on scientific evaluation. Useful as a reference for professionals in any of the areas of speech and language processing.},
author = {Jurafsky, Daniel and Martin, James H},
doi = {10.1162/089120100750105975},
file = {:Users/Walter/Documents/Literature/jurafsky{\_}martin.pdf:pdf},
isbn = {0130950696},
issn = {08912017},
journal = {Speech and Language Processing An Introduction to Natural Language Processing Computational Linguistics and Speech Recognition},
pages = {0--934},
title = {{Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition}},
url = {http://www.mitpressjournals.org/doi/pdf/10.1162/089120100750105975},
volume = {21},
year = {2009}
}
@article{He2009,
abstract = {With the continuous expansion of data availability in many large-scale, complex, and networked systems, such as surveillance, security, Internet, and finance, it becomes critical to advance the fundamental understanding of knowledge discovery and analysis from raw data to support decision-making processes. Although existing knowledge discovery and data engineering techniques have shown great success in many real-world applications, the problem of learning from imbalanced data (the imbalanced learning problem) is a relatively new challenge that has attracted growing attention from both academia and industry. The imbalanced learning problem is concerned with the performance of learning algorithms in the presence of underrepresented data and severe class distribution skews. Due to the inherent complex characteristics of imbalanced data sets, learning from such data requires new understandings, principles, algorithms, and tools to transform vast amounts of raw data efficiently into information and knowledge representation. In this paper, we provide a comprehensive review of the development of research in learning from imbalanced data. Our focus is to provide a critical review of the nature of the problem, the state-of-the-art technologies, and the current assessment metrics used to evaluate learning performance under the imbalanced learning scenario. Furthermore, in order to stimulate future research in this field, we also highlight the major opportunities and challenges, as well as potential important research directions for learning from imbalanced data.},
author = {He, Haibo and Garcia, Edwardo a.},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/He, Garcia - 2009 - Learning from imbalanced data.pdf:pdf},
journal = {IEEE Transactions on Knowledge and Data Engineering},
keywords = {Active learning,Assessment metrics,Classification,Cost-sensitive learning,Imbalanced learning,Kernel-based learning,Sampling methods},
number = {9},
pages = {1263--1284},
title = {{Learning from imbalanced data}},
volume = {21},
year = {2009}
}
@phdthesis{Weiss2003a,
abstract = {The main goal of classifier learning is to generate a model that makes few misclassification errors. Given this emphasis on error minimization, it makes sense to try to under-stand how the induction process gives rise to classifiers that make errors and whether wecan identify those parts of the classifier that generate most of the errors. In this thesis weprovide the first comprehensive studies of two major sources of classification errors. Thefirst study concerns small disjuncts, which are those disjuncts within a classifier thatcover only a few training examples. An analysis of classifiers induced from thirty datasets shows that these small disjuncts are extremely error prone and often account for themajority of all classification errors. Because small disjuncts largely determine classifierperformance, we use them as a "lens" through which to study classifier induction. Factorssuch as pruning, training-set size, noise and class imbalance are each analyzed to determine how they affect small disjuncts and, more generally, classifier learning.The second study analyzes the effect that rare classes and class distribution have onlearning. Those examples belonging to rare classes are shown to be misclassified muchmore often than common classes. The thesis then goes on to analyze the impact that Page 4 iii varying the class distribution of the training data has on classifier performance. The experimental results indicate that the naturally occurring class distribution is not always best for learning and that a balanced class distribution should be chosen to generate aclassifier robust to different misclassification costs. It is often necessary to limit theamount of training data used for learning, due to the costs associated with obtaining andlearning from this data. This thesis presents a budget-sensitive progressive-sampling al-gorithm for selecting training examples in this situation. This algorithm is shown to pro-duce a class distribution that performs quite well for learning (i.e., is near optimal)},
author = {Weiss, Gary Mitchell},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Weiss - 2003 - THE EFFECT OF SMALL DISJUNCTS AND.pdf:pdf},
school = {Rutgers, The State University of New Jersey},
title = {{THE EFFECT OF SMALL DISJUNCTS AND}},
year = {2003}
}
@article{TinKamHo1995,
abstract = {Decision trees are attractive classifiers due to their high execution speed. But trees derived with traditional methods often cannot be grown to arbitrary complexity for possible loss of generalization accuracy on unseen data. The limitation on complexity usually means suboptimal accuracy on training data. Following the principles of stochastic modeling, we propose a method to construct tree-based classifiers whose capacity can be arbitrarily expanded for increases in accuracy for both training and unseen data. The essence of the method is to build multiple trees in randomly selected subspaces of the feature space. Trees in, different subspaces generalize their classification in complementary ways, and their combined classification can be monotonically improved. The validity of the method is demonstrated through experiments on the recognition of handwritten digits},
author = {{Tin Kam Ho}},
doi = {10.1109/ICDAR.1995.598994},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Tin Kam Ho - 1995 - Random decision forests.pdf:pdf},
isbn = {0-8186-7128-9},
journal = {Proceedings of 3rd International Conference on Document Analysis and Recognition},
keywords = {Classification tree analysis,Decision trees,Handwriting recognition,Hidden Markov models,Multilayer perceptrons,Optimization methods,Stochastic processes,Testing,Tin,Training data,complexity,decision theory,generalization accuracy,handwritten digits,optical character recognition,random decision forests,stochastic modeling,suboptimal accuracy,tree-based classifiers},
pages = {278--282},
title = {{Random decision forests}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=598994},
volume = {1},
year = {1995}
}
@article{Aggarwal2012,
abstract = {The problem of classification has been widely studied in the data mining, machine learning, database, and information retrieval communities with applications in a number of diverse domains, such as target marketing, medical diagnosis, news group filtering, and document organization. In this paper we will provide a survey of a wide variety of text classification algorithms},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Aggarwal, CharuC. and Zhai, ChengXiang},
doi = {10.1007/978-1-4614-3223-4_6},
eprint = {arXiv:1011.1669v3},
file = {:Users/Walter/Documents/Literature/textminingSurvey.pdf:pdf},
isbn = {978-1-4614-3222-7},
issn = {1098-6596},
journal = {Mining Text Data},
keywords = {Text Classification},
pages = {163--222},
pmid = {25246403},
title = {{A Survey of Text Classification Algorithms}},
url = {http://dx.doi.org/10.1007/978-1-4614-3223-4{\_}6},
year = {2012}
}
@article{Verbiest2015,
abstract = {One of the most powerful, popular and accurate classification techniques is support vector machines (SVMs). In this work, we want to evaluate whether the accuracy of SVMs can be further improved using training set selection (TSS), where only a subset of training instances is used to build the SVM model. By contrast to existing approaches, we focus on wrapper TSS techniques, where candidate subsets of training instances are evaluated using the SVM training accuracy. We consider five wrapper TSS strategies and show that those based on evolutionary approaches can significantly improve the accuracy of SVMs.},
author = {Verbiest, Nele and Derrac, Joaquin and Cornelis, Chris and Garcia, Salvador and Herrera, Francisco},
isbn = {1568-4946},
issn = {15684946},
journal = {Applied Soft Computing Journal},
keywords = {Data reduction,Support vector machines,Training set selection},
pages = {10--22},
publisher = {Elsevier B.V.},
title = {{Evolutionary wrapper approaches for training set selection as preprocessing mechanism for support vector machines: Experimental evaluation and support vector analysis}},
volume = {38},
year = {2015}
}
@article{LeCun2010,
abstract = {Intelligent tasks, such as visual perception, auditory perception, and language understanding require the construction of good internal representations of the world (or "features")? which must be invariant to irrelevant variations of the input while, preserving relevant information. A major question for Machine Learning is how to learn such good features automatically. Convolutional Networks (ConvNets) are a biologically-inspired trainable architecture that can learn invariant features. Each stage in a ConvNets is composed of a filter bank, some nonlinearities, and feature pooling layers. With multiple stages, a ConvNet can learn multi-level hierarchies of features. While ConvNets have been successfully deployed in many commercial applications from OCR to video surveillance, they require large amounts of labeled training samples. We describe new unsupervised learning algorithms, and new non-linear stages that allow ConvNets to be trained with very few labeled samples. Applications to visual object recognition and vision navigation for off-road mobile robots are described.},
author = {LeCun, Yann and Kavukcuoglu, Koray and Farabet, Cl{\'{e}}ment},
doi = {10.1109/ISCAS.2010.5537907},
isbn = {9781424453085},
issn = {02714302},
journal = {ISCAS 2010 - 2010 IEEE International Symposium on Circuits and Systems: Nano-Bio Circuit Fabrics and Systems},
pages = {253--256},
title = {{Convolutional networks and applications in vision}},
year = {2010}
}
@article{Buja2009,
abstract = {We propose to furnish visual statistical methods with an inferential framework and protocol, modelled on confirmatory statistical testing. In this framework, plots take on the role of test statistics, and human cognition the role of statistical tests. Statistical significance of 'discoveries' is ...},
author = {Buja, A and Cook, D and Hofmann, H and Lawrence, M and Lee, E K and Swayne, D F and Wickham, H},
doi = {10.1098/rsta.2009.0120},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Buja et al. - 2009 - Statistical inference for exploratory data analysis and model diagnostics.pdf:pdf},
issn = {1364-503X, 1471-2962},
journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
keywords = {0120 or via,10,1098,2009,author for correspondence,available at http,cognitive perception,dicook,doi,dx,edu,electronic supplementary material is,iastate,org,permutation tests,rotation tests,rsta,simulation,statistical graphics,visual data mining},
number = {1906},
pages = {4361--4383},
pmid = {19805449},
title = {{Statistical inference for exploratory data analysis and model diagnostics}},
url = {http://rsta.royalsocietypublishing.org/cgi/doi/10.1098/rsta.2009.0120{\%}5Cnpapers2://publication/doi/10.1098/rsta.2009.0120},
volume = {367},
year = {2009}
}
@article{Wang2004,
author = {Wang, G. Gary and Shan, Songqing},
doi = {10.4271/2004-01-0240},
isbn = {0768013194},
journal = {SAE Transactions, Journal of Materials and Manufacturing},
pages = {101--110},
title = {{Design Space Reduction for Multi-Objective Optimization and Robust Design Optimization Problems}},
url = {http://www.sae.org/technical/papers/2004-01-0240},
year = {2004}
}
@article{Rubenstein2012,
abstract = {In current robotics research there is a vast body of work on algorithms and control methods for groups of decentralized cooperating robots, called a swarm or collective. These algorithms are generally meant to control collectives of hundreds or even thousands of robots; however, for reasons of cost, time, or complexity, they are generally validated in simulation only, or on a group of a few tens of robots. To address this issue, this paper presents Kilobot, a low-cost robot designed to make testing collective algorithms on hundreds or thousands of robots accessible to robotics researchers. To enable the possibility of large Kilobot collectives where the number of robots is an order of magnitude larger than the largest that exist today, each robot is made with only {\$}14 worth of parts and takes 5 minutes to assemble. Furthermore, the robot design allows a single user to easily operate a large Kilobot collective, such as programming, powering on, and charging all robots, which would be difficult or impossible to do with many existing robotic systems},
author = {Rubenstein, Michael and Ahler, Christian and Nagpal, Radhika},
journal = {International Conference on Robotics and Automation (ICRA)},
title = {{Kilobot : A Low Cost Scalable Robot System for Collective Behaviors}},
year = {2012}
}
@article{Demsar2006,
abstract = {Abstract While methods for comparing two learning algorithms on a single data set have been scrutinized for quite some time already, the issue of statistical tests for comparisons of more algorithms on multiple data sets, which is even more essential to typical machine learning studies, has been all but ignored. This article reviews the current practice and then theoretically and empirically examines several suitable tests. Based on that, we recommend a set of simple, yet safe and robust non-parametric tests for statistical comparisons of classifiers: the Wilcoxon signed ranks test for comparison of two classifiers and the Friedman test with the corresponding post-hoc tests for comparison of more classifiers over multiple data sets. Results of the latter can also be neatly presented with the newly introduced CD (critical difference) diagrams. Keywords: comparative studies, statistical methods, Wilcoxon signed ranks test, Friedman test, multiple comparisons tests},
annote = {THIS IS MY EXPERIMENTAL DESIGN},
author = {Dem{\v{s}}ar, J},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Dem{\v{s}}ar, Dem{\v{s}}ar - 2006 - Statistical Comparisons of Classifiers over Multiple Data Sets.pdf:pdf},
isbn = {9781424450404},
issn = {1532-4435},
journal = {The Journal of Machine Learning Research},
keywords = {comparative studies,friedman test,multiple comparisons tests,statistical methods,wilcoxon signed ranks test},
pages = {1--30},
pmid = {20451214},
title = {{Statistical Comparisons of Classifiers over Multiple Data Sets}},
volume = {7},
year = {2006}
}
@article{DeHaro-Garcia2009,
abstract = {Abstract  Instance selection is becoming more and more relevant due to the huge amount of data that is being constantly produced. However, although current algorithms are useful for fairly large datasets, scaling problems are found when the number of instances is of hundreds of thousands or millions. In the best case, these algorithms are of efficiency O(n 2), n being the number of instances. When we face huge problems, scalability is an issue, and most algorithms are not applicable. This paper presents a divide-and-conquer recursive approach to the problem of instance selection for instance based learning for very large problems. Our method divides the original training set into small subsets where the instance selection algorithm is applied. Then the selected instances are rejoined in a new training set and the same procedure, partitioning and application of an instance selection algorithm, is repeated. In this way, our approach is based on the philosophy of divide-and-conquer applied in a recursive manner. The proposed method is able to match, and even improve, for the case of storage reduction, the results of well-known standard algorithms with a very significant reduction of execution time. An extensive comparison in 30 datasets form the UCI Machine Learning Repository shows the usefulness of our method. Additionally, the method is applied to 5 huge datasets with from 300,000 to more than a million instances, with very good results and fast execution time.},
author = {{De Haro-Garc{\'{i}}a}, Aida and Garc{\'{i}}a-Pedrajas, Nicol{\'{a}}s},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/De Haro-Garc{\'{i}}a, Garc{\'{i}}a-Pedrajas - 2009 - A divide-and-conquer recursive approach for scaling up instance selection algorithms.pdf:pdf},
issn = {13845810},
journal = {Data Mining and Knowledge Discovery},
keywords = {Divide-and-conquer,Instance based learning,Instance selection,Scalability},
pages = {392--418},
title = {{A divide-and-conquer recursive approach for scaling up instance selection algorithms}},
volume = {18},
year = {2009}
}
@article{Bottino2012,
abstract = {The study of human attractiveness with pattern analysis techniques is an emerging research field. One still largely unresolved problem is which are the facial features relevant to attractiveness, how they combine together, and the number of independent parameters required for describing and identifying harmonious faces. In this paper, we present a first study about this problem, applied to face profiles. First, according to several empirical results, we hypothesize the existence of two well separated manifolds of attractive and unattractive face profiles. Then, we analyze with manifold learning techniques their intrinsic dimensionality. Finally, we show that the profile data can be reduced, with various techniques, to the intrinsic dimensions, largely without loosing their ability to discriminate between attractive and unattractive faces. {\textcopyright} 2012 Springer-Verlag.},
author = {Bottino, Andrea and Laurentini, Aldo},
doi = {10.1007/978-3-642-33275-3},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Bottino, Laurentini - 2012 - Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications.pdf:pdf},
isbn = {978-3-642-33274-6},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {dimensionality reduction,facial attractiveness,intrinsic dimensionality,manifold learning,profiles},
number = {NOVEMBER 2013},
pages = {59--66},
title = {{Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84865581069{\&}partnerID=tZOtx3y1},
volume = {7441},
year = {2012}
}
@article{Weiss2003,
abstract = {For large, real-world inductive learning problems, the number of training examples oftenmust be limited due to the costs associated with procuring, preparing, and storing the trainingexamples and/or the computational costs associated with learning from them. In such circum-stances, one question of practical importance is: if only n training examples can be selected,in what proportion should the classes be represented? In this article we help to answer thisquestion by analyzing, for a fixed training-set size, the relationship between the class distribu-tion of the training data and the performance of classification trees induced from these data.We study twenty-six data sets and, for each, determine the best class distribution for learning. The naturally occurring class distribution is shown to generally perform well when classifierperformance is evaluated using undifferentiated error rate (0/1 loss). However, when the areaunder the ROC curve is used to evaluate classifier performance, a balanced distribution isshown to perform well. Since neither of these choices for class distribution always generatesthe best-performing classifier, we introduce a “budget-sensitive” progressive sampling algo-rithm for selecting training examples based on the class associated with each example. Anempirical analysis of this algorithm shows that the class distribution of the resulting trainingset yields classifiers with good (nearly-optimal) classification performance},
author = {Weiss, Gary M and Provost, Foster},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Weiss, Provost - 2003 - Learning When Training Data are Costly The Effect of Class Distribution on Tree Induction.pdf:pdf},
journal = {Journal of Artificial Intelligence Research},
keywords = {Changes and compilation copyright {\textcopyright} The American A,Copyright {\textcopyright} 2003 AI Access},
pages = {315--354},
title = {{Learning When Training Data are Costly : The Effect of Class Distribution on Tree Induction}},
volume = {19},
year = {2003}
}
@article{Garcia2006a,
abstract = {In real-world applications, it has been often observed that class imbalance (significant differences in class prior probabilities) may produce an important deterioration of the classifier performance, in particular with patterns belonging to the less represented classes. This effect becomes especially significant on instance-based learning due to the use of some dissimilarity measure. We analyze the effects of class imbalance on the classifier performance and how the overlap has influence on such an effect, as well as on several techniques proposed in the literature to tackle the class imbalance. Besides, we study how these methods affect to the performance on both classes, not only on the minority class as usual.},
author = {Garc{\'{i}}a, V and Alejo, R and S{\'{a}}nchez, J S and Sotoca, J M and Mollineda, R A},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Garc{\'{i}}a et al. - 2006 - Combined Effects of Class Imbalance and Class Overlap on Instance-Based Classification.pdf:pdf},
journal = {Intelligent Data Engineering and Automated Learning - IDEAL 2006},
pages = {371--378},
title = {{Combined Effects of Class Imbalance and Class Overlap on Instance-Based Classification}},
year = {2006}
}
@article{Jia2017,
abstract = {In this paper, a fused feature vector consisting of raw time series and texture feature information is proposed for space object classification. The time series data includes historical orbit trajectories and asteroid light curves. The texture feature is derived from recurrence plots using Gabor filters for both unsupervised learning and supervised learning algorithms. The simulation results show that the classification algorithms using the fused feature vector achieve better performance than those using raw time series or texture features only.},
author = {Jia, Bin and Pham, Khanh D and Blasch, Erik and Shen, Dan and Wang, Zhonghai and Chen, Genshe},
file = {:Users/Walter/Documents/Literature/Jia.pdf:pdf},
title = {{Space Object Classification using Fused Features of Time Series Data}},
url = {https://amostech.com/TechnicalPapers/2017/Poster/Jia.pdf},
year = {2017}
}
@article{Wilhelm2001,
abstract = {This paper provides a technical review of topics relevant to applying column generation methods to solve integer programs but emphasizes formulation issues as a means of achieving its goal, which is to bridge the gap between methodological development and application. Type I, II and III column generation approaches are described in detail and each is demonstrated by a set of prototypical formulations that provide a historical perspective of milestone contributions. Technical issues, including formulation, context, algorithm design and implementation are also related. Formulation issues encompass the restricted master problem (RMP) and sub- problem (SP) structure, symmetry, complexity and the Integrality Property. Context issues comprise theoretical principles, dealing with binary or general integer variables and generating rows as well as columns. Algorithm design issues include branching strategies, SP solution strategies and problem-specific techniques. Implementation issues include determining an initial basic feasible solution, managing a pool of generated columns, optimizing the RMP at each iteration, and handling degeneracy and tailing off.},
author = {Wilhelm, We},
issn = {1389-4420},
journal = {Optimization and Engineering},
keywords = {column generation,integer programming},
pages = {159--200},
title = {{A technical review of column generation in integer programming}},
url = {http://link.springer.com/article/10.1023/A:1013141227104},
volume = {2},
year = {2001}
}
@article{Turner2016,
abstract = {We propose a general model explanation system (MES) for “explaining” the output of black box classifiers. In this introduction we use the motivating example of a classifier trained to detect fraud in a credit card transaction history. The key aspect is that we provide explanations applicable to a single prediction, rather than provide an interpretable set of parameters. The labels in the provided examples are usually negative. Hence, we focus on explaining positive predictions (alerts).},
archivePrefix = {arXiv},
arxivId = {1606.09517},
author = {Turner, Ryan},
doi = {10.1109/MLSP.2016.7738872},
eprint = {1606.09517},
file = {:Users/Walter/Documents/Literature/Turner2015{\_}MES.pdf:pdf},
isbn = {9781509007462},
issn = {21610371},
journal = {IEEE International Workshop on Machine Learning for Signal Processing, MLSP},
pages = {1--5},
title = {{A model explanation system}},
volume = {2016-Novem},
year = {2016}
}
@article{Ramesh2015,
abstract = {Machine learning techniques is most commonly used technique in text mining. Support Vector Machine (SVM) is a most useful supervised learning technique for text classification. In this paper we proposed advanced Multi Class Instance Selection based support vector machine (AMCISSVM) to increasing efficiency of support vector machine. The proposed algorithm is compared with Multi Class Instance Selection (MCIS) and Neighborhood Property based Pattern Selection (NPPS) algorithm. The advanced MCIS has shown high accuracy to multi datasets. These experimental datasets are retrieved from UCI machine learning repositories.},
annote = {From Duplicate 1 (An Advanced Multi Class Instance Selection based Support Vector Machine for Text Classification - Ramesh, B.; Sathiaseelan, J. G R)

- Poorly written
- MCIS method for instance selection
- don't provide original accuracy
- pretty poor study},
author = {{Ramesh B} and Sathiaseelan, J. G R and Ramesh, B. and Sathiaseelan, J. G R},
doi = {10.1016/j.procs.2015.07.400},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Ramesh B, Sathiaseelan - 2015 - ScienceDirect An Advanced Multi Class Instance Selection based Support Vector Machine for Text Classific.pdf:pdf},
issn = {18770509},
journal = {Procedia Computer Science},
keywords = {Classification,Instance Selection,Support Vector Machine,Text Mining},
pages = {1124--1130},
publisher = {Elsevier Masson SAS},
title = {{An Advanced Multi Class Instance Selection based Support Vector Machine for Text Classification}},
url = {http://dx.doi.org/10.1016/j.procs.2015.07.400 www.sciencedirect.com},
volume = {57},
year = {2015}
}
@article{Parikh2012,
author = {Parikh, Devi and Kovashka, Adriana and Parkash, Amar and Grauman, Kristen},
file = {:Users/Walter/Documents/Literature/5084-22510-1-PB.pdf:pdf},
isbn = {9781577355687},
journal = {Aaai},
keywords = {Subarea Spotlights Track},
pages = {2153--2159},
title = {{Relative Attributes for Enhanced Human-Machine Communication.}},
url = {http://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/download/5084/5387},
year = {2012}
}
@misc{Kalmus1969,
author = {Kalmus, Henry},
title = {{Moving Target Indicator}},
url = {http://oai.dtic.mil/oai/oai?verb=getRecord{\&}metadataPrefix=html{\&}identifier=ADD002194},
year = {1969}
}
@article{Zitzler2001,
abstract = {The Strength Pareto Evolutionary Algorithm(SPEA) (Zitzler and Thiele 1999) is a relatively recent technique for finding or approximating the Pareto-optimal set for multiobjective optimization problems. In different studies (Zitzler and Thiele 1999; Zitzler, Deb, and Thiele 2000) SPEA has shown very good performance in comparison to other multiobjective evolutionary algorithms, and therefore it has been a point of reference in various recent investigations, e.g., (Corne, Knowles, and Oates 2000). Furthermore, it has been used in different applications, e.g., (Lahanas, Milickovic, Baltas, and Zamboglou 2001). In this paper, an improved ver- sion, namely SPEA2, is proposed, which incorporates in contrast to its predecessor a fine-grained fitness assignment strategy, a density estimation technique, and an enhanced archive truncation method. The comparison of SPEA2 with SPEA and two other modern elitist methods, PESA and NSGA-II, on different test problems yields promising results. 1},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Zitzler, Eckart and Laumanns, Marco and Thiele, Lothar},
doi = {10.1.1.28.7571},
eprint = {arXiv:1011.1669v3},
file = {:Users/Walter/Documents/Literature/eth-24689-01.pdf:pdf},
isbn = {TIK-Report No. 103},
issn = {03772217},
journal = {Evolutionary Methods for Design Optimization and Control with Applications to Industrial Problems},
pages = {95--100},
pmid = {15003161},
title = {{SPEA2: Improving the Strength Pareto Evolutionary Algorithm}},
year = {2001}
}
@article{Lehmann2005,
abstract = {Categorization of medical images means selecting the appropriate class for a given image out of a set of pre-defined categories. This is an important step for data mining and content-based image retrieval (CBIR). So far, published approaches are capable to distinguish up to 10 categories. In this paper, we evaluate automatic categorization into more than 80 categories describing the imaging modality and direction as well as the body part and biological system examined. Based on 6231 reference images from hospital routine, 85.5{\%} correctness is obtained combining global texture features with scaled images. With a frequency of 97.7{\%}, the correct class is within the best ten matches, which is sufficient for medical CBIR applications. {\textcopyright} 2004 Elsevier Ltd. All rights reserved.},
author = {Lehmann, Thomas M. and G{\"{u}}ld, Mark O. and Deselaers, Thomas and Keysers, Daniel and Schubert, Henning and Spitzer, Klaus and Ney, Hermann and Wein, Berthold B.},
doi = {10.1016/j.compmedimag.2004.09.010},
isbn = {0895-6111 (Print)$\backslash$r0895-6111 (Linking)},
issn = {08956111},
journal = {Computerized Medical Imaging and Graphics},
keywords = {Classifier combination,Content-based image retrieval (CBIR),Data mining,Feature extraction,Image categorization,Medical imaging,Pattern recognition,Texture analysis},
number = {2-3},
pages = {143--155},
pmid = {15755534},
title = {{Automatic categorization of medical images for content-based retrieval and data mining}},
volume = {29},
year = {2005}
}
@article{Backlund2015a,
author = {Backlund, Peter B. and Shahan, David W. and Seepersad, Carolyn Conner},
doi = {10.1080/0305215X.2014.908869},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Backlund, Shahan, Seepersad - 2015 - Classifier-guided sampling for discrete variable, discontinuous design space exploration Convergenc.pdf:pdf},
issn = {0305-215X},
journal = {Engineering Optimization},
number = {5},
pages = {579--600},
title = {{Classifier-guided sampling for discrete variable, discontinuous design space exploration: Convergence and computational performance}},
url = {http://www.tandfonline.com/doi/abs/10.1080/0305215X.2014.908869},
volume = {47},
year = {2015}
}
@article{Zhu2017a,
abstract = {We propose a new active learning by query synthesis approach using Generative Adversarial Networks (GAN). Different from regular active learning, the resulting algorithm adaptively synthesizes training instances for querying to increase learning speed. We generate queries according to the uncertainty principle, but our idea can work with other active learning principles. We report results from various numerical experiments to demonstrate the effectiveness the proposed approach. In some settings, the proposed algorithm outperforms traditional pool-based approaches. To the best our knowledge, this is the first active learning work using GAN.},
archivePrefix = {arXiv},
arxivId = {1702.07956},
author = {Zhu, Jia-Jie and Bento, Jos{\'{e}}},
eprint = {1702.07956},
file = {:Users/Walter/Documents/Literature/1702.07956.pdf:pdf},
pages = {1--11},
title = {{Generative Adversarial Active Learning}},
url = {http://arxiv.org/abs/1702.07956},
year = {2017}
}
@inproceedings{Perarnau2016,
archivePrefix = {arXiv},
arxivId = {1611.06355},
author = {Perarnau, Guim and Weijer, Joost Van De and Raducanu, Bogdan and {\'{A}}lvarez, Jose M and Csiro, Data},
booktitle = {NIPS},
eprint = {1611.06355},
file = {:Users/Walter/Documents/Literature/1611.06355v1.pdf:pdf},
title = {{Invertible Conditional GANs for image editing}},
year = {2016}
}
@article{Orriols2005,
abstract = {The class imbalance problem has been said recently to hinder the performance of learning systems. In fact, many of them are designed with the assumption of well-balance datasets. However, it is very common to find higher presence of one of the classes in real classification problems. The aim of this paper is to make a preliminary analysis on the effect of the class imbalance problem in learning classifier systems. Particularly we focus our study on UCS, a supervised version of XCS classifier system. We analyze UCS's behavior on unbalanced datasets and find that UCS is sensitive to high levels of class imbalance. We study strategies for dealing with class imbalances, acting either at the sampling level or at the classifier system's level.},
author = {Orriols, Albert and Bernado-Mansilla, Ester},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Orriols, Bernado-Mansilla - 2005 - The Class Imbalance Problem in Learning Classifier Systems A Preliminary Study.pdf:pdf},
isbn = {1595930973},
journal = {Proceedings of the 2005 workshops on Genetic and evolutionary computation},
keywords = {class imbalance,evolutionary computation,genetic algorithms,learning,learning classifier systems,machine},
pages = {74--78},
title = {{The Class Imbalance Problem in Learning Classifier Systems : A Preliminary Study}},
year = {2005}
}
@article{Huang2005,
abstract = {The area under the ROC (receiver operating characteristics) curve, or simply AUC, has been traditionally used in medical diagnosis since the 1970s. It has recently been proposed as an alternative single-number measure for evaluating the predictive ability of learning algorithms. However, no formal arguments were given as to why AUC should be preferred over accuracy. We establish formal criteria for comparing two different measures for learning algorithms and we show theoretically and empirically that AUC is a better measure (defined precisely) than accuracy. We then reevaluate well-established claims in machine learning based on accuracy using AUC and obtain interesting and surprising new results. For example, it has been well-established and accepted that Naive Bayes and decision trees are very similar in predictive accuracy. We show, however, that Naive Bayes is significantly better than decision trees in AUC. The conclusions drawn in this paper may make a significant impact on machine learning and data mining applications.},
author = {Huang, Jin and Ling, Charles X.},
isbn = {1041-4347},
issn = {10414347},
journal = {IEEE Transactions on Knowledge and Data Engineering},
keywords = {AUC of ROC,Accuracy,Evaluation of learning algorithms,ROC},
number = {3},
pages = {299--310},
title = {{Using AUC and accuracy in evaluating learning algorithms}},
volume = {17},
year = {2005}
}
@article{Berthelot2017,
abstract = {We propose a new equilibrium enforcing method paired with a loss derived from the Wasserstein distance for training auto-encoder based Generative Adversarial Networks. This method balances the generator and discriminator during training. Additionally, it provides a new approximate convergence measure, fast and stable training and high visual quality. We also derive a way of controlling the trade-off between image diversity and visual quality. We focus on the image generation task, setting a new milestone in visual quality, even at higher resolutions. This is achieved while using a relatively simple model architecture and a standard training procedure.},
archivePrefix = {arXiv},
arxivId = {1703.10717},
author = {Berthelot, David and Schumm, Thomas and Metz, Luke},
doi = {1703.10717},
eprint = {1703.10717},
file = {:Users/Walter/Documents/Literature/1703.10717.pdf:pdf},
pages = {1--10},
title = {{BEGAN: Boundary Equilibrium Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1703.10717},
year = {2017}
}
@article{Debole2005,
abstract = {The existence, public availability, and widespread acceptance of a standard benchmark for a given information retrieval (IR) task are beneficial to research on this task, since they allow different researchers to experimentally compare their own systems by comparing the results they have obtained on this benchmark. The Reuters-21578 test collection, together with its earlier variants, has been such a standard benchmark for the text categorization (TC) task throughout the last ten years. However, the benefits that this has brought about have somehow been limited by the fact that different researchers have ``carved'' different subsets out of this collection, and tested their systems on one of these subsets only; systems that have been tested on different Reuters-21578 subsets are thus not readily comparable. In this paper we present a systematic, comparative experimental study of the three subsets of Reuters-21578 that have been most popular among TC researchers. The results we obtain allow us to determine the relative hardness of these subsets, thus establishing an indirect means for comparing TC systems that have, or will be, tested on these different subsets.},
annote = {Could be a candidate for final "show off" experiment . Pretty detailed how dataset is made, report weird averaged results, show slick way of weighting features},
author = {Debole, Franca and Sebastiani, Fabrizio},
doi = {10.1002/asi.20147},
file = {:Users/Walter/Documents/Literature/10.1.1.385.4992.pdf:pdf},
isbn = {1532-2890},
issn = {15322882},
journal = {Journal of the American Society for Information Science and Technology},
number = {6},
pages = {584--596},
title = {{An analysis of the relative hardness of reuters-21578 subsets}},
volume = {56},
year = {2005}
}
@article{Uysal2014,
abstract = {Preprocessing is one of the key components in a typical text classification framework. This paper aims to extensively examine the impact of preprocessing on text classification in terms of various aspects such as classification accuracy, text domain, text language, and dimension reduction. For this purpose, all possible combinations of widely used preprocessing tasks are comparatively evaluated on two different domains, namely e-mail and news, and in two different languages, namely Turkish and English. In this way, contribution of the preprocessing tasks to classification success at various feature dimensions, possible interactions among these tasks, and also dependency of these tasks to the respective languages and domains are comprehensively assessed. Experimental analysis on benchmark datasets reveals that choosing appropriate combinations of preprocessing tasks, rather than enabling or disabling them all, may provide significant improvement on classification accuracy depending on the domain and language studied on. {\textcopyright} 2013 Elsevier Ltd. All rights reserved.},
annote = {stop word removal
lower case
all subject to workign or not working},
author = {Uysal, Alper Kursat and Gunal, Serkan},
doi = {10.1016/j.ipm.2013.08.006},
file = {:Users/Walter/Documents/Literature/The impact of preprocessing on text classification.pdf:pdf},
isbn = {5359777370},
issn = {03064573},
journal = {Information Processing and Management},
keywords = {Pattern recognition,Text categorization,Text classification,Text preprocessing},
number = {1},
pages = {104--112},
publisher = {Elsevier Ltd},
title = {{The impact of preprocessing on text classification}},
url = {http://dx.doi.org/10.1016/j.ipm.2013.08.006},
volume = {50},
year = {2014}
}
@article{White2002,
abstract = {We illustrate several types of cartographic displays that can enhance understanding from hierarchical analysis techniques such as regression trees. When the observations have spatial locations, maps of the predicted values, maps of the residuals, and maps of the predicting relationships of the tree may help to reveal associations between predictors and response. We propose an objective method for constructing maps that may help to show the geographical similarities and differences between observations based on their positions in the prediction tree. This mapping method divides the color spectrum to assign colors to the leaves, using the same hierarchical pattern as the prediction tree does to divide the data. We illustrate regression tree cartography with two examples and suggest how the prediction tree mapping method could be used for classification trees and for dendrograms produced by hierarchical clustering methods.},
author = {White, Denis and Sifneos, Jean C},
doi = {10.1198/106186002484},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/White, Sifneos - 2002 - Regression Tree Cartography.pdf:pdf},
issn = {1061-8600},
journal = {Journal of Computational and Graphical Statistics},
keywords = {classi cation and regression,color symbolism,hierarchical clustering,trees},
number = {3},
pages = {600--614},
title = {{Regression Tree Cartography}},
volume = {11},
year = {2002}
}
@article{Dietterich2000,
abstract = {Ensemble methods are learning algorithms that construct a set of classifiers and then classify new data points by taking a (weighted) vote of their predictions. The original ensemble method is Bayesian averaging, but more recent algorithms include error-correcting output coding, Bagging, and boosting. This paper reviews these methods and explains why ensembles can often perform better than any single classifier. Some previous studies comparing ensemble methods are reviewed, and some new experiments are presented to uncover the reasons that Adaboost does not overfit rapidly.},
author = {Dietterich, Thomas G},
doi = {10.1007/3-540-45014-9_1},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Dietterich - 2000 - Ensemble Methods in Machine Learning.pdf:pdf},
isbn = {3540677046},
issn = {0738-4602},
journal = {Lecture Notes in Computer Science},
pages = {1--15},
title = {{Ensemble Methods in Machine Learning}},
url = {http://www.springerlink.com/index/LG5742LR8810612K.pdf},
volume = {1857},
year = {2000}
}
@article{Achanta2011,
author = {Achanta, R and Shaji, A and Smith, K and Lucchi, A and Fua, P and S{\"{u}}sstrunk, Sabine},
doi = {10.1109/tpami.2012.120},
file = {:Users/Walter/Documents/Literature/Superpixel{\_}PAMI2011-2.pdf:pdf},
isbn = {0162-8828},
issn = {01628828},
journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
number = {11},
pages = {2274--2282},
pmid = {22641706},
title = {{SLIC Superpixels Compared to State-of-the-Art Superpixel Methods}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6205760{\%}7B{\%}25{\%}7D0Apapers3://publication/doi/10.1109/TPAMI.2012.120},
volume = {34},
year = {2011}
}
@inproceedings{Ruttenberg2015,
address = {Washington DC},
author = {Ruttenberg, Brian E and Wilkins, Matthew P},
booktitle = {18th International Conference on Information Fusion},
file = {:Users/Walter/Documents/Literature/ReasoningonRSOHierarchiesusingProbabilisticProgramming.pdf:pdf},
isbn = {9780996452717},
number = {1},
pages = {1315--1321},
title = {{Reasoning on Resident Space Object Hierarchies Using Probabilistic Programming}},
year = {2015}
}
@article{Zhang2004,
author = {Zhang, H},
journal = {AA},
keywords = {American Association for Artific,Copyright {\textcopyright} 2004},
number = {2},
pages = {3},
title = {{The optimality of naive Bayes}},
url = {http://www.aaai.org/Papers/FLAIRS/2004/Flairs04-097.pdf},
volume = {1},
year = {2004}
}
@article{Poole2015,
author = {Poole, Mark and Murray-Krezan, Jeremy},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Poole, Murray-Krezan - 2015 - Autonomous Object Characterization with Large Datasets.pdf:pdf},
journal = {AMOS Tech},
title = {{Autonomous Object Characterization with Large Datasets}},
year = {2015}
}
@article{Triguero2012,
abstract = {The nearest neighbor (NN) rule is one of the most successfully used techniques to resolve classification and pattern recognition tasks. Despite its high classification accuracy, this rule suffers from several shortcomings in time response, noise sensitivity, and high storage requirements. These weaknesses have been tackled by many different approaches, including a good and well-known solution that we can find in the literature, which consists of the reduction of the data used for the classification rule (training data). Prototype reduction techniques can be divided into two different approaches, which are known as prototype selection and prototype generation (PG) or abstraction. The former process consists of choosing a subset of the original training data, whereas PG builds new artificial prototypes to increase the accuracy of the NN classification. In this paper, we provide a survey of PG methods specifically designed for the NN rule. From a theoretical point of view, we propose a taxonomy based on the main characteristics presented in them. Furthermore, from an empirical point of view, we conduct a wide experimental study that involves small and large datasets to measure their performance in terms of accuracy and reduction capabilities. The results are contrasted through nonparametrical statistical tests. Several remarks are made to understand which PG models are appropriate for application to different datasets.},
author = {Triguero, Isaac and Derrac, Joaqu{\'{i}}n and Garc{\'{i}}a, Salvador and Herrera, Francisco},
doi = {10.1109/TSMCC.2010.2103939},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Triguero et al. - 2012 - A taxonomy and experimental study on prototype generation for nearest neighbor classification.pdf:pdf},
isbn = {1094-6977},
issn = {10946977},
journal = {IEEE Transactions on Systems, Man and Cybernetics Part C: Applications and Reviews},
keywords = {Classification,learning vector quantization (LVQ),nearest neighbor (NN),prototype generation (PG),taxonomy},
number = {1},
pages = {86--100},
title = {{A taxonomy and experimental study on prototype generation for nearest neighbor classification}},
volume = {42},
year = {2012}
}
@article{Lin2005,
author = {Lin, Chih-hsiang},
journal = {{\ldots} Conference on Data Mining},
pages = {68--79},
title = {{Mining frequent itemsets from data streams with a time-sensitive sliding window}},
url = {http://books.google.com/books?hl=en{\&}lr={\&}id=tQqcOdHS-REC{\&}oi=fnd{\&}pg=PA68{\&}dq=Mining+Frequent+Itemsets+from+Data+Streams+with+a+Time-Sensitive+Sliding+Window{\&}ots=JUdrM{\_}T7Ld{\&}sig=AostXAx8NWJJqNMvKcUuIT7Qa1Y},
year = {2005}
}
@article{Odena2018,
abstract = {Recent work (Pennington et al, 2017) suggests that controlling the entire distribution of Jacobian singular values is an important design consideration in deep learning. Motivated by this, we study the distribution of singular values of the Jacobian of the generator in Generative Adversarial Networks (GANs). We find that this Jacobian generally becomes ill-conditioned at the beginning of training. Moreover, we find that the average (with z from p(z)) conditioning of the generator is highly predictive of two other ad-hoc metrics for measuring the 'quality' of trained GANs: the Inception Score and the Frechet Inception Distance (FID). We test the hypothesis that this relationship is causal by proposing a 'regularization' technique (called Jacobian Clamping) that softly penalizes the condition number of the generator Jacobian. Jacobian Clamping improves the mean Inception Score and the mean FID for GANs trained on several datasets. It also greatly reduces inter-run variance of the aforementioned scores, addressing (at least partially) one of the main criticisms of GANs.},
archivePrefix = {arXiv},
arxivId = {1802.08768},
author = {Odena, Augustus and Buckman, Jacob and Olsson, Catherine and Brown, Tom B. and Olah, Christopher and Raffel, Colin and Goodfellow, Ian},
eprint = {1802.08768},
file = {:Users/Walter/Documents/Literature/1802.08768.pdf:pdf},
title = {{Is Generator Conditioning Causally Related to GAN Performance?}},
url = {http://arxiv.org/abs/1802.08768},
year = {2018}
}
@article{Sivarajah2017,
abstract = {Big Data (BD), with their potential to ascertain valued insights for enhanced decision-making process, have recently attracted substantial interest from both academics and practitioners. Big Data Analytics (BDA) is increasingly becoming a trending practice that many organizations are adopting with the purpose of constructing valuable information from BD. The analytics process, including the deployment and use of BDA tools, is seen by organizations as a tool to improve operational efficiency though it has strategic potential, drive new revenue streams and gain competitive advantages over business rivals. However, there are different types of analytic applications to consider. Therefore, prior to hasty use and buying costly BD tools, there is a need for organizations to first understand the BDA landscape. Given the significant nature of the BD and BDA, this paper presents a state-of-the-art review that presents a holistic view of the BD challenges and BDA methods theorized/proposed/employed by organizations to help others understand this landscape with the objective of making robust investment decisions. In doing so, systematically analysing and synthesizing the extant research published on BD and BDA area. More specifically, the authors seek to answer the following two principal questions: Q1 ??? What are the different types of BD challenges theorized/proposed/confronted by organizations? and Q2 ??? What are the different types of BDA methods theorized/proposed/employed to overcome BD challenges?. This systematic literature review (SLR) is carried out through observing and understanding the past trends and extant patterns/themes in the BDA research area, evaluating contributions, summarizing knowledge, thereby identifying limitations, implications and potential further research avenues to support the academic community in exploring research themes/patterns. Thus, to trace the implementation of BD strategies, a profiling method is employed to analyze articles (published in English-speaking peer-reviewed journals between 1996 and 2015) extracted from the Scopus database. The analysis presented in this paper has identified relevant BD research studies that have contributed both conceptually and empirically to the expansion and accrual of intellectual wealth to the BDA in technology and organizational resource management discipline.},
author = {Sivarajah, Uthayasankar and Kamal, Muhammad Mustafa and Irani, Zahir and Weerakkody, Vishanth},
doi = {10.1016/j.jbusres.2016.08.001},
issn = {01482963},
journal = {Journal of Business Research},
keywords = {Big Data,Big Data Analytics,Challenges,Methods,Systematic literature review},
pages = {263--286},
publisher = {The Authors},
title = {{Critical analysis of Big Data challenges and analytical methods}},
url = {http://dx.doi.org/10.1016/j.jbusres.2016.08.001},
volume = {70},
year = {2017}
}
@article{Cui2017,
abstract = {This paper aims to extend the technique of fast neural style transfer to multiple styles, allowing the user to trans-fer the contents of any input image into an aggregation of multiple styles. We first implement single-style transfer: we train our fast style transfer network, which is a feed-forward convolutional neural network, over the Microsoft COCO Image Dataset 2014, and we connect this transformation network to a pre-trained VGG16 network (Frossard). After training on a desired style (or combination of them), we can input any desired image and have it rendered in this new vi-sual genre. We also add improved upsampling and instance normalization to the original networks for improved visual quality. Second, we extend style transfer to multiple styles, by training the network to learn parameters that will blend the weights. From our work we demonstrate similar results to previously seen single-style transfer, and promising pre-liminary results for multi-style transfer.},
author = {Cui, Brandon and Qi, Calvin and Wang, Aileen},
file = {:Users/Walter/Documents/Literature/401.pdf:pdf},
pages = {2017},
title = {{Multi-style Transfer: Generalizing Fast Style Transfer to Several Genres}},
url = {http://cs231n.stanford.edu/reports/2017/pdfs/401.pdf},
year = {2017}
}
@article{Dash1997,
abstract = {Feature selection has been the focus of interest for quite some time and much work has been done. With the creation of huge databases and the consequent requirements for good machine learning techniques, new problems arise and novel approaches to feature selection are in demand. This survey is a comprehensive overview of many existing methods from the 1970's to the present. It identifies four steps of a typical feature selection method, and categorizes the different existing methods in terms of generation procedures and evaluation functions, and reveals hitherto unattempted combinations of generation procedures and evaluation functions. Representative methods are chosen from each category for detailed explanation and discussion via example. Benchmark datasets with different characteristics are used for comparative study. The strengths and weaknesses of different methods are explained. Guidelines for applying feature selection methods are given based on data types and domain characteristics. This survey identifies the future research areas in feature selection, introduces newcomers to this field, and paves the way for practitioners who search for suitable methods for solving domain-specific real-world applications. (Intelligent Data Analysis, Vol. I, no. 3, http:llwwwelsevier.co{\&}ocate/ida) 0 1997 Elsevier Science B.V. All rights reserved.},
author = {{Dash '}, M and Liu, H},
doi = {10.1016/S1088-467X(97)00008-5},
file = {:Users/Walter/Documents/Literature/10.1.1.463.8806.pdf:pdf},
isbn = {1088-467X},
issn = {1088467X},
journal = {IDA ELSEVlER Intelligent Data Analysis},
keywords = {Classification,Feature selection,Framework},
number = {97},
pages = {131--156},
pmid = {21600290},
title = {{Feature Selection for Classification}},
url = {www.elsevier.comAocate/ida},
volume = {1},
year = {1997}
}
@article{Hart1968,
abstract = {Not Available},
author = {Hart, P.},
isbn = {1595931805},
issn = {0018-9448},
journal = {IEEE Transactions on Information Theory},
pages = {1966--1967},
title = {{The condensed nearest neighbor rule (Corresp.)}},
volume = {14},
year = {1968}
}
@article{Parikh2011,
abstract = {Human-nameable visual attributes offer many advan-tages when used as mid-level features for object recogni-tion, but existing techniques to gather relevant attributes can be inefficient (costing substantial effort or expertise) and/or insufficient (descriptive properties need not be dis-criminative). We introduce an approach to define a vo-cabulary of attributes that is both human understandable and discriminative. The system takes object/scene-labeled images as input, and returns as output a set of attributes elicited from human annotators that distinguish the cate-gories of interest. To ensure a compact vocabulary and ef-ficient use of annotators' effort, we 1) show how to actively augment the vocabulary such that new attributes resolve inter-class confusions, and 2) propose a novel " nameabil-ity " manifold that prioritizes candidate attributes by their likelihood of being associated with a nameable property. We demonstrate the approach with multiple datasets, and show its clear advantages over baselines that lack a name-ability model or rely on a list of expert-provided attributes.},
annote = {{\#}{\#}{\#} Paper information

- Venue: IEEE CVPR
- Year: 2011
- Author: @Parikh2011

{\#}{\#}{\#} Background quotes

- "attributes are defined by human lan- guage makes them useful to compute meaningful descriptions of unfamiliar objects"

- "Unfortunately, “nameability” and discriminativeness appear to be at odds."

{\#}{\#}{\#} Background information

{\#}{\#}{\#} Their method 

- Want to create attributes that users can name. Do so by looking for hyperplanes that help separate the categories. Then somehow predict how nameable an attribute is. 

- predict how nameable an attribute is by looking at distance to some manifold

- I believe they use the attributes to discriminate between the images, so probably not as accurate as possible

- We speculate that there is shared structure among nameable visual attributes. In other words, those attributes in the given image descriptor space corresponding to truly nameable properties occupy only a portion of that space—a manifold of decision bound- ary directions. If so, that means we can prioritize the candi- dates according to their distance to this manifold.

"we design the approach to actively minimize the amount of meaningless inquiries presented to an annotator, so that hu- man effort is mostly spent assigning meaning to divisions in feature space that actually have it, as opposed to discard- ing un-interpretable splits. We accomplish this with two key ideas: at each iteration, our approach 1) focuses on at- tribute hypotheses that complement the classification power of existing attributes collected thus far, and 2) predicts the nameability of each discriminative hypothesis and priori- tizes those likely to be nameable. For the latter, we explore whether there exists some manifold structure in the space of nameable hyperplane separators. Ultimately,"


{\#}{\#}{\#} Evaluation

- Human experiments with mechanical turk (although not doing it live, collect labels before hand)
- No proofs


{\#}{\#}{\#} Unique paper aspects 

- discovers nameable attributes},
author = {Parikh, Devi and Grauman, Kristen},
file = {:Users/Walter/Documents/Literature/parikh{\_}grauman{\_}CVPR2011.pdf:pdf},
title = {{Interactively Building a Discriminative Vocabulary of Nameable Attributes}},
url = {https://www.cc.gatech.edu/{~}parikh/Publications/ParikhGrauman{\_}CVPR2011{\_}nameable.pdf},
year = {2011}
}
@article{Dietterich1997,
abstract = {The multiple instance problem arises in tasks where the training examples areambiguous: a single example object may have many alternative feature vectors (in-stances) that describe it, and yet only one of those feature vectors may be responsi-ble for the observed classi cation of the object. This paper describes and comparesthree kinds of algorithms that learn axis-parallel rectangles to solve the multiple- instance problem. Algorithms that ignore the multiple instance problem performvery poorly. An algorithm that directly confronts the multiple instance problem (byattempting to identify which feature vectors are responsible for the observed classi-cations) performs best, giving 89{\%} correct predictions on a musk-odor predictiontask. The paper also illustrates the use of arti cial data to debug and compare thesealgorithms},
annote = {Largely unhelpful but interesting need

- need: many keys open lock, don't know which ones, find them
- provided incomplete information and still need to make a prediction (great key analogy)},
author = {Dietterich, Thomas G and Lathrop, H and Lozano-Perez, T},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Dietterich, Lathrop, Lozano-Perez - 1997 - Solving the Multiple-Instance Problem with Axis-Parallel Rectangles.pdf:pdf},
journal = {Artificial Intelligence},
number = {November 2003},
pages = {31--71},
title = {{Solving the Multiple-Instance Problem with Axis-Parallel Rectangles}},
year = {1997}
}
@article{Baehrens2010,
abstract = {After building a classifier with modern tools of machine learning we typically have a black box at hand that is able to predict well for unseen data. Thus, we get an answer to the question what is the most likely label of a given unseen data point. However, most methods will provide no answer why the model predicted a particular label for a single instance and what features were most influential for that particular instance. The only method that is currently able to provide such explanations are decision trees. This paper proposes a procedure which (based on a set of assumptions) allows to explain the decisions of any classification method.},
author = {Baehrens, David and Schroeter, Timon and Harmeling, Stefan and Kawanabe, Motoaki and Hansen, Katja and Klaus-Robert, Muller},
file = {:Users/Walter/Documents/Literature/baehrens10a.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {ames mutagenicity,black box model,explaining,kernel methods,nonlinear},
pages = {1803--1831},
title = {{How to Explain Individual Classification Decisions Motoaki Kawanabe}},
volume = {11},
year = {2010}
}
@article{Weng2008,
abstract = {The area of imbalanced datasets is still relatively new, and it is known that the use of overall accuracy is not an appropriate evaluation measure for imbalanced datasets, because of the dominating effect of the majority class. Although, researchers have tried other existing measurements, but there is still no single evaluation measure that work well with imbalanced dataset. In this paper, we introduce a novel measure as a better alternative for evaluating imbalanced dataset. We provide a theoretical background for the new evaluation technique that is designed to cope with cost biases, which changes the previous view about class independent evaluation methods cannot deal with costs, such as ROC curves. We also provide a general guideline for the ideal baseline performance when building classifiers with a known misclassification cost.},
author = {Weng, Cheng G and Poon, Josiah},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Weng, Poon - 2008 - A New Evaluation Measure for Imbalanced Datasets.pdf:pdf},
journal = {Proceeding AusDM '08 Proceedings of the 7th Australasian Data Mining Conference},
pages = {27--32},
title = {{A New Evaluation Measure for Imbalanced Datasets}},
volume = {87},
year = {2008}
}
@article{Chawla2002a,
abstract = {An approach to the construction of classifiers from imbalanced datasets is described. A dataset is imbalanced if the classification categories are not approximately equally rep-resented. Often real-world data sets are predominately composed of " normal " examples with only a small percentage of " abnormal " or " interesting " examples. It is also the case that the cost of misclassifying an abnormal (interesting) example as a normal example is often much higher than the cost of the reverse error. Under-sampling of the majority (nor-mal) class has been proposed as a good means of increasing the sensitivity of a classifier to the minority class. This paper shows that a combination of our method of over-sampling the minority (abnormal) class and under-sampling the majority (normal) class can achieve better classifier performance (in ROC space) than only under-sampling the majority class. This paper also shows that a combination of our method of over-sampling the minority class and under-sampling the majority class can achieve better classifier performance (in ROC space) than varying the loss ratios in Ripper or class priors in Naive Bayes. Our method of over-sampling the minority class involves creating synthetic minority class examples. Experiments are performed using C4.5, Ripper and a Naive Bayes classifier. The method is evaluated using the area under the Receiver Operating Characteristic curve (AUC) and the ROC convex hull strategy.},
archivePrefix = {arXiv},
arxivId = {1106.1813},
author = {Chawla, Nitesh V. and Bowyer, Kevin W. and Hall, Lawrence O. and Kegelmeyer, W. Philip},
doi = {10.1613/jair.953},
eprint = {1106.1813},
file = {:Users/Walter/Documents/Literature/live-953-2037-jair.pdf:pdf;:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Chawla et al. - 2002 - SMOTE Synthetic Minority Over-sampling Technique.pdf:pdf},
isbn = {013805326X},
issn = {10769757},
journal = {Journal of Artificial Intelligence Research},
pages = {321--357},
pmid = {18190633},
title = {{SMOTE: Synthetic minority over-sampling technique}},
volume = {16},
year = {2002}
}
@article{Le2014,
abstract = {Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words. For example, "powerful," "strong" and "Paris" are equally distant. In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that Paragraph Vectors outperform bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks.},
archivePrefix = {arXiv},
arxivId = {1405.4053},
author = {Le, Quoc V. and Mikolov, Tomas},
doi = {10.1145/2740908.2742760},
eprint = {1405.4053},
isbn = {9781634393973},
issn = {10495258},
pmid = {9377276},
title = {{Distributed Representations of Sentences and Documents}},
url = {http://arxiv.org/abs/1405.4053},
volume = {32},
year = {2014}
}
@article{Arjovsky2017,
abstract = {We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to other distances between distributions.},
archivePrefix = {arXiv},
arxivId = {1701.07875},
author = {Arjovsky, Martin and Chintala, Soumith and Bottou, L{\'{e}}on},
eprint = {1701.07875},
file = {:Users/Walter/Documents/Literature/1701.07875.pdf:pdf},
isbn = {1406.2661},
issn = {1701.07875},
title = {{Wasserstein GAN}},
url = {http://arxiv.org/abs/1701.07875},
year = {2017}
}
@inproceedings{Domingos1997,
abstract = {If it is to qualify as knowledge, a learner's output should be accurate,$\backslash$nstable and comprehensible. Learning multiple models can improve significantly$\backslash$non the accuracy and stability of single models, but at the cost of$\backslash$nlosing their comprehensibility (when they possess it, as do, for$\backslash$nexample, simple decision trees and rule sets). This paper proposes$\backslash$nand evaluates CMM, a meta-learner that seeks to retain most of the$\backslash$naccuracy gains of multiple model approaches, while still producing$\backslash$na single comprehensible model. CMM is based on reapplying the base$\backslash$nlearner to recover the frontiers implicit in the multiple model ensemble.$\backslash$nThis is done by giving the base learner a new training set, composed$\backslash$nof a large number of examples generated and classified according$\backslash$nto the ensemble, plus the original examples. CMM is evaluated using$\backslash$nC4.5RULES as the base learner, and bagging as the multiple-model$\backslash$nmethodology. On 26 benchmark datasets, CMM retains on average 60{\%}$\backslash$nof the accuracy gains obtained by bagging ...},
annote = {Gennerate data via ensemble and then relearn classifier from generated data

- Neat idea!
- source for wanting interpretable model
-},
author = {Domingos, Pedro},
booktitle = {In Proceedings of the Fourteenth International Conference on Machine Learning},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Domingos - 1997 - Knowledge Acquisition from Examples Via Multiple Models.pdf:pdf},
keywords = {jabref:noKeywordAssigned},
pages = {98--106},
title = {{Knowledge Acquisition from Examples Via Multiple Models}},
year = {1997}
}
@article{Ribeiro2016,
abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
archivePrefix = {arXiv},
arxivId = {1602.04938},
author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
doi = {10.1145/2939672.2939778},
eprint = {1602.04938},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Ribeiro, Singh, Guestrin - 2016 - {\&}quotWhy Should I Trust You{\&}quot Explaining the Predictions of Any Classifier.pdf:pdf},
isbn = {9781450321389},
issn = {9781450321389},
journal = {22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
title = {{Why Should I Trust You: Explaining the Predictions of Any Classifier}},
url = {http://arxiv.org/abs/1602.04938},
year = {2016}
}
@article{Aha1991,
abstract = {Storing and using specific instances improves the performance of several supervised learning algorithms. These include algorithms that learn decision trees, classification rules, and distributed networks. However, no investigation has analyzed algorithms that use only specific instances to solve incremental learning tasks. In this paper, we describe a framework and methodology, called instance-based learning, that generates classification predictions using only specific instances. Instance-based learning algorithms do not maintain a set of abstractions derived from specific instances. This approach extends the nearest neighbor algorithm, which has large storage requirements. We describe how storage requirements can be significantly reduced with, at most, minor sacrifices in learning rate and classification accuracy. While the storage-reducing algorithm performs well on several real-world databases, its performance degrades rapidly with the level of attribute noise in training instances. Therefore, we extended it with a significance test to distinguish noisy instances. This extended algorithm's performance degrades gracefully with increasing noise levels and compares favorably with a noise-tolerant decision tree algorithm},
annote = {Not helpful for imbalanced},
author = {Aha, David W and Kibler, Dennis and Albert, Marc K},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Aha, Kibler, Albert - 1991 - Instance-Based Learning Algorithms.pdf:pdf},
journal = {Machine learning},
keywords = {incremental learning,instance-based concept descriptions,learning theory,supervised concept learning},
number = {1},
pages = {37--66},
title = {{Instance-Based Learning Algorithms}},
url = {http://link.springer.com.proxy.lib.iastate.edu/article/10.1007/BF00153759},
volume = {6},
year = {1991}
}
@article{Goodfellow2015,
abstract = {Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.},
archivePrefix = {arXiv},
arxivId = {1412.6572},
author = {Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian},
eprint = {1412.6572},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Goodfellow, Shlens, Szegedy - 2015 - Explaining and Harnessing Adversarial Examples.pdf:pdf},
isbn = {1412.6572},
journal = {Iclr 2015},
pages = {1--11},
pmid = {729514},
title = {{Explaining and Harnessing Adversarial Examples}},
url = {http://arxiv.org/abs/1412.6572},
year = {2015}
}
@article{Bagga1998,
abstract = {Cross-document coreference occurs when the same person, place, event, or concept is discussed in more than one text source. Computer recognition of this phenomenon is important because it helps break "the document boundary" by allowing a user to ex- amine information about a particular entity from multiple text sources at the same time. In this paper we describe a cross-document coreference resolution algorithm which uses the Vector Space Model to re- solve ambiguities between people having the same name. In addition, we also describe a scoring algo- rithm for evaluating the cross-document coreference chains produced by our system and we compare our algorithm to the scoring algorithm used in the MUC- 6 (within document) coreference task.},
annote = {BCubed},
author = {Bagga, Amit and Baldwin, Breck},
doi = {10.3115/980451.980859},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Bagga, Baldwin - 1998 - Entity-based cross-document coreferencing using the Vector Space Model.pdf:pdf},
journal = {Proceedings of the 17th international conference on Computational linguistics -},
pages = {79--85},
title = {{Entity-based cross-document coreferencing using the Vector Space Model}},
url = {http://dl.acm.org/citation.cfm?id=980451.980859},
volume = {1},
year = {1998}
}
@article{Kuncheva1999,
abstract = {Nearest neighbor classifiers demand significant computational resources (time and memory). Editing of the reference set and feature selection are two different approaches to this problem. Here we encode the two approaches within the same genetic algorithm {\{}(GA){\}} and simultaneously select features and reference cases. Two data sets were used: the {\{}SATIMAGE{\}} data and a generated data set. The {\{}GA{\}} was found to be an expedient solution compared to editing followed by feature selection, feature selection followed by editing, and the individual results from feature selection and editing.},
author = {Kuncheva, Ludmila I and Jain, Lakhmi C},
journal = {Pattern Recognition Letters},
keywords = {1-nn,editing for the nearest,er,feature selection,gas,genetic algorithms,neighbor classi},
number = {11-13},
pages = {1149--1156},
title = {{Nearest neighbor classifier : Simultaneous editing and feature selection}},
volume = {20},
year = {1999}
}
@book{Quinlan1992,
author = {Quinlan, J. Ross},
title = {{C4.5: programs for machine learning}},
year = {1992}
}
@article{Zeliff2016a,
abstract = {{\textcopyright} Copyright 2016 by ASME. Design spaces that consist of millions or billions of design combinations pose a challenge to current methods for identifying optimal solutions. Complex analyses can also lead to lengthy computation times that further challenge the effectiveness of an algorithm in terms of solution quality and run-time. This work explores combining the design space exploration approach of a Multi-Objective Genetic Algorithm with different instance-based, statistical, rule-based and ensemble classifiers to reduce the number of unnecessary function evaluations associated with poorly performing designs. Results indicate that introducing a classifier to identify child designs that are likely to push the Pareto frontier toward an optima reduce the number of function calculations by 75-85{\%}, depending on the classifier implemented.},
author = {Zeliff, Kayla and Bennette, W. and Ferguson, S.},
doi = {10.1115/DETC2016-60125},
file = {:Users/Walter/Documents/Literature/detc2016{\_}60125.pdf:pdf},
isbn = {9780791850107},
journal = {Proceedings of the ASME 2016 International Design Engineering Technical Conference {\&} Computers and Information in Engineering Conference},
pages = {1--12},
title = {{MULTI-OBJECTIVE COMPOSITE PANEL OPTIMIZATION USING MACHINE LEARNING CLASSIFIERS AND GENETIC ALGORITHMS}},
volume = {2A-2016},
year = {2016}
}
@article{Garcia2008,
abstract = {Prototype selection problem consists of reducing the size of databases by removing samples that are considered noisy or not influential on nearest neighbour classification tasks. Evolutionary algorithms have been used recently for prototype selection showing good results. However, due to the complexity of this problem when the size of the databases increases, the behaviour of evolutionary algorithms could deteriorate considerably because of a lack of convergence. This additional problem is known as the scaling up problem. Memetic algorithms are approaches for heuristic searches in optimization problems that combine a population-based algorithm with a local search. In this paper, we propose a model of memetic algorithm that incorporates an ad hoc local search specifically designed for optimizing the properties of prototype selection problem with the aim of tackling the scaling up problem. In order to check its performance, we have carried out an empirical study including a comparison between our proposal and previous evolutionary and non-evolutionary approaches studied in the literature. The results have been contrasted with the use of non-parametric statistical procedures and show that our approach outperforms previously studied methods, especially when the database scales up. ?? 2008 Elsevier Ltd. All rights reserved.},
annote = {Working on k-nn
scaling to big data sets},
author = {Garcia, Salvador and Cano, Jose Ramon and Herrera, Francisco},
doi = {10.1016/j.patcog.2008.02.006},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Data mining,Data reduction,Evolutionary algorithms,Memetic algorithms,Nearest neighbour rule,Prototype selection,Scaling up},
number = {8},
pages = {2693--2709},
title = {{A memetic algorithm for evolutionary prototype selection: A scaling up approach}},
volume = {41},
year = {2008}
}
@book{Han2006,
author = {Han, Jiawei and Kamber, Micheline},
edition = {Second Edi},
publisher = {Diane Cerra},
title = {{Data Mining: Concepts and Techniques}},
year = {2006}
}
@article{Amigo2009,
abstract = {There is a wide set of evaluation metrics available to compare the quality of text clustering algorithms. In this article, we define a few intuitive formal constraints on such metrics which shed light on which aspects of the quality of a clustering are captured by different metric families. These formal constraints are validated in an experiment involving human assessments, and compared with other constraints proposed in the literature. Our analysis of a wide range of metricsshows that onlyBCubed satisfies all formal constraints.We also extend the analysis to the problem of overlapping clustering, where items can simulta- neously belong to more than one cluster. As Bcubed cannot be directly applied to this task, we propose a modified version of Bcubed that avoids the problems found with other metrics.},
annote = {Extended BCubed},
author = {Amig{\'{o}}, Enrique and Gonzalo, Julio and Artiles, Javier and Verdejo, Felisa},
doi = {10.1007/s10791-008-9066-8},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Amig{\'{o}} et al. - 2009 - A comparison of extrinsic clustering evaluation metrics based on formal constraints.pdf:pdf},
isbn = {1386-4564 (Print) 1573-7659 (Online)},
issn = {1386-4564},
journal = {Information Retrieval},
keywords = {Clustering,Evaluation metrics,Formal constraints},
number = {4},
pages = {461--486},
title = {{A comparison of extrinsic clustering evaluation metrics based on formal constraints}},
url = {http://dx.doi.org/10.1007/s10791-008-9066-8},
volume = {12},
year = {2009}
}
@article{Lundberg2016,
abstract = {Understanding why a model made a certain prediction is crucial in many data science fields. Interpretable predictions engender appropriate trust and provide insight into how the model may be improved. However, with large modern datasets the best accuracy is often achieved by complex models even experts struggle to interpret, which creates a tension between accuracy and interpretability. Recently, several methods have been proposed for interpreting predictions from complex models by estimating the importance of input features. Here, we present how a model-agnostic additive representation of the importance of input features unifies current methods. This representation is optimal, in the sense that it is the only set of additive values that satisfies important properties. We show how we can leverage these properties to create novel visual explanations of model predictions. The thread of unity that this representation weaves through the literature indicates that there are common principles to be learned about the interpretation of model predictions that apply in many scenarios.},
archivePrefix = {arXiv},
arxivId = {1611.07478},
author = {Lundberg, Scott M and Lee, Su-In},
eprint = {1611.07478},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Lundberg, Lee - 2016 - An unexpected unity among methods for interpreting model predictions.pdf:pdf},
number = {Nips},
pages = {1--6},
title = {{An unexpected unity among methods for interpreting model predictions}},
year = {2016}
}
@article{Sanchez1997,
author = {S{\'{a}}nchez, J.S. and Pla, F. and Ferri, F.J.},
doi = {10.1016/S0167-8655(97)00035-4},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {gabriel graph,nearest neighbour,prototype selection,relative neighbourhood graph},
number = {6},
pages = {507--513},
title = {{Prototype selection for the nearest neighbour rule through proximity graphs}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0167865597000354},
volume = {18},
year = {1997}
}
@article{Chawla2003,
abstract = {Many real world data mining applications involve learning from imbalanced data sets. Learning from data sets that contain very few instances of the minority (or interesting) class usually produces biased classifiers that have a higher predictive accuracy over the majority class(es), but poorer predictive accuracy over the minority class. SMOTE (Synthetic Minority Over-sampling TEchnique) is specifically designed for learning from imbalanced data sets. This paper presents a novel approach for learning from imbalanced data sets, based on a combination of the SMOTE algorithm and the boosting procedure. Unlike standard boosting where all misclassified examples are given equal weights, SMOTEBoost creates synthetic examples from the rare or minority class, thus indirectly changing the updating weights and compensating for skewed distributions. SMOTEBoost applied to several highly and moderately imbalanced data sets shows improvement in prediction performance on the minority class and overall improved F-values.},
author = {Chawla, Nitesh V and Lazarevic, Aleksandar and Hall, Lawrence O and Bowyer, Kevin W},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Chawla et al. - 2003 - SMOTEBoost Improving Prediction of the Minority Class in Boosting.pdf:pdf},
journal = {Knowledge Discovery in Databases: PKDD 2003},
pages = {107--119},
title = {{SMOTEBoost : Improving Prediction of the Minority Class in Boosting}},
year = {2003}
}
@article{Lakkaraju2016,
abstract = {Predictive models deployed in the real world may assign incorrect labels to instances with high confidence. Such errors or unknown unknowns are rooted in model incompleteness, and typically arise because of the mismatch between training data and the cases encountered at test time. As the models are blind to such errors, input from an oracle is needed to identify these failures. In this paper, we formulate and address the problem of informed discovery of unknown unknowns of any given predictive model where unknown unknowns occur due to systematic biases in the training data. We propose a model-agnostic methodology which uses feedback from an oracle to both identify unknown unknowns and to intelligently guide the discovery. We employ a two-phase approach which first organizes the data into multiple partitions based on the feature similarity of instances and the confidence scores assigned by the predictive model, and then utilizes an explore-exploit strategy for discovering unknown unknowns across these partitions. We demonstrate the efficacy of our framework by varying the underlying causes of unknown unknowns across various applications. To the best of our knowledge, this paper presents the first algorithmic approach to the problem of discovering unknown unknowns of predictive models.},
annote = {Looking to discover failure states of predictive models.},
archivePrefix = {arXiv},
arxivId = {1610.09064},
author = {Lakkaraju, Himabindu and Kamar, Ece and Caruana, Rich and Horvitz, Eric},
eprint = {1610.09064},
file = {:Users/Walter/Documents/Literature/unknown{\_}unknowns{\_}identify{\_}algo.pdf:pdf},
title = {{Identifying Unknown Unknowns in the Open World: Representations and Policies for Guided Exploration}},
url = {http://arxiv.org/abs/1610.09064},
year = {2016}
}
@article{Sener2017a,
abstract = {Convolutional neural networks (CNNs) have been successfully applied to many recognition and learning tasks using a universal recipe; training a deep model on a very large dataset of supervised examples. However, this approach is rather restrictive in practice since collecting a large set of labeled images is very expensive. One way to ease this problem is coming up with smart ways for choosing images to be labelled from a very large collection (ie. active learning). Our empirical study suggests that many of the active learning heuristics in the literature are not effective when applied to CNNs in batch setting. Inspired by these limitations, we define the problem of active learning as core-set selection, ie. choosing set of points such that a model learned over the selected subset is competitive for the remaining data points. We further present a theoretical result characterizing the performance of any selected subset using the geometry of the datapoints. As an active learning algorithm, we choose the subset which is expected to yield best result according to our characterization. Our experiments show that the proposed method significantly outperforms existing approaches in image classification experiments by a large margin.},
archivePrefix = {arXiv},
arxivId = {1708.00489},
author = {Sener, Ozan and Savarese, Silvio},
eprint = {1708.00489},
file = {:Users/Walter/Documents/Literature/1e20c5a8f21faf44a41b495d90dd63a6f63d.pdf:pdf},
pages = {1--12},
title = {{Active Learning for Convolutional Neural Networks: A Core-Set Approach}},
url = {http://arxiv.org/abs/1708.00489},
year = {2017}
}
@misc{Rabiner1989,
abstract = {This tutorial provides an overview of the basic theory of hidden Markov models (HMMs) as originated by L.E. Baum and T. Petrie (1966) and gives practical details on methods of implementation of the theory along with a description of selected applications of the theory to distinct problems in speech recognition. Results from a number of original sources are combined to provide a single source of acquiring the background required to pursue further this area of research. The author first reviews the theory of discrete Markov chains and shows how the concept of hidden states, where the observation is a probabilistic function of the state, can be used effectively. The theory is illustrated with two simple examples, namely coin-tossing, and the classic balls-in-urns system. Three fundamental problems of HMMs are noted and several practical techniques for solving these problems are given. The various types of HMMs that have been studied, including ergodic as well as left-right models, are described},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Rabiner, Lawrence R.},
booktitle = {Proceedings of the IEEE},
doi = {10.1109/5.18626},
eprint = {arXiv:1011.1669v3},
file = {:Users/Walter/Documents/Literature/tutorial on hmm and applications.pdf:pdf},
isbn = {1558601244},
issn = {15582256},
number = {2},
pages = {257--286},
pmid = {21920608},
title = {{A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition}},
volume = {77},
year = {1989}
}
@article{DeHaro-Garcia2012,
abstract = {Instance selection is becoming more and more relevant due to the huge amount of data that is constantly being produced. However, although current algorithms are useful for fairly large datasets, many scaling problems are found when the number of instances is hundreds of thousands or millions. Most of the widely used instance selection algorithms are of complexity at least O(n 2), n being the number of instances. When we face very large problems, the scalability becomes an issue, and most of the algorithms are not applicable. This paper presents a methodology for scaling up instance selection algorithms by means of a parallel procedure that performs instance selection on small subsets of the original dataset. The results obtained with the application of instance selection to small subsets are combined using a voting scheme. The method achieves a very good performance in terms of testing error and storage reduction, while the execution time of the process is decreased very significantly. The parallel algorithm also removes any kind of constraint imposed by memory size, as the whole dataset does not need to be stored in memory. The usefulness of our method is shown by an extensive comparison using 35 datasets of medium and large sizes from the UCI Machine Learning Repository. Additionally, our method is applied to eight very large datasets with very good results and fast execution time. {\textcopyright} 2012 2012 Elsevier B.V. All rights reserved.},
annote = {parameters for CHC},
author = {{De Haro-Garc{\'{i}}a}, Aida and Garc{\'{i}}a-Pedrajas, Nicol{\'{a}}s and {Del Castillo}, Juan Antonio Romero},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/De Haro-Garc{\'{i}}a, Garc{\'{i}}a-Pedrajas, Del Castillo - 2012 - Large scale instance selection by means of federal instance selection.pdf:pdf},
isbn = {3642153801},
issn = {0169023X},
journal = {Data and Knowledge Engineering},
keywords = {Instance selection,Instance-based learning,Parallel algorithms,Scalability,Very large problems},
pages = {58--77},
publisher = {Elsevier B.V.},
title = {{Large scale instance selection by means of federal instance selection}},
volume = {75},
year = {2012}
}
@article{Talbot2009,
abstract = {Machine learning is an increasingly used computational tool within human-computer interaction research. While most researchers currently utilize an iterative approach to refining classifier models and performance, we propose that ensemble classification techniques may be a viable and even preferable alternative. In ensemble learning, algorithms combine multiple classifiers to build one that is superior to its components. In this paper, we present EnsembleMatrix, an interactive visualization system that presents a graphical view of confusion matrices to help users understand relative merits of various classifiers. EnsembleMatrix allows users to directly interact with the visualizations in order to explore and build combination models. We evaluate the efficacy of the system and the approach in a user study. Results show that users are able to quickly combine multiple classifiers operating on multiple feature sets to produce an ensemble classifier with accuracy that approaches best-reported performance classifying images in the CalTech-101 dataset.},
author = {Talbot, Justin and Lee, Bongshin and Kapoor, Ashish and Tan, Desney S.},
doi = {10.1145/1518701.1518895},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Talbot et al. - 2009 - EnsembleMatrix interactive visualization to support machine learning with multiple classifiers.pdf:pdf},
isbn = {9781605582467},
issn = {9781605582467},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {1283--1292},
title = {{EnsembleMatrix: interactive visualization to support machine learning with multiple classifiers}},
url = {http://portal.acm.org/citation.cfm?id=1518895},
year = {2009}
}
@article{Dietterich1998,
abstract = {This article reviews five approximate statistical tests for determining whether one learning algorithm outperforms another on a particular learning task. These tests are compared experimentally to determine their probability of incorrectly detecting a difference when no difference exists (type I error). Two widely used statistical tests are shown to have high probability of type I error in certain situations and should never be used: a test for difference of two proportions and a paired-differences t test based on taking several random train-test splits. A third test, a paired-differences t test based on 10-fold cross-validation, exhibits somewhat elevated probability of type I error. A fourth test, McNemar's test, is shown to have low type I error. The fifth test is a new test, 5 x 2 cv, based on five iterations of twofold cross-validation. Experiments show that this test also has acceptable type I error. The article also measures the power (ability to detect algorithm differences when they do exist) of these tests. The cross-validated t test is the most powerful. The 5 x 2 cv test is shown to be slightly more powerful than McNemar's test. The choice of the best test is determined by the computational cost of running the learning algorithm. For algorithms that can be executed only once, McNemar's test is the only test with acceptable type I error. For algorithms that can be executed 10 times, the 5 x 2 cv test is recommended, because it is slightly more powerful and because it directly measures variation due to the choice of training set.},
annote = {Possible experimental designs for comparing},
author = {Dietterich, Thomas G.},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Dietterich - 1998 - Approximate Statistical Tests for Comparing Supervised Classification Learning Algorithms.pdf:pdf},
isbn = {0899-7667},
issn = {0899-7667},
journal = {Neural Computation},
pages = {1895--1923},
pmid = {9744903},
title = {{Approximate Statistical Tests for Comparing Supervised Classification Learning Algorithms}},
volume = {10},
year = {1998}
}
@article{Wang2013,
abstract = {Text classification/categorization (TC) is to assign new unlabeled natural language documents to the predefined thematic categories. Centroid-based classifier (CC) has been widely used for TC because of its simplicity and efficiency. However, it has also been long criticized for its relatively low classification accuracy compared with state-of-the-art classifiers such as support vector machines (SVMs). In this paper, we find that for CC using only border instances rather than all instances to construct centroid vectors can obtain higher generalization accuracy. Along this line, we propose Border-Instance-based Iteratively Adjusted Centroid Classifier (IACC{\_}BI), which relies on the border instances found by some routines, e.g. 1-Nearest-and-1-Furthest-Neighbors strategy, to construct centroid vectors for CC. IACC{\_}BI then iteratively adjusts the initial centroid vectors according to the misclassified training instances. Our extensive experiments on 11 real-world text corpora demonstrate that IACC{\_}BI improves the performance of centroid-based classifiers greatly and obtains classification accuracy competitive to the well-known SVMs, while at significantly lower computational costs. ?? 2012 Elsevier B.V.},
author = {Wang, Deqing and Wu, Junjie and Zhang, Hui and Xu, Ke and Lin, Mengxiang},
doi = {10.1016/j.neucom.2012.08.019},
file = {:Users/Walter/Documents/Literature/IACC{\_}BI neurocomputing2013.pdf:pdf},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Border instance,Centroid-based classifier,Iterative adjustment,Support vector machines (SVMs),Text classification/categorization},
pages = {299--308},
title = {{Towards enhancing centroid classifier for text classification-A border-instance approach}},
volume = {101},
year = {2013}
}
@article{Lau2016,
abstract = {Recently, Le and Mikolov (2014) proposed doc2vec as an extension to word2vec (Mikolov et al., 2013a) to learn document-level embeddings. Despite promising results in the original paper, others have struggled to reproduce those results. This paper presents a rigorous empirical evaluation of doc2vec over two tasks. We compare doc2vec to two baselines and two state-of-the-art document embedding methodologies. We found that doc2vec performs robustly when using models trained on large external corpora, and can be further improved by using pre-trained word embeddings. We also provide recommendations on hyper-parameter settings for general purpose applications, and release source code to induce document embeddings using our trained doc2vec models.},
annote = {From Duplicate 1 (An Empirical Evaluation of doc2vec with Practical Insights into Document Embedding Generation - Lau, Jey Han; Baldwin, Timothy)

- StackExchange classify duplicate questions
- sentence similarity
- English wikipedia
- AP-News},
archivePrefix = {arXiv},
arxivId = {1607.05368},
author = {Lau, Jey Han and Baldwin, Timothy},
doi = {10.18653/v1/W16-1609},
eprint = {1607.05368},
file = {:Users/Walter/Documents/Literature/1607.05368.pdf:pdf},
number = {2014},
pages = {78--86},
title = {{An Empirical Evaluation of doc2vec with Practical Insights into Document Embedding Generation}},
url = {http://arxiv.org/abs/1607.05368},
year = {2016}
}
@article{Weikum2002,
abstract = {"Statistical natural-language processing is, in my estimation, one of the most fast-moving and exciting areas of computer science these days. Anyone who wants to learn this field would be well advised to get this book. For that matter, the same goes for anyone who is already in the field. I know that it is going to be one of the most well-thumbed books on my bookshelf." - Eugene Charniak, Department of Computer Science, Brown University Statistical approaches to processing natural language text have become dominant in recent years. This foundational text is the first comprehensive introduction to statistical natural language processing (NLP) to appear. The book contains all the theory and algorithms needed for building NLP tools. It provides broad but rigorous coverage of mathematical and linguistic foundations, as well as detailed discussion of statistical methods, allowing students and researchers to construct their own implementations. The book covers collocation finding, word sense disambiguation, probabilistic parsing, information retrieval, and other applications. More on this book},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Weikum, Gerhard},
doi = {10.1145/601858.601867},
eprint = {arXiv:1011.1669v3},
file = {:Users/Walter/Documents/Literature/Manning{\_}Schuetze{\_}StatisticalNLP.pdf:pdf},
isbn = {0262133601},
issn = {01635808},
journal = {ACM SIGMOD Record},
number = {3},
pages = {37},
pmid = {20955506},
title = {{Foundations of statistical natural language processing}},
volume = {31},
year = {2002}
}
@article{Kolcz2003,
abstract = {The task of finding duplicate records in large databases has long been an area of active research in the data-mining community. The gravity of the problem posed by duplicate “contamination ” of the data has rarely been addressed directly, however, with more efforts being directed at the related problem of class imbalance. We consider the consequences of duplicate presence on the quality of classifiers learnt with such data, emphasizing that contamination rates among the training and test collections may be considerably different. The discussion is supported by experiments using the spam-detection task as an example where dealing with varying degrees of duplication is commonplace. Our results indicate the generally detrimental effect of duplicate presence on classification accuracy, but also demonstrate that, for classifiers such as Naive Bayes Perceptron with Margins, duplicate rates need to be rather high to be truly harmful.},
author = {Kolcz, Aleksander and Chowdhury, Abdur and Alspector, Joshua},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Kolcz, Chowdhury, Alspector - 2003 - Data duplication an imbalance problem.pdf:pdf},
title = {{Data duplication : an imbalance problem?}},
year = {2003}
}
@article{Gabrilovich2004,
abstract = {Text categorization algorithms usually represent documents as bags of words and consequently have to deal with huge numbers of features. Most previous studies found that the majority of these features are relevant for classification, and that the performance of text categorization with support vector machines peaks when no feature selection is performed. We describe a class of text categorization problems that are characterized with many redundant features. Even though most of these features are relevant, the underlying concepts can be concisely captured using only a few features, while keeping all of them has substantially detrimental effect on categorization accuracy. We develop a novel measure that captures feature redundancy, and use it to analyze a large collection of datasets. We show that for problems plagued with numerous redundant features the performance of C4.5 is significantly superior to that of SVM, while aggressive feature selection allows SVM to beat C4.5 by a narrow margin.},
author = {Gabrilovich, Evgeniy and Markovitch, Shaul},
doi = {10.1145/1015330.1015388},
file = {:Users/Walter/Documents/Literature/textCategorization.pdf:pdf},
isbn = {1581138285},
journal = {Twenty-first international conference on Machine learning - ICML '04},
pages = {41},
title = {{Text categorization with many redundant features}},
url = {http://portal.acm.org/citation.cfm?doid=1015330.1015388{\%}5Cnhttp://dl.acm.org/citation.cfm?id=1015330.1015388},
volume = {69},
year = {2004}
}
@inproceedings{Selvaraju2016,
abstract = {We propose a technique for making Convolutional Neural Network (CNN)-based models more transparent by visualizing the regions of input that are "important" for predictions from these models - or visual explanations. Our approach, called Gradient-weighted Class Activation Mapping (Grad-CAM), uses the class-specific gradient information flowing into the final convolutional layer of a CNN to produce a coarse localization map of the important regions in the image. Grad-CAM is a strict generalization of the Class Activation Mapping. Unlike CAM, Grad-CAM requires no re-training and is broadly applicable to any CNN-based architectures. We also show how Grad-CAM may be combined with existing pixel-space visualizations to create a high-resolution class-discriminative visualization (Guided Grad-CAM). We generate Grad-CAM and Guided Grad-CAM visual explanations to better understand image classification, image captioning, and visual question answering (VQA) models. In the context of image classification models, our visualizations (a) lend insight into their failure modes showing that seemingly unreasonable predictions have reasonable explanations, and (b) outperform pixel-space gradient visualizations (Guided Backpropagation and Deconvolution) on the ILSVRC-15 weakly supervised localization task. For image captioning and VQA, our visualizations expose the somewhat surprising insight that common CNN + LSTM models can often be good at localizing discriminative input image regions despite not being trained on grounded image-text pairs. Finally, we design and conduct human studies to measure if Guided Grad-CAM explanations help users establish trust in the predictions made by deep networks. Interestingly, we show that Guided Grad-CAM helps untrained users successfully discern a "stronger" deep network from a "weaker" one even when both networks make identical predictions.},
archivePrefix = {arXiv},
arxivId = {1610.02391},
author = {Selvaraju, Ramprasaath R. and Das, Abhishek and Vedantam, Ramakrishna and Cogswell, Michael and Parikh, Devi and Batra, Dhruv},
booktitle = {NIPS},
eprint = {1610.02391},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Selvaraju et al. - 2016 - Grad-CAM Why did you say that Visual Explanations from Deep Networks via Gradient-based Localization.pdf:pdf},
title = {{Grad-CAM: Why did you say that? Visual Explanations from Deep Networks via Gradient-based Localization}},
url = {http://arxiv.org/abs/1610.02391},
year = {2016}
}
@article{Backlund2015,
author = {Backlund, Peter B and Seepersad, Carolyn Conner and Kiehne, Thomas M},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Backlund, Seepersad, Kiehne - 2015 - All-Electric Ship Energy System Design Using Classifier-Guided Sampling.pdf:pdf},
number = {1},
pages = {77--85},
title = {{All-Electric Ship Energy System Design Using Classifier-Guided Sampling}},
volume = {1},
year = {2015}
}
@inproceedings{Howard2015,
author = {Howard, Michael and Klem, Bernie and Gorman, Joe},
booktitle = {Proceedings of the Advanced Maui Optical and Space Surveillance Technologies Conference},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Howard, Klem, Gorman - 2015 - RSO Characterization with Photometric Data Using Machine Learning.pdf:pdf},
title = {{RSO Characterization with Photometric Data Using Machine Learning}},
year = {2015}
}
@article{ElHindi2011,
annote = {preform prototype selection than the algorithm, not really tailored to NN

apply a variety of knn type algorithms, then NN},
author = {{El Hindi}, Khalil and Al-Akhras, Mousa},
issn = {12100552},
journal = {Neural Network World},
keywords = {Artificial neural network,Instance reduction,Instance selection,Instance-based learning,Machine learning,Noise filtering,Over learning,Overfitting,Prototype selection},
pages = {311--325},
title = {{Smoothing decision boundaries to avoid overfitting in neural network training}},
volume = {21},
year = {2011}
}
@inproceedings{Zeliff2017,
abstract = {{\textcopyright} Copyright 2017 ASME. Previous work tested a multi-objective genetic algorithm that was integrated with a machine learning classifier to reduce the number of objective function calls. Four machine learning classifiers and a baseline "No Classifier" option were evaluated. Using a machine learning classifier to create a hybrid multiobjective genetic algorithm reduced objective function calls by 75-85{\%} depending on the classifier used. This work expands the analysis of algorithm performance by considering six standard benchmark problems from the literature. The problems are designed to test the ability of the algorithm to identify the Pareto frontier and maintain population diversity. Results indicate a tradeoff between the objectives of Pareto frontier identification and solution diversity. The "No Classifier" baseline multiobjective genetic algorithm produces the frontier with the closest proximity to the true frontier while a classifier option provides the greatest diversity when the number of generations is fixed. However, there is a significant reduction in computational expense as the number of objective function calls required is significantly reduced, highlighting the advantage of this hybrid approach.},
address = {Cleveland, Ohio},
author = {Zeliff, Kayla and Bennette, Walter D and Ferguson, Scott M},
booktitle = {ASME 2017 International Design Engineering Technical Conferences and Computers and Information in Engineering Conference},
doi = {10.1115/DETC2017-68332},
file = {:Users/Walter/Documents/Literature/idetc2017{\_}68332.pdf:pdf},
isbn = {9780791858127},
title = {{Benchmarking the Performance of a Machine Learning Classifier Enabled Multiobjective Genetic Algorithm on Six Standard Test Functions}},
volume = {68332},
year = {2017}
}
@article{Zhang2003,
abstract = {This paper describes an application of a simplekNN approach to a novel classification problemwith an unbalanced class distribution. We dis-cuss here one particular problem area, extractingprotein names from the biological literature.Specifically, we empirically study the effects ofunder-sampling on the k nearest neighbor kNNapproach and five different methods of choosingnegative training examples. Our experimentalresults show that the kNN method is sensitive tothe number of negative examples selected andthe random selection of negative examplesworks better than three of the other four example selection methods.},
author = {Zhang, Jianping and Mani, Inderjeet},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Zhang, Mani - 2003 - kNN Approach to Unbalanced Data Distributions A Case Study involving Information Etraction.pdf:pdf},
journal = {Proceedings of Workshop on Learning from Imbalanced Datasets},
title = {{kNN Approach to Unbalanced Data Distributions: A Case Study involving Information Etraction}},
year = {2003}
}
@article{Walton2011,
abstract = {A new robust optimisation algorithm, which can be regarded as a modification of the recently developed cuckoo search, is presented. The modification involves the addition of information exchange between the top eggs, or the best solutions. Standard optimisation benchmarking functions are used to test the effects of these modifications and it is demonstrated that, in most cases, the modified cuckoo search performs as well as, or better than, the standard cuckoo search, a particle swarm optimiser, and a differential evolution strategy. In particular the modified cuckoo search shows a high convergence rate to the true global minimum even at high numbers of dimensions. {\textcopyright} 2011 Elsevier Ltd. All rights reserved.},
author = {Walton, S. and Hassan, O. and Morgan, K. and Brown, M. R.},
doi = {10.1016/j.chaos.2011.06.004},
isbn = {0960-0779},
issn = {09600779},
journal = {Chaos, Solitons and Fractals},
number = {9},
pages = {710--718},
publisher = {Elsevier Ltd},
title = {{Modified cuckoo search: A new gradient free optimisation algorithm}},
url = {http://dx.doi.org/10.1016/j.chaos.2011.06.004},
volume = {44},
year = {2011}
}
@article{Bansal2018,
abstract = {A classifier's low confidence in prediction is often indica-tive of whether its prediction will be wrong; in this case, in-puts are called known unknowns. In contrast, unknown un-knowns (UUs) are inputs on which a classifier makes a high confidence mistake. Identifying UUs is especially impor-tant in safety-critical domains like medicine (diagnosis) and law (recidivism prediction). Previous work by Lakkaraju et al. (2017) on identifying unknown unknowns assumes that the utility of each revealed UU is independent of the oth-ers, rather than considering the set holistically. While this assumption yields an efficient discovery algorithm, we ar-gue that it produces an incomplete understanding of the clas-sifier's limitations. In response, this paper proposes a new class of utility models that rewards how well the discovered UUs cover (or " explain ") a sample distribution of expected queries. Although choosing an optimal cover is intractable, even if the UUs were known, our utility model is monotone submodular, affording a greedy discovery strategy. Exper-imental results on four datasets show that our method out-performs bandit-based approaches and achieves within 60.9{\%} utility of an omniscient, tractable upper bound.},
author = {Bansal, Gagan and Weld, Daniel S and Allen, Paul G},
file = {:Users/Walter/Documents/Literature/bansal-aaai18.pdf:pdf},
journal = {Aaai 2018},
pages = {8},
title = {{A Coverage-Based Utility Model for Identifying Unknown Unknowns}},
url = {http://aiweb.cs.washington.edu/ai/pubs/bansal-aaai18.pdf},
year = {2018}
}
@article{Monard2004,
abstract = {There are several aspects that might influence the performance achieved by existing learning systems. It has been reported that one of these aspects is related to class imbalance in which examples in training data belonging to one class heavily outnumber the examples in the other class. In this situation, which is found in real world data describing an infrequent but important event, the learning system may have difficulties to learn the concept related to the minority class. In this work we perform a broad experimental evaluation involving ten methods, three of them proposed by the authors, to deal with the class imbalance problem in thirteen UCI data sets. Our experiments provide evidence that class imbalance does not systematically hinder the performance of learning systems. In fact, the problem seems to be related to learning with too few minority class examples in the presence of other complicating factors, such as class overlapping. Two of our proposed methods deal with these conditions directly, allying a known over-sampling method with data cleaning methods in order to produce better-defined class clusters. Our comparative experiments show that, in general, over-sampling methods provide more accurate results than under-sampling methods considering the area under the ROC curve (AUC). This result seems to contradict results previously published in the literature. Two of our proposed methods, Smote + Tomek and Smote + ENN, presented very good results for data sets with a small number of positive examples. Moreover, Random over-sampling, a very simple over-sampling method, is very competitive to more complex over-sampling methods. Since the over-sampling methods provided very good performance results, we also measured the syntactic complexity of the decision trees induced from over-sampled data. Our results show that these trees are usually more complex then the ones induced from original data. Random over-sampling usually produced the smallest increase in the mean number of induced rules and Smote + ENN the smallest increase in the mean number of conditions per rule, when compared among the investigated over-sampling methods.},
author = {Monard, Maria Carolina},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Monard - 2004 - A Study of the Behavior of Several Methods for Balancing Machine Learning Training Data.pdf:pdf},
journal = {Sigkdd Explorations},
number = {1},
pages = {20--29},
title = {{A Study of the Behavior of Several Methods for Balancing Machine Learning Training Data}},
volume = {6},
year = {2004}
}
@article{Feature2016,
author = {Feature, Unsupervised and Towards, Learning},
number = {1},
pages = {1--23},
title = {{Feature Topic : Big Data Table of Contents}},
volume = {7},
year = {2016}
}
@article{Webb2005,
abstract = {We counsel caution in the application of ROC analysis for prediction of classifier performance under varying class distributions.We argue that it is not reasonable to expect ROC analysis to provide accurate predic- tion of model performance under varying distributions if the classes contain causally relevant subclasses whose frequencies may vary at different rates or if there are attributes upon which the classes are causally dependent.},
author = {Webb, Geoffrey I. and Ting, Kai Ming},
issn = {08856125},
journal = {Machine Learning},
keywords = {Model evaluation,ROC analysis},
pages = {25--32},
title = {{On the application of ROC analysis to predict classification performance under varying class distributions}},
volume = {58},
year = {2005}
}
@article{Monard2002,
abstract = {Several aspects may influence the performance achieved by a classifier created by a Machine Learning system. One of these aspects is related to the difference between the numbers of examples belonging to each class. When this difference is large, the learning system may have difficulties to learn the concept related to the minority class. In this work 1, we discuss several issues related to learning with skewed class distributions, such as the relationship between cost-sensitive learning and class distributions, and the limitations of accuracy and error rate to measure the performance of classifiers. Also, we survey some methods proposed by the Machine Learning community to solve the problem of learning with imbalanced data sets, and discuss some limitations of these methods.},
author = {Monard, Mc and Batista, G},
isbn = {1 58603 292 5},
journal = {Advances in Logic, Artificial Intelligence and Robotics},
keywords = {class imbalance,cost-sensitive learning,machine learning},
pages = {173 -- 180},
title = {{Learning with skewed class distributions}},
url = {http://www.icmc.usp.br/{~}gbatista/files/laptec2002.pdf},
year = {2002}
}
@article{Sun2006,
abstract = {Classification of data with imbalanced class distributionhas posed a significant drawback of the performance attain-able by most standard classifier learning algorithms, whichassume a relatively balanced class distribution and equalmisclassification costs. This learning difficulty attracts a lotof research interests. Most efforts concentrate on bi-classproblems. However, bi-class is not the only scenario wherethe class imbalance problem prevails. Reported solutionsfor bi-class applications are not applicable to multi-classproblems. In this paper, we develop a cost-sensitive boost-ing algorithm to improve the classification performance ofimbalanced data involving multiple classes. One barrier ofapplying the cost-sensitive boosting algorithm to the imbal-anced data is that the cost matrix is often unavailable for aproblem domain. To solve this problem, we apply GeneticAlgorithm to search the optimum cost setup of each class.Empirical tests show that the proposed cost-sensitive boost-ing algorithm improves the classification performances ofimbalanced data sets significantly.},
author = {Sun, Yanmin and Kamel, Mohamed and Wang, Yang},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Sun, Kamel, Wang - 2006 - Boosting for Learning Multiple Classes with Imbalanced Class Distribution.pdf:pdf},
isbn = {0769527019},
journal = {ICDM},
pages = {592--602},
title = {{Boosting for Learning Multiple Classes with Imbalanced Class Distribution}},
volume = {6},
year = {2006}
}
@incollection{Srinivasan1999,
annote = {ROCCH extends to multiple classes},
author = {Srinivasan, Ashwin},
booktitle = {Technical report PRG-TR-2-99},
title = {{Note on the location of optimal classifiers in n-dimensional ROC space}},
year = {1999}
}
@article{Richards2011,
abstract = {With the coming data deluge from synoptic surveys, there is a need for frameworks that can quickly and automatically produce calibrated classification probabilities for newly observed variables based on small numbers of time-series measurements. In this paper, we introduce a methodology for variable-star classification, drawing from modern machine-learning techniques. We describe how to homogenize the information gleaned from light curves by selection and computation of real-numbered metrics (features), detail methods to robustly estimate periodic features, introduce tree-ensemble methods for accurate variable-star classification, and show how to rigorously evaluate a classifier using cross validation. On a 25-class data set of 1542 well-studied variable stars, we achieve a 22.8{\%} error rate using the random forest (RF) classifier; this represents a 24{\%} improvement over the best previous classifier on these data. This methodology is effective for identifying samples of specific science classes: for pulsational variables used in Milky Way tomography we obtain a discovery efficiency of 98.2{\%} and for eclipsing systems we find an efficiency of 99.1{\%}, both at 95{\%} purity. The RF classifier is superior to other methods in terms of accuracy, speed, and relative immunity to irrelevant features; the RF can also be used to estimate the importance of each feature in classification. Additionally, we present the first astronomical use of hierarchical classification methods to incorporate a known class taxonomy in the classifier, which reduces the catastrophic error rate from 8{\%} to 7.8{\%}. Excluding low-amplitude sources, the overall error rate improves to 14{\%}, with a catastrophic error rate of 3.5{\%}.},
archivePrefix = {arXiv},
arxivId = {1101.1959},
author = {Richards, Joseph W. and Starr, Dan L. and Butler, Nathaniel R. and Bloom, Joshua S. and Brewer, John M. and Crellin-Quick, Arien and Higgins, Justin and Kennedy, Rachel and Rischard, Maxime},
doi = {10.1088/0004-637X/733/1/10},
eprint = {1101.1959},
file = {:Users/Walter/Documents/Literature/Richards{\_}2011{\_}ApJ{\_}733{\_}10.pdf:pdf},
issn = {0004-637X},
journal = {The Astrophysical Journal},
keywords = {data analysis,even less clear the,general,in time and in,light curve is both,methods,more poorly sampled the,of a given source,on conceptual grounds,photometric,precision,provide an incomplete picture,second,stars,statistical,techniques,the,this picture is,variables},
number = {1},
pages = {10},
title = {{on Machine-Learned Classification of Variable Stars With Sparse and Noisy Time-Series Data}},
url = {http://stacks.iop.org/0004-637X/733/i=1/a=10?key=crossref.b01929a3dbee1845bb80f17663f565fa},
volume = {733},
year = {2011}
}
@article{Xingquan2006,
abstract = {Finding a small set of representative instances for large datasets can bring various benefits to data mining practitioners so they can (1) build a learner superior to the one constructed from the whole massive data; and (2) avoid working on the whole original dataset all the time. We propose in this paper a scalable representative instance selection and ranking (SRISTAR pronounced 3STAR) mechanism, which carries two unique features: (1) it provides a representative instance ranking list, so that users can always select instances from the top to the bottom, based on the number of examples they prefer; and (2) it investigates the behaviors of the underlying examples for instance selection, and the selection procedure tries to optimize the expected future error. Given a dataset, we first cluster instances into small data cells, each of which consists of instances with similar behaviors. Then we progressively evaluate data cells and their combinations, and order them into a list such that the learners built from the top cells are more accurate},
author = {Zhu, Xingquan and Wu, Xindong},
isbn = {0769525210},
issn = {10514651},
journal = {Proceedings - International Conference on Pattern Recognition},
pages = {352--355},
title = {{Scalable representative instance selection and ranking}},
volume = {3},
year = {2006}
}
@article{Lin2015,
abstract = {Instance selection is an important data pre-processing step in the knowledge discovery process. However, the dataset sizes of various domain problems are usually very large, and some are even non-stationary, composed of both old data and a large amount of new data samples. Current algorithms for solving this type of scalability problem have certain limitations, meaning they require a very high computational cost over very large scale datasets during instance selection. To this end, we introduce the ReDD (Representative Data Detection) approach, which is based on outlier pattern analysis and prediction. First, a machine learning model, or detector, is used to learn the patterns of (un)representative data selected by a specific instance selection method from a small amount of training data. Then, the detector can be used to detect the rest of the large amount of training data, or newly added data. We empirically evaluate ReDD over 50 domain datasets to examine the effectiveness of the learned detector, using four very large scale datasets for validation. The experimental results show that ReDD not only reduces the computational cost nearly two or three times by three baselines, but also maintains the final classification accuracy.},
annote = {From Duplicate 1 (Learning to Detect Representative Data for Large Scale Instance Selection - Lin, Wei-Chao; Tsai, Chih-Fong; Ke, Shih-Wen; Hung, Chia-Wen; Eberle, William)

A must read

Looks like a really neat idea, I may have had this idea at one point

From Duplicate 2 (Learning to Detect Representative Data for Large Scale Instance Selection - Lin, Wei-Chao; Tsai, Chih-Fong; Ke, Shih-Wen; Hung, Chia-Wen; Eberle, William)

What about time to construct SVM, this requires reduction in number of samples

5 training and test sets},
author = {Lin, Wei-Chao and Tsai, Chih-Fong and Ke, Shih-Wen and Hung, Chia-Wen and Eberle, William},
doi = {10.1016/j.jss.2015.04.038},
issn = {01641212},
journal = {Journal of Systems and Software},
keywords = {Data mining,Data reduction,Instance selection,Outlier detection,data mining,data reduction,instance selection,machine learning},
pages = {1--8},
publisher = {Elsevier Ltd.},
title = {{Learning to Detect Representative Data for Large Scale Instance Selection}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0164121215000813 http://www.sciencedirect.com/science/article/pii/S0164121215000813},
volume = {106},
year = {2015}
}
@article{Weiss2010,
abstract = {Abstract Many classifier induction systems express the induced classifier in terms of a disjunctive description. Small disjuncts are those disjuncts that classify few training examples. These disjuncts are interesting because they are known to have a much higher error rate than large disjuncts and are responsible for many, if not most, of all classification errors. Previous research has investigated this phenomenon by performing ad hoc analyses of a small number of data sets. In this article we provide a much more systematic study of small disjuncts and analyze how they affect classifiers induced from thirty real-world data sets. A new metric, error concentration, is used to show that for these thirty data sets classification errors are often heavily concentrated toward the smaller disjuncts. Various factors, including pruning, training-set size, noise and class imbalance are then analyzed to determine how they affect small disjuncts and the distribution of errors across disjuncts. This analysis provides many insights into why some data sets are difficult to learn from and also provides a better understanding of classifier learning in general. We believe that such an understanding is critical to the development of improved classifier induction algorithms. 1},
author = {Weiss, G.M.},
isbn = {978-1-4419-1279-4},
journal = {Data Mining},
pages = {193--226},
title = {{The impact of small disjuncts on classifier learning}},
url = {http://www.springerlink.com/index/JL51627XK8G79557.pdf},
volume = {8},
year = {2010}
}
@article{Japkowicz2002,
abstract = {In machine learning problems, differences in prior class probabilities -- or class imbalances -- have been reported to hinder the performance of some standard classifiers, such as decision trees. This paper presents a systematic study aimed at answering three different questions. First, we attempt to understand the nature of the class imbalance problem by establishing a relationship between concept complexity, size of the training set and class imbalance level. Second, we discuss several basic re-sampling or cost-modifying methods previously proposed to deal with the class imbalance problem and compare their effectiveness. The results obtained by such methods on artificial domains are linked to results in real-world domains. Finally, we investigate the assumption that the class imbalance problem does not only affect decision tree systems but also affects other classification systems such as Neural Networks and Support Vector Machines.},
author = {Japkowicz, Nathalie and Stephen, Shaju},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Japkowicz, Stephen - 2002 - The class imbalance problem A systematic study.pdf:pdf},
journal = {Intelligent Data Analysis},
keywords = {0,c5,class imbalances,concept learning,misclassification costs,multi-layer perceptrons,re-sampling,support},
number = {5},
pages = {429--449},
shorttitle = {The class imbalance problem},
title = {{The class imbalance problem: A systematic study}},
url = {http://iospress.metapress.com/content/MXUG8CJKJYLNK3N0},
volume = {6},
year = {2002}
}
@article{Grochowski2004,
abstract = {This paper is an continuation of the accompanying paper with the same main title. The first paper reviewed instance selection algorithms, here results of empirical comparison and comments are pre- sented. Several test were performed mostly on benchmark data sets from the machine learning repository at UCI. Instance selection algorithms were tested with neural networks and machine learning algorithms. 1},
author = {Grochowski, Marek and Jankowski, Norbert},
isbn = {3-540-22123-9},
issn = {03029743},
journal = {Artificial Intelligence and Soft Computing},
pages = {580--585},
title = {{Comparison of Instance Selection Algorithms II Results and Comments}},
year = {2004}
}
@article{Zitzler2000,
abstract = {In this paper, we provide a systematic comparison of various evolutionary approaches to multiobjective optimization using six carefully chosen test functions. Each test function involves a particular feature that is known to cause difficulty in the evolutionary optimization process, mainly in converging to the Pareto-optimal front (e.g., multimodality and deception). By investigating these different problem features separately, it is possible to predict the kind of problems to which a certain technique is or is not well suited. However, in contrast to what was suspected beforehand, the experimental results indicate a hierarchy of the algorithms under consideration. Furthermore, the emerging effects are evidence that the suggested test functions provide sufficient complexity to compare multiobjective optimizers. Finally, elitism is shown to be an important factor for improving evolutionary multiobjective search.},
author = {Zitzler, Eckart and Deb, Kalyanmoy and Thiele, Lothar},
doi = {10.1162/106365600568202},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Zitzler, Deb, Thiele - 2000 - Comparison of Multiobjective Evolutionary Algorithms Empirical Results.pdf:pdf},
isbn = {106365600568202},
issn = {1063-6560},
journal = {Evolutionary Computation},
keywords = {evolutionary algorithms,multiobjective optimization,pareto optimality,test functions},
number = {2},
pages = {173--195},
pmid = {10843520},
title = {{Comparison of Multiobjective Evolutionary Algorithms: Empirical Results}},
volume = {8},
year = {2000}
}
@article{Bradley1999,
abstract = {This paper is intended to serve as an overview of a rapidly emerging research and applications area. In addition to providing a general overview, motivating the importance of data mining problems within the area of knowledge discovery in databases, our aim is to list some of the pressing research challenges, and outline opportunities for contributions by the optimization research communities. Towards these goals, we include formulations of the basic categories of data mining methods as optimization problems. We also provide examples of successful mathematical programming approaches to some data mining problems.},
author = {Bradley, P. S. and Fayyad, U. M. and Mangasarian, O. L.},
journal = {INFORMS Journal on Computing},
number = {April 2015},
pages = {217--238},
title = {{Mathematical Programming for Data Mining: Formulations and Challenges}},
volume = {11},
year = {1999}
}
@article{Garcia2012,
abstract = {In supervised classification, we often encounter many real world problems in which the data do not have an equitable distribution among the different classes of the problem. In such cases, we are dealing with the so-called imbalanced data sets. One of the most used techniques to deal with this problem consists of preprocessing the data previously to the learning process. This paper proposes a method belonging to the family of the nested generalized exemplar that accomplishes learning by storing objects in Euclidean n-space. Classification of new data is performed by computing their distance to the nearest generalized exemplar. The method is optimized by the selection of the most suitable generalized exemplars based on evolutionary algorithms. An experimental analysis is carried out over a wide range of highly imbalanced data sets and uses the statistical tests suggested in the specialized literature. The results obtained show that our evolutionary proposal outperforms other classic and recent models in accuracy and requires to store a lower number of generalized examples. {\textcopyright} 2011 Elsevier B.V. All rights reserved.},
annote = {EA for nn and rule induction},
author = {Garc{\'{i}}a, Salvador and Derrac, Joaqu{\'{i}}n and Triguero, Isaac and Carmona, Crist{\'{o}}bal J. and Herrera, Francisco},
isbn = {0950-7051},
issn = {09507051},
journal = {Knowledge-Based Systems},
keywords = {Evolutionary algorithms,Imbalanced classification,Instance selection,Nested generalized exemplar learning,Rule induction},
pages = {3--12},
title = {{Evolutionary-based selection of generalized instances for imbalanced classification}},
volume = {25},
year = {2012}
}
@article{Hoiem2012,
abstract = {This paper shows how to analyze the influences of object characteristics$\backslash$non detection performance and the frequency and impact of different$\backslash$ntypes of false positives. In particular, we examine effects of occlusion,$\backslash$nsize, aspect ratio, visibility of parts, viewpoint, localization$\backslash$nerror, and confusion with semantically similar objects, other labeled$\backslash$nobjects, and background. We analyze two classes of detectors: the$\backslash$nVedaldi et al. multiple kernel learning detector and different versions$\backslash$nof the Felzenszwalb et al. detector. Our study shows that sensitivity$\backslash$nto size, localization error, and confusion with similar objects are$\backslash$nthe most impactful forms of error. Our analysis also reveals that$\backslash$nmany different kinds of improvement are necessary to achieve large$\backslash$ngains, making more detailed analysis essential for the progress of$\backslash$nrecognition research. By making our software and annotations available,$\backslash$nwe make it effortless for future researchers to perform similar analysis.},
author = {Hoiem, Derek and Chodpathumwan, Yodsawalai and Dai, Qieyun},
doi = {10.1007/978-3-642-33712-3_25},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Hoiem, Chodpathumwan, Dai - 2012 - Diagnosing error in object detectors.pdf:pdf},
isbn = {9783642337116},
issn = {03029743},
journal = {European conference on computer vision},
pages = {340--353},
title = {{Diagnosing error in object detectors}},
year = {2012}
}
@article{Sebban2000,
abstract = {The quality of a decision tree is usually evaluated through its complexity and its generalization accuracy. Tree-simpliÞcation procedures aim at optimizing these two performance criteria. Among them, data reduction techniques differ from pruning by their simpliÞcation strategy. Actually, while pruning algorithms directly control tree size to combat the overÞtting problem, data reduction techniques perform a data preprocessing prior to decision tree construction to improve the learning set quality. Recent experimental results have shown that randomly manipulating training set size has a direct impact on tree size, and therefore recommend the use of the latter simpliÞcation strategy. In this paper, we provide theoretical arguments justifying data preprocessing in favor of tree simpliÞcation. We also investigate new data reduction techniques, usually used in the Þeld of prototype selection. From experiments with 22 datasets, we show that some of them are very efficient to improve standard post-pruning performances.},
author = {Sebban, M and Nock, R and Chauchat, J H and Rakotomalala, R},
journal = {International Journal of Computers, Systems and Signals},
keywords = {decision tree,prototype selection,pruning,tree simplification},
number = {1},
pages = {85--105},
title = {{Impact of learning set quality and size on decision tree performances}},
volume = {1},
year = {2000}
}
@article{Longadge2013,
abstract = {In last few years there are major changes and evolution has been done on classification of data. As the application area of technology is increases the size of data also increases. Classification of data becomes difficult because of unbounded size and imbalance nature of data. Class imbalance problem become greatest issue in data mining. Imbalance problem occur where one of the two classes having more sample than other classes. The most of algorithm are more focusing on classification of major sample while ignoring or misclassifying minority sample. The minority samples are those that rarely occur but very important. There are different methods available for classification of imbalance data set which is divided into three main categories, the algorithmic approach, data-preprocessing approach and feature selection approach. Each of this technique has their own advantages and disadvantages. In this paper systematic study of each approach is define which gives the right direction for research in class imbalance problem.},
author = {Longadge, Rushi and Dongre, Snehalata and Malik, Latesh},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Longadge, Dongre, Malik - 2013 - Class Imbalance Problem in Data Mining Review.pdf:pdf},
keywords = {class imbalance problem,data,detecting network intrusions,fraud in banking operations,imbalance,other areas,rare class mining,situations are observed in,skewed data,such as detecting},
title = {{Class Imbalance Problem in Data Mining : Review}},
year = {2013}
}
@article{Vanwinckelen2012,
abstract = {Evaluation of predictive models is a ubiq- uitous task in machine learning and data mining. Cross-validation is often used as a means for evaluating models. There appears to be some confusion among re- searchers, however, about best practices for cross-validation, and about the interpreta- tion of cross-validation results. In particular, repeated cross-validation is often advocated, and so is the reporting of standard devia- tions, confidence intervals, or an indication of ”significance”. In this paper, we argue that, under many practical circumstances, when the goal of the experiments is to see how well the model returned by a learner will perform in practice in a particular domain, repeated cross-validation is not useful, and the report- ing of confidence intervals or significance is misleading. Our arguments are supported by experimental results.},
author = {Vanwinckelen, Gitte and Blockeel, Hendrik},
file = {:Users/Walter/Documents/Literature/OnEstimatingModelAccuracy.pdf:pdf},
journal = {21st Belgian-Dutch Conference on Machine Learning},
keywords = {conditional prediction error,predictive model evaluation,repeated cross-validation},
pages = {39--44},
title = {{On estimating model accuracy with repeated cross-validation}},
url = {https://lirias.kuleuven.be/handle/123456789/346385},
year = {2012}
}
@article{Bach2015,
abstract = {Understanding and interpreting classification decisions of automated image classification systems is of high value in many applications, as it allows to verify the reasoning of the system and provides additional information to the human expert. Although machine learning methods are solving very successfully a plethora of tasks, they have in most cases the disadvantage of acting as a black box, not providing any information about what made them arrive at a particular decision. This work proposes a general solution to the problem of understanding classification decisions by pixel-wise decomposition of nonlinear classifiers. We introduce a methodology that allows to visualize the contributions of single pixels to predictions for kernel-based classifiers over Bag of Words features and for multilayered neural networks. These pixel contributions can be visualized as heatmaps and are provided to a human expert who can intuitively not only verify the validity of the classification decision, but also focus further analysis on regions of potential interest. We evaluate our method for classifiers trained on PASCAL VOC 2009 images, synthetic image data containing geometric shapes, the MNIST handwritten digits data set and for the pre-trained ImageNet model available as part of the Caffe open source package.},
author = {Bach, Sebastian and Binder, Alexander and Montavon, Gr{\'{e}}goire and Klauschen, Frederick and M{\"{u}}ller, Klaus Robert and Samek, Wojciech},
doi = {10.1371/journal.pone.0130140},
file = {:Users/Walter/Documents/Literature/journal.pone.0130140.PDF:PDF},
isbn = {10.1371/journal.pone.0130140},
issn = {19326203},
journal = {PLoS ONE},
number = {7},
pages = {1--46},
pmid = {26161953},
title = {{On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation}},
volume = {10},
year = {2015}
}
@article{Garcia2009,
abstract = {Classification in imbalanced domains is a recent challenge in data mining. We refer to imbalanced classification when data presents many examples from one class and few from the other class, and the less representative class is the one which has more interest from the point of view of the learning task. One of the most used techniques to tackle this problem consists in preprocessing the data previously to the learning process. This preprocessing could be done through under-sampling; removing examples, mainly belonging to the majority class; and over-sampling, by means of replicating or generating new minority examples. In this paper, we propose an under-sampling procedure guided by evolutionary algorithms to perform a training set selection for enhancing the decision trees obtained by the C4.5 algorithm and the rule sets obtained by PART rule induction algorithm. The proposal has been compared with other under-sampling and over-sampling techniques and the results indicate that the new approach is very competitive in terms of accuracy when comparing with over-sampling and it outperforms standard under-sampling. Moreover, the obtained models are smaller in terms of number of leaves or rules generated and they can considered more interpretable. The results have been contrasted through non-parametric statistical tests over multiple data sets. Crown Copyright {\textcopyright} 2009.},
author = {Garc{\'{i}}a, Salvador and Fernandez, Alberto and Herrera, Francisco},
isbn = {15684946},
issn = {15684946},
journal = {Applied Soft Computing Journal},
keywords = {Data reduction,Decision trees,Evolutionary algorithms,Imbalanced classification,Rule induction,Training set selection},
pages = {1304--1314},
title = {{Enhancing the effectiveness and interpretability of decision tree and rule induction classifiers with evolutionary training set selection over imbalanced problems}},
volume = {9},
year = {2009}
}
@article{Cowell1993,
abstract = {Probabilistic expert systems based on Bayesian networks require initial specification of both qualitative graphical structure and quantitative conditional probability assessments. As (possibly incomplete) data accumulate on real cases, the parameters of the system may adapt, but it is also essential that the initial specifications be monitored with respect to their predictive performance. A range of monitors based on standardized scoring rules that are designed to detect both qualitative and quantitative departures from the specified model is presented. A simulation study demonstrates the efficacy of these monitors at uncovering such departures},
annote = {Do not see the benefit of this article},
author = {Cowell, Roger and Dawid, A. Phillip and Spiegelhalter, David},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Cowell, Dawid, Spiegelhalter - 1993 - Sequential Model Criticism in Probabilistic Expert Systems.pdf:pdf},
isbn = {0898713730},
issn = {09670661},
journal = {IEEE Transactions on Patern Analysis and Machine Intelligence},
number = {3},
pages = {209--219},
title = {{Sequential Model Criticism in Probabilistic Expert Systems}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=204903{\&}tag=1},
volume = {15},
year = {1993}
}
@article{Ishida2007,
abstract = {The area under the ROC curve (AUC) is considered a relevant criterion to deal with imbalanced data, misclassification costs and noisy data. Based on this preference, we present an algorithm for rule subset selection. The algorithm builds a Pareto Front using the Sensitivity and Specificity criteria selecting rules from a large set of rules. An empirical study is carried out to verify the influence of the A priori Parameter in Pareto Front Elite Algorithm. We compare our results with other rule induction algorithms and the results show that the new algorithm obtains a set of rules with greater values of the AUC.},
author = {Ishida, Celso Y. and Pozo, a. T R},
isbn = {0769529763},
journal = {Proceedings of The 7th International Conference on Intelligent Systems Design and Applications, ISDA 2007},
pages = {497--502},
title = {{Optimization of the AUC criterion for rule subset selection}},
year = {2007}
}
@article{Dai2015,
abstract = {Paragraph Vectors has been recently proposed as an unsupervised method for learning distributed representations for pieces of texts. In their work, the authors showed that the method can learn an embedding of movie review texts which can be leveraged for sentiment analysis. That proof of concept, while encouraging, was rather narrow. Here we consider tasks other than sentiment analysis, provide a more thorough comparison of Paragraph Vectors to other document modelling algorithms such as Latent Dirichlet Allocation, and evaluate performance of the method as we vary the dimensionality of the learned representation. We benchmarked the models on two document similarity data sets, one from Wikipedia, one from arXiv. We observe that the Paragraph Vector method performs significantly better than other methods, and propose a simple improvement to enhance embedding quality. Somewhat surprisingly, we also show that much like word embeddings, vector operations on Paragraph Vectors can perform useful semantic results.},
archivePrefix = {arXiv},
arxivId = {1507.07998},
author = {Dai, Andrew M. and Olah, Christopher and Le, Quoc V.},
eprint = {1507.07998},
file = {:Users/Walter/Documents/Literature/1507.07998.pdf:pdf},
pages = {1--8},
title = {{Document Embedding with Paragraph Vectors}},
url = {http://arxiv.org/abs/1507.07998},
year = {2015}
}
@article{Bar-Hillel2003,
author = {Bar-Hillel, a and Hertz, T and Shental, N and Weinshall, D},
file = {:Users/Walter/Documents/Literature/cf474bce70cfab2e944c4be01f99e741f1f4.pdf:pdf},
isbn = {1577351894},
journal = {Machine Learning},
keywords = {clustering,feature selection,learning from partial knowledge,may more important,semi,supervised learning,than specific algorithm,which later used},
pages = {11},
title = {{Learning Distance Functions using Equivalence Relations}},
url = {http://www.aaai.org/Papers/ICML/2003/ICML03-005.pdf},
volume = {20},
year = {2003}
}
@article{Krause2016,
abstract = {Understanding predictive models, in terms of interpreting and identifying actionable insights, is a challenging task. Often the importance of a feature in a model is only a rough esti-mate condensed into one number. However, our research goes beyond these nave estimates through the design and imple-mentation of an interactive visual analytics system, Prospec-tor. By providing interactive partial dependence diagnostics, data scientists can understand how features affect the predic-tion overall. In addition, our support for localized inspection allows data scientists to understand how and why specific dat-apoints are predicted as they are, as well as support for tweak-ing feature values and seeing how the prediction responds. Our system is then evaluated using a case study involving a team of data scientists improving predictive models for de-tecting the onset of diabetes from electronic medical records.},
author = {Krause, Josua and Perer, Adam and Ng, Kenney},
doi = {10.1145/2858036.2858529},
file = {:Users/Walter/Documents/Literature/adamPerer-Prospector-CHI2016.pdf:pdf},
isbn = {9781450333627},
journal = {ACM Conference on Human Factors in Computing Systems},
keywords = {Author Keywords interactive machine learning,Miscellaneous,partial dependence,predictive modeling,visual analytics},
pages = {5686--5697},
title = {{Interacting with Predictions: Visual Inspection of Black-box Machine Learning Models}},
url = {http://dl.acm.org/citation.cfm?doid=2858036.2858529},
year = {2016}
}
@article{Kohonen1997,
author = {Kohonen, Teuvo},
doi = {10.1109/ICNN.1997.611622},
file = {:Users/Walter/Documents/Literature/selfOrganizingMap.pdf:pdf},
isbn = {0-7803-4122-8},
issn = {0925-2312},
journal = {Proceedings of ICNN'97, International Conference on Neural Networks},
pages = {PL1----PL6},
title = {{Exploration of Very Large Databases by Self-Organizing Maps}},
year = {1997}
}
@misc{Tsymbal2004,
abstract = {In the real world concepts are often not stable but change with time. Typical examples of this are weather prediction rules and customers preferences. The underlying data distribution may change as well. Often these changes make the model built on old data inconsistent with the new data, and regular updating of the model is necessary. This problem, known as concept drift, complicates the task of learning a model from data and requires special approaches, different from commonly used techniques, which treat arriving instances as equally important contributors to the final concept. This paper considers different types of concept drift, peculiarities of the problem, and gives a critical review of existing approaches to the problem.},
author = {Tsymbal, Alexey},
booktitle = {Computer Science Department, Trinity College Dublin},
doi = {10.1.1.58.9085},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Tsymbal - 2004 - The problem of concept drift definitions and related work.pdf:pdf},
isbn = {1011158908},
pages = {2004--15},
title = {{The problem of concept drift: definitions and related work}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.58.9085{\&}rep=rep1{\&}type=pdf{\%}5Cnhttp://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.58.9085{\&}rep=rep1{\&}type=pdf},
year = {2004}
}
@article{Huijser2017,
abstract = {This paper is on active learning where the goal is to reduce the data annotation burden by interacting with a (human) oracle during training. Standard active learning methods ask the oracle to annotate data samples. Instead, we take a profoundly different approach: we ask for annotations of the decision boundary. We achieve this using a deep generative model to create novel instances along a 1d line. A point on the decision boundary is revealed where the instances change class. Experimentally we show on three data sets that our method can be plugged-in to other active learning schemes, that human oracles can effectively annotate points on the decision boundary, that our method is robust to annotation noise, and that decision boundary annotations improve over annotating data samples.},
annote = {They talk about how they map to Z

They say use a model for large scale experiments},
archivePrefix = {arXiv},
arxivId = {1703.06971},
author = {Huijser, Miriam and Gemert, Jan C.Van},
doi = {10.1109/ICCV.2017.565},
eprint = {1703.06971},
file = {:Users/Walter/Documents/Literature/Huijser{\_}Active{\_}Decision{\_}Boundary{\_}ICCV{\_}2017{\_}paper.pdf:pdf},
isbn = {9781538610329},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {5296--5305},
title = {{Active Decision Boundary Annotation with Deep Generative Models}},
volume = {2017-Octob},
year = {2017}
}
@article{Sriraman2017,
author = {Sriraman, Anand and Karande, Shirish},
file = {:Users/Walter/Documents/Literature/108{\_}Sriraman{\_}WiP{\_}CameraReady{\_}HCOMP2017{\_}Paper108.pdf:pdf},
title = {{Creating Curriculum Of Unknown Unknowns By Battling Machine Opponents}},
year = {2017}
}
@article{Laxman2007,
abstract = {Frequent episode discovery is a popular framework for mining data available as a long sequence of events. An episode is essentially a short ordered sequence of event types and the frequency of an episode is some suitable measure of how often the episode occurs in the data sequence. Recently,we proposed a new frequency measure for episodes based on the notion of non-overlapped occurrences of episodes in the event sequence, and showed that, such a definition, in addition to yielding computationally efficient algorithms, has some important theoretical properties in connecting frequent episode discovery with HMM learning. This paper presents some new algorithms for frequent episode discovery under this non-overlapped occurrences-based frequency definition. The algorithms presented here are better (by a factor of N, where N denotes the size of episodes being discovered) in terms of both time and space complexities when compared to existing methods for frequent episode discovery. We show through some simulation experiments, that our algorithms are very efficient. The new algorithms presented here have arguably the least possible orders of spaceand time complexities for the task of frequent episode discovery.},
author = {Laxman, Srivatsan and Sastry, P. S. and Unnikrishnan, K. P.},
doi = {10.1145/1281192.1281238},
isbn = {9781595936097},
journal = {Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining},
keywords = {Data Mining,Database Management},
pages = {410--419},
title = {{A fast algorithm for finding frequent episodes in event streams}},
url = {http://dl.acm.org/citation.cfm?id=1281238{\%}5Cnhttp://portal.acm.org/citation.cfm?id=1281238},
year = {2007}
}
@book{Ahn2006,
abstract = {Product Description This book deals with the fundamentals of genetic algorithms and their applications in a variety of different areas of engineering and science Most significant update to the second edition is the MATLAB codes that accompany the text Provides a thorough discussion of hybrid genetic algorithms Features more examples than first edition},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Ahn, Chang Wook},
booktitle = {Studies in Computational Intelligence},
doi = {10.1007/11543138_2},
eprint = {arXiv:1011.1669v3},
file = {:Users/Walter/Documents/Literature/R.L.Haupt, S.E.Haupt - Practical Genetic Algorithms.pdf:pdf},
isbn = {3540317589},
issn = {1860949X},
pages = {7--22},
pmid = {13483293},
title = {{Practical genetic algorithms}},
volume = {18},
year = {2006}
}
@article{Wang2014,
abstract = {In binary classification problems, receiver operating characteristic (ROC) graphs are commonly used for visualizing, organizing and selecting classifiers based on their performances. An important issue in the ROC literature is to obtain the ROC convex hull (ROCCH) that covers potentially optima for a given set of classifiers [1]. Maximizing the ROCCH means to maximize the true positive rate ( tpr) and minimize the false positive rate ( fpr) for every classifier in ROC space, while tpr and fpr are conflicting with each other. In this paper, we propose multiobjective genetic programming (MOGP) to obtain a group of nondominated classifiers, with which the maximum ROCCH can be achieved. Four different multiobjective frameworks, including Nondominated Sorting Genetic Algorithm II (NSGA-II), Multiobjective Evolutionary Algorithms Based on Decomposition (MOEA/D), Multiobjective selection based on dominated hypervolume (SMS-EMOA), and Approximation-Guided Evolutionary Multi-Objective (AG-EMOA) are adopted into GP, because all of them are successfully applied into many problems and have their own characters. To improve the performance of each individual in GP, we further propose a memetic approach into GP by defining two local search strategies specifically designed for classification problems. Experimental results based on 27 well-known UCI data sets show that MOGP performs significantly better than single objective algorithms such as FGP, GGP, EGP, and MGP, and other traditional machine learning algorithms such as C4.5, Naive Bayes, and PRIE. The experiments also demonstrate the efficacy of the local search operator in the MOGP framework. ?? 2013 Elsevier B.V.},
archivePrefix = {arXiv},
arxivId = {1303.3145},
author = {Wang, Pu and Tang, Ke and Weise, Thomas and Tsang, E. P K and Yao, Xin},
eprint = {1303.3145},
issn = {09252312},
journal = {Neurocomputing},
keywords = {AUC,Classification,Decision tree,Evolutionary multiobjective algorithm,Genetic programming,Memetic algorithm,ROC analysis,ROCCH},
pages = {102--118},
publisher = {Elsevier},
title = {{Multiobjective genetic programming for maximizing ROC performance}},
volume = {125},
year = {2014}
}
@article{Lamirel2014,
author = {Lamirel, Jean-Charles and Cuxac, Pascal and Chivukula, Aneesh Sreevallabh and Hajlaoui, Kafil},
doi = {10.1007/s10844-014-0317-4},
file = {:Users/Walter/Documents/Literature/optimizing text classification through efficient feature selection based on quality metric.pdf:pdf},
isbn = {0925-9902},
issn = {0925-9902},
journal = {Journal of Intelligent Information Systems},
keywords = {clustering quality index,feature maximization,feature selection,supervised learning,text,unbalanced data},
pages = {379--396},
title = {{Optimizing text classification through efficient feature selection based on quality metric}},
url = {http://link.springer.com/10.1007/s10844-014-0317-4},
year = {2014}
}
@article{Wu2017,
abstract = {Despite outperforming the human in many tasks, deep neural network models are also criticized for the lack of transparency and interpretability in decision making. The opaqueness results in uncertainty and low confidence when deploying such a model in model sharing scenarios, when the model is developed by a third party. For a supervised machine learning model, sharing training process including training data provides an effective way to gain trust and to better understand model predictions. However, it is not always possible to share all training data due to privacy and policy constraints. In this paper, we propose a method to disclose a small set of training data that is just sufficient for users to get the insight of a complicated model. The method constructs a boundary tree using selected training data and the tree is able to approximate the complicated model with high fidelity. We show that traversing data points in the tree gives users significantly better understanding of the model and paves the way for trustworthy model sharing.},
annote = {This is a little hokey or at the very least not easy to interpret with no training.

Suspect not peer reviewed

{\#}{\#}{\#} Background quotes

- Want to know how to share models with confidence

- "Sharing a trained model is cost-effective. Machine learning models are seen offered as a service in the cloud and charged in a Pay-As-You-Go model" 

- "Providing means to interpret a model is effective to improve model transparency and gain trust from model users."

- "A common approach is to find examples or prototypes to guide decision making, which is along the line of the case based reasoning (Aamodt and Plaza 1994)."

{\#}{\#}{\#} Background information

{\#}{\#}{\#} Their method 

- In this paper, we tackle this problem by provid- ing users a small set of training data points that are enough to characterize the decision boundaries of a complicated DNN model

- Explicable Boundary Tree

- Our aim is to approximate the decision boundaries of a givenDNNclassifier accurately with a small number of training data points.

- "The decision boundary of two classes can be represented as Equation 1, in which x is a set of training data points."

- EB-tree gives the traversal path on the tree as an explanation which assists the model users to understand the reasons behind pre- dictions.



{\#}{\#}{\#} Evaluation

- Use two image classification tasks
- MNIST with LeNet-5
- CIFAR-10 with RCNN 
- evaluate model mimicking ability (accuracy) (FScore?)
- Human study: 10 users, no ML experience, pick most descriptive from very similar choices


{\#}{\#}{\#} Unique paper aspects 

- Suspect this is not peer reviewed




{\#}{\#}{\#} Follow up

- (Bien and Tibshirani 2011). 

- ***Kim, B.; Khanna, R.; and Koyejo, O. O. 2016. Examples are not enough, learn to criticize! criticism for interpretability. In Advances in Neural Information Processing Systems, 2280–2288.***

- Learning an explainable model for more complex models 
- Craven and Shavlik 1996
- Schmitz, Aldrich, and Gouws 1999
- Boz 2002; Che et al. 2016

- Visualizing deep neural network neurons
- Kabra, Robie, and Branson 2015
- Yosinski et al. 2015 
- Zeiler and Fergus 2014

- Sample perbutation (LIME)
- Ribeiro, Singh, and Guestrin 2016
- Fong and Vedaldi 2017

- Most important data points: Koh and Liang 2017},
archivePrefix = {arXiv},
arxivId = {1709.03730},
author = {Wu, Huijun and Wang, Chen and Yin, Jie and Lu, Kai and Zhu, Liming},
eprint = {1709.03730},
file = {:Users/Walter/Documents/Literature/BoundaryVisualization.pdf:pdf},
title = {{Interpreting Shared Deep Learning Models via Explicable Boundary Trees}},
url = {http://arxiv.org/abs/1709.03730},
year = {2017}
}
@article{Olafsson2008,
abstract = {With the rapid growth of databases in many modern enterprises data mining has become an increasingly important approach for data analysis. The operations research community has contributed significantly to this field, especially through the formulation and solution of numerous data mining problems as optimization problems, and several operations research applications can also be addressed using data mining methods. This paper provides a survey of the intersection of operations research and data mining. The primary goals of the paper are to illustrate the range of interactions between the two fields, present some detailed examples of important research work, and provide comprehensive references to other important work in the area. The paper thus looks at both the different optimization methods that can be used for data mining, as well as the data mining process itself and how operations research methods can be used in almost every step of this process. Promising directions for future research are also identified throughout the paper. Finally, the paper looks at some applications related to the area of management of electronic services, namely customer relationship management and personalization. ?? 2006 Elsevier B.V. All rights reserved.},
author = {Olafsson, Sigurdur and Li, Xiaonan and Wu, Shuning},
isbn = {0377-2217},
issn = {03772217},
journal = {European Journal of Operational Research},
keywords = {Classification,Clustering,Data mining,Heuristics,Mathematical programming,Optimization},
pages = {1429--1448},
title = {{Operations research and data mining}},
volume = {187},
year = {2008}
}
@article{Chawla2004,
author = {Chawla, Nitesh V and Japkowicz, Nathalie and Drive, Prentice},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Chawla, Japkowicz, Drive - 2004 - Editorial Special Issue on Learning from Imbalanced Data Sets Aleksander Ko l cz.pdf:pdf},
journal = {ACM SIGKDD Explorations Newsletter},
number = {1},
pages = {1--6},
title = {{Editorial : Special Issue on Learning from Imbalanced Data Sets Aleksander Ko l cz}},
volume = {6},
year = {2004}
}
@inproceedings{Mahabal2017a,
abstract = {Astronomy light curves are sparse, gappy, and heteroscedastic. As a result standard time series methods regularly used for financial and similar datasets are of little help and astronomers are usually left to their own instruments and techniques to classify light curves. A common approach is to derive statistical features from the time series and to use machine learning methods, generally supervised, to separate objects into a few of the standard classes. In this work, we transform the time series to two-dimensional light curve representations in order to classify them using modern deep learning techniques. In particular, we show that convolutional neural networks based classifiers work well for broad characterization and classification. We use labeled datasets of periodic variables from CRTS survey and show how this opens doors for a quick classification of diverse classes with several possible exciting extensions.},
annote = {Create an image from light curves and learn CNN},
archivePrefix = {arXiv},
arxivId = {1709.06257},
author = {Mahabal, Ashish and Sheth, Kshiteej and Gieseke, Fabian and Pai, Akshay and Djorgovski, S. George and Drake, Andrew and Graham, Matthew and Collaboration, the CSS/CRTS/PTF},
booktitle = {IEEE SSCI 2017},
eprint = {1709.06257},
file = {:Users/Walter/Documents/Literature/LightCurveDeepLearningPaper.pdf:pdf},
isbn = {9781538627266},
pages = {2757--2764},
title = {{Deep-Learnt Classification of Light Curves}},
url = {http://arxiv.org/abs/1709.06257},
year = {2017}
}
@article{Zemel2013,
abstract = {We propose a learning algorithm for fair clas- sification that achieves both group fairness (the proportion of members in a protected group receiving positive classification is iden- tical to the proportion in the population as a whole), and individual fairness (similar in- dividuals should be treated similarly). We formulate fairness as an optimization prob- lem of finding a good representation of the data with two competing goals: to encode the data as well as possible, while simultaneously obfuscating any information about member- ship in the protected group. We show posi- tive results of our algorithm relative to other known techniques, on three datasets. More- over, we demonstrate several advantages to our approach. First, our intermediate rep- resentation can be used for other classifica- tion tasks (i.e., transfer learning is possible); secondly, we take a step toward learning a distance metric which can find important di- mensions of the data for classification},
author = {Zemel, Richard S and Wu, Yu and Swersky, Kevin and Pitassi, Toniann and Dwork, Cynthia},
file = {:Users/Walter/Documents/Literature/zemel13.pdf:pdf},
journal = {Proceedings of the 30th International Conference on Machine Learning},
pages = {325--333},
title = {{Learning Fair Representations}},
url = {http://jmlr.org/proceedings/papers/v28/zemel13.html},
volume = {28},
year = {2013}
}
@article{Rish2001,
author = {Rish, I},
doi = {10.1039/b104835j},
issn = {14639076},
journal = {IJCAI 2001 workshop on empirical methods in artificial {\ldots}},
pages = {41--46},
title = {{An empirical study of the naive Bayes classifier}},
url = {http://www.cc.gatech.edu/home/isbell/classes/reading/papers/Rish.pdf},
year = {2001}
}
@article{Ratanamahatana2004,
abstract = {The Dynamic Time Warping (DTW) distance measure is a technique that has long been known in speech recognition community. It allows a non-linear mapping of one signal to another by minimizing the distance between the two. A decade ago, DTW was introduced into Data Mining community as a utility for various tasks for time series problems including classification, clustering, and anomaly detection. The technique has flourished, particularly in the last three years, and has been applied to a variety of problems in various disciplines. In spite of DTW's great success, there are still several persistent “myths” about it. These myths have caused confusion and led to much wasted research effort. In this work, we will dispel these myths with the most comprehensive set of time series experiments ever conducted},
author = {Ratanamahatana, Ca and Keogh, E},
doi = {10.1097/01.CCM.0000279204.24648.44},
file = {:Users/Walter/Documents/Literature/7b64537a5ce4415177ab6ccb69480c2120c6.pdf:pdf},
isbn = {978-0-89871-593-4},
issn = {00903493},
journal = {Third Workshop on Mining Temporal and Sequential Data},
keywords = {data mining,dynamic time warping,experimentation},
pages = {22--25},
pmid = {15513920},
title = {{Everything you know about dynamic time warping is wrong}},
year = {2004}
}
@article{Szegedy2013,
abstract = {Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninter- pretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extent. We can cause the network to misclas- sify an image by applying a certain hardly perceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.},
archivePrefix = {arXiv},
arxivId = {arXiv:1312.6199v4},
author = {Szegedy, Christian and Zaremba, W and Sutskever, I},
doi = {10.1021/ct2009208},
eprint = {arXiv:1312.6199v4},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Szegedy, Zaremba, Sutskever - 2013 - Intriguing properties of neural networks.pdf:pdf},
isbn = {1549-9618},
issn = {15499618},
journal = {arXiv preprint arXiv: {\ldots}},
pages = {1--10},
pmid = {22545027},
title = {{Intriguing properties of neural networks}},
url = {http://arxiv.org/abs/1312.6199},
year = {2013}
}
@article{Richter2005,
abstract = {A basic observation is that case-based reasoning has roots in different disciplines: cognitive science, knowledge representation and processing, machine learning and mathematics. As a consequence, there are foundational aspects from each of these areas. We briefly discuss them and comment on the relations between these types of foundations.},
archivePrefix = {arXiv},
arxivId = {arXiv:1312.0049v1},
author = {Richter, Michael M. and Aamodt, Agnar},
doi = {10.1017/S0269888906000695},
eprint = {arXiv:1312.0049v1},
isbn = {0000000000000},
issn = {0269-8889},
journal = {The Knowledge Engineering Review},
number = {1983},
pages = {203},
title = {{Case-based reasoning foundations}},
volume = {20},
year = {2005}
}
@article{Zhang2014,
abstract = {Computer vision systems today fail frequently. They also fail abruptly without warning or explanation. Alleviating the former has been the primary focus of the community. In this work, we hope to draw the community's attention to the latter, which is arguably equally problematic for real applications. We promote two metrics to evaluate failure prediction. We show that a surprisingly straightforward and general approach, that we call ALERT, can predict the likely accuracy (or failure) of a variety of computer vision systems – semantic segmentation, vanishing point and camera parameter estimation, and image memorability prediction – on individual input images. We also explore attribute prediction, where classifiers are typically meant to generalize to new unseen categories. We show that ALERT can be useful in predicting failures of this transfer. Finally, we leverage ALERT to improve the performance of a downstream application of attribute prediction: zero-shot learning. We show that ALERT can outperform several strong baselines for zero-shot learning on four datasets.},
annote = {"While minimizing failures has been the primary focus of the community, embracing and effectively dealing with the failures has been mostly ignored"},
author = {Zhang, Peng and Wang, Jiuling and Farhadi, Ali and Hebert, Martial and Parikh, Devi},
doi = {10.1109/CVPR.2014.456},
file = {:Users/Walter/Documents/Literature/Zhang{\_}Predicting{\_}Failures{\_}of{\_}2014{\_}CVPR{\_}paper.pdf:pdf},
isbn = {9781479951178},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {3566--3573},
title = {{Predicting failures of vision systems}},
year = {2014}
}
@article{Ibrahim2013,
abstract = {Lip reading is a process used to recognize speech from the viewed physical movements of the lips. In this paper, we present a new automatic lip-reading system that uses geometrical information extracted from video sequences in the classification of dynamic lip movements and implemented in four variants of Hidden Markov Models. In the recognition of the English digits 0 to 9 as spoken by the subjects available in the CUAVE database, the proposed system is able to produce a word recognition performance of up to 68{\%}, a result better than that obtained using a conventional appearance-based Discrete Cosine Transform technique. The two approaches are also compared when operating under simulated changes in environment conditions that arise from head movements and alterations in image illumination. The performance of the appearance-based approach was adversely affected by such rotational and brightness changes, yet the performance of the geometrical-based method remained consistent, demonstrating its potential to be effective as part of a multimodal speech recognition system for use in noisy environments.},
annote = {AdaBoost},
author = {Ibrahim, M. Z. and Mulvaney, D. J.},
doi = {10.1109/EUROCON.2013.6625256},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Ibrahim, Mulvaney - 2013 - Robust geometrical-based lip-reading using hidden Markov models.pdf:pdf},
isbn = {9781467322324},
journal = {IEEE EuroCon 2013},
keywords = {Convex hull,Discrete cosine transform,Hidden Markov models,Lip geometry,Lip reading,OpenCV,Skin detection},
number = {July},
pages = {2011--2016},
title = {{Robust geometrical-based lip-reading using hidden Markov models}},
year = {2013}
}
@incollection{Salerno2013,
author = {Salerno, J.J. and Smith, J.E. and Geiler, W.M. and McCabe, P.K. and Panasyuk, A.V. and Bennette, W.D. and Kwiat, A.},
booktitle = {Handbook of Computational Approaches to Counterterrorism},
doi = {10.1007/978-1-4614-5311-6_17},
isbn = {9781461453116},
pages = {363--400},
title = {{The NOEM: A Tool for Understanding/Exploring the Complexities of Today's Operational Environment.}},
year = {2013}
}
@article{Ho2002,
annote = {maximize accuracy
minimize number of instances
knn
wrapper{\~{}}genetic algorithm},
author = {Ho, S Y and Liu, C C and Liu, S},
doi = {10.1016/S0167-8655(02)00109-5},
isbn = {0167-8655},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {feature selection,intelligent genetic algorithm,minimum reference set,nearest neighbor classifier},
number = {13},
pages = {1495--1503},
title = {{Design of an optimal nearest neighbor classifier using an intelligent genetic algorithm}},
url = {http://www.sciencedirect.com/science/article/pii/S0167865502001095},
volume = {23},
year = {2002}
}
@article{Lipton2016,
abstract = {Supervised machine learning models boast re-markable predictive capabilities. But can you trust your model? Will it work in deployment? What else can it tell you about the world? We want models to be not only good, but inter-pretable. And yet the task of interpretation ap-pears underspecified. Papers provide diverse and sometimes non-overlapping motivations for in-terpretability, and offer myriad notions of what attributes render models interpretable. Despite this ambiguity, many papers proclaim inter-pretability axiomatically, absent further explana-tion. In this paper, we seek to refine the dis-course on interpretability. First, we examine the motivations underlying interest in interpretabil-ity, finding them to be diverse and occasionally discordant. Then, we address model properties and techniques thought to confer interpretability, identifying transparency to humans and post-hoc explanations as competing notions. Throughout, we discuss the feasibility and desirability of dif-ferent notions, and question the oft-made asser-tions that linear models are interpretable and that deep neural networks are not.},
annote = {sweeth athsma example},
archivePrefix = {arXiv},
arxivId = {arXiv:1606.03490v1},
author = {Lipton, Zachary C},
eprint = {arXiv:1606.03490v1},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Lipton - 2016 - The Mythos of Model Interpretability.pdf:pdf},
journal = {ICML Workshop on Human Interpretability in Machine Learning},
keywords = {Black Box,Deep Learning,Interpretability,Machine Learning,Supervised Learning},
title = {{The Mythos of Model Interpretability}},
year = {2016}
}
@article{Li2016,
abstract = {Performance of machine learning algorithms depends critically on identifying a good set of hyperparameters. While current methods offer efficiencies by adaptively choosing new configurations to train, an alternative strategy is to adaptively allocate resources across the selected configurations. We formulate hyperparameter optimization as a pure-exploration non-stochastic infinitely many armed bandit problem where allocation of additional resources to an arm corresponds to training a configuration on larger subsets of the data. We introduce Hyperband for this framework and analyze its theoretical properties, providing several desirable guarantees. We compare Hyperband with state-of-the-art Bayesian optimization methods and a random search baseline on a comprehensive benchmark including 117 datasets. Our results on this benchmark demonstrate that while Bayesian optimization methods do not outperform random search trained for twice as long, Hyperband in favorable settings offers valuable speedups.},
archivePrefix = {arXiv},
arxivId = {1603.06560},
author = {Li, Lisha and Jamieson, Kevin and DeSalvo, Giulia and Rostamizadeh, Afshin and Talwalkar, Ameet},
eprint = {1603.06560},
journal = {arXiv preprint},
keywords = {arms,bandits with infinitely many,deep learning,hyperparameter optimization,model selection,online optimization},
title = {{Efficient Hyperparameter Optimization and Infinitely Many Armed Bandits}},
url = {http://arxiv.org/abs/1603.06560},
year = {2016}
}
@article{Provost1997,
abstract = {We analyze critically the use of classification accuracy to compare classifiers on natural data sets, providing a thorough investigation using ROC analysis, standard machine learning algorithms, and standard benchmark data sets. The results raise serious concerns about the use of accuracy for comparing classifiers and drawinto question the conclusions that can be drawn from such studies. In the course of the presentation, we describe and demonstrate what we believe to be the proper use of ROC analysis for comparative studies in machine learning research. We argue that this methodology is preferable both for making practical choices and for drawing scientific conclusions.},
annote = {Why maximizing accuracy is not minimizing cost
Maybe don't use accuracy

- no one classifier will always be superior
- assigning class cost is virtually impossible},
author = {Provost, Foster and Fawcett, Tom and Kohavi, Ron},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Provost, Fawcett, Kohavi - 1997 - The Case Against Accuracy Estimation for Comparing Induction Algorithms.pdf:pdf},
isbn = {1-55860-556-8},
journal = {Proceedings of the Fifteenth International Conference on Machine Learning1},
pages = {445--453},
pmid = {15933812881907238759},
title = {{The Case Against Accuracy Estimation for Comparing Induction Algorithms}},
year = {1997}
}
@article{Reeves1998,
author = {Reeves, C R and Taylor, S J},
journal = {Parallel Problem Solving from Nature - {\{}PPSN V{\}}},
pages = {633--642},
title = {{Selection of Training Data for Neural Networks by a Genetic Algorithm}},
volume = {1498},
year = {1998}
}
@article{Swets1988,
abstract = {Diagnostic systems of several kinds are used to distinguish between two classes of events, essentially "signals" and "noise". For them, analysis in terms of the "relative operating characteristic" of signal detection theory provides a precise and valid measure of diagnostic accuracy. It is the only measure available that is uninfluenced by decision biases and prior probabilities, and it places the performances of diverse systems on a common, easily interpreted scale. Representative values of this measure are reported here for systems in medical imaging, materials testing, weather forecasting, information retrieval, polygraph lie detection, and aptitude testing. Though the measure itself is sound, the values obtained from tests of diagnostic systems often require qualification because the test data on which they are based are of unsure quality. A common set of problems in testing is faced in all fields. How well these problems are handled, or can be handled in a given field, determines the degree of confidence that can be placed in a measured value of accuracy. Some fields fare much better than others.},
annote = {ROC curve
Measuring accuracy with test data
Can't increase true positive without increasing false positive for a fixed classifier.},
author = {Swets, J a},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Swets - 1988 - Measuring the accuracy of diagnostic systems.pdf:pdf},
isbn = {0036-8075 (Print)$\backslash$r0036-8075 (Linking)},
issn = {0036-8075},
journal = {Science (New York, N.Y.)},
keywords = {ROC curves,accuracy measure},
mendeley-tags = {ROC curves,accuracy measure},
number = {4857},
pages = {1285--1293},
pmid = {3287615},
title = {{Measuring the accuracy of diagnostic systems.}},
volume = {240},
year = {1988}
}
@inproceedings{Wilkinson2011,
author = {Wilkinson, Leland and Anand, Anushka and Tuan, Dang Nhon},
booktitle = {Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining},
isbn = {9781450308137},
keywords = {random projections,supervised classification},
pages = {6--14},
title = {{CHIRP : A New Classifier Based on Composite Hypercubes on Iterated Random Projections}},
year = {2011}
}
@article{Demiralp2014,
abstract = {Visualization design can benefit from careful consideration of perception, as different assignments of visual encoding variables such as color, shape and size affect how viewers interpret data. In this work, we introduce perceptual kernels: distance ma- trices derived from aggregate perceptual judgments. Perceptual kernels represent perceptual differences between and within visual variables in a reusable form that is directly applicable to visualization evaluation and automated design. We report results from crowd- sourced experiments to estimate kernels for color, shape, size and combinations thereof. We analyze kernels estimated using five different judgment types—including Likert ratings among pairs, ordinal triplet comparisons, and manual spatial arrangement—and compare them to existing perceptualmodels. We derive recommendations for collecting perceptual similarities, and then demonstrate how the resulting kernels can be applied to automate visualization design decisions.},
author = {Demiralp, {\c{C}}aʇatay and Bernstein, Michael S. and Heer, Jeffrey},
doi = {10.1109/TVCG.2014.2346978},
file = {:Users/Walter/Documents/Literature/1933{\_}20tvcg12-demiralp-2346978.pdf:pdf},
isbn = {1077-2626},
issn = {10772626},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {Automated visualization,Crowdsourcing,Design,Encoding,Model,Perception,Visual embedding,Visualization},
number = {12},
pages = {1933--1943},
pmid = {26356907},
title = {{Learning perceptual kernels for visualization design}},
volume = {20},
year = {2014}
}
@article{Provost1997a,
abstract = {Applications of inductive learning algorithms to real world data mining problems have shown repeatedly that using accuracy to compare classifers is not adequate because the underlying assumptions rarely hold We present a method for the comparison of classifer performance that is robust to imprecise class distributions and misclassication costs The ROC convex hull method combines techniques from ROC analysis decision analysis and computational geometry and adapts them to the particulars of analyzing learned classifers The method is efficient and incremental minimizes the management of classifer performance data and allows for clear visual comparisons and sensitivity analyses},
annote = {Good article for bashing accuracy

Advocate identifying classifiers that could be optimal. These would lie on the convex hull of the ROC curves.

GAVE ME AN IDEA FOR AN OBJECTIVE FUNCTION. WANT TO LOOK AND SEE IF OTHERS HAVE USED IT
https://scholar.google.com/scholar?cites=1549565557425400165{\&}as{\_}sdt=5,33{\&}sciodt=0,33{\&}hl=en


- maybe good source for imbalanced data
- references papers where techniques may be compared
- explains how cost sensitive is currently being licked
- references cost sensitve techniques right before conclusion 
- few can handle cost changes without chaning classifiers},
author = {Provost, Foster and Fawcett, Tom},
isbn = {1-57735-027-8},
journal = {Third International Conference on Knowledge Discovery and Data Mining},
pages = {43--48},
title = {{Analysis and Visualization of Classi er Performance : Comparison under Imprecise Class and Cost Distributions Evaluating and Visualizing Classi er}},
url = {http://dblp.uni-trier.de/db/conf/kdd/kdd97.html{\#}ProvostF97},
year = {1997}
}
@article{Sullivan2014,
abstract = {Citizen-science projects engage volunteers to gather or process data to address scientific questions. But citizen-science projects vary in their ability to contribute usefully for science, conservation, or public policy. eBird has evolved from a basic citizen-science project into a collective enterprise, taking a novel approach to citizen science by developing cooperative partnerships among experts in a wide range of fields: population and distributions, conservation biologists, quantitative ecologists, statisticians, computer scientists, GIS and informatics specialists, application developers, and data administrators. The goal is to increase data quantity through participant recruitment and engagement, but also to quantify and control for data quality issues such as observer variability, imperfect detection of species, and both spatial and temporal bias in data collection. Advances at the interface among ecology, statistics, and computer science allow us to create new species distribution models that provide accurate estimates across broad spatial and temporal scales with extremely detailed resolution. eBird data are openly available and used by a broad spectrum of students, teachers, scientists, NGOs, government agencies, land managers, and policy makers. Feedback from this broad data use community helps identify development priorities. As a result, eBird has become a major source of biodiversity data, increasing our knowledge of the dynamics of species distributions, and having a direct impact on the conservation of birds and their habitats.},
author = {Sullivan, Brian L. and Aycrigg, Jocelyn L. and Barry, Jessie H. and Bonney, Rick E. and Bruns, Nicholas and Cooper, Caren B. and Damoulas, Theo and Dhondt, Andr{\'{e}} a. and Dietterich, Tom and Farnsworth, Andrew and Fink, Daniel and Fitzpatrick, John W. and Fredericks, Thomas and Gerbracht, Jeff and Gomes, Carla and Hochachka, Wesley M. and Iliff, Marshall J. and Lagoze, Carl and {La Sorte}, Frank a. and Merrifield, Matthew and Morris, Will and Phillips, Tina B. and Reynolds, Mark and Rodewald, Amanda D. and Rosenberg, Kenneth V. and Trautmann, Nancy M. and Wiggins, Andrea and Winkler, David W. and Wong, Weng-Keen and Wood, Christopher L. and Yu, Jun and Kelling, Steve},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Sullivan et al. - 2014 - The eBird enterprise An integrated approach to development and application of citizen science.pdf:pdf},
issn = {00063207},
journal = {Biological Conservation},
month = {jan},
pages = {31--40},
publisher = {Elsevier Ltd},
title = {{The eBird enterprise: An integrated approach to development and application of citizen science}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0006320713003820},
volume = {169},
year = {2014}
}
@article{Safe2004,
author = {Safe, Martin and Carballido, Jessica and Ponzoni, Ignacio and Brignole, Nelida},
journal = {Advances in Artificial Intelligence},
pages = {405--413},
title = {{On Stopping Criteria for Genetic Algorithms}},
volume = {17},
year = {2004}
}
@inproceedings{Adler,
abstract = {—Data-trained predictive models see widespread use, but for the most part they are used as black boxes which output a prediction or score. It is therefore hard to acquire a deeper understanding of model behavior, and in particular how different features influence the model prediction. This is important when interpreting the behavior of complex models, or asserting that certain problematic attributes (like race or gender) are not unduly influencing decisions. In this paper, we present a technique for auditing black-box models, which lets us study the extent to which existing models take advantage of particular features in the dataset, without knowing how the models work. Our work focuses on the problem of indirect influence: how some features might indirectly influence outcomes via other, related features. As a result, we can find attribute influences even in cases where, upon further direct examination of the model, the attribute is not referred to by the model at all. Our approach does not require the black-box model to be retrained. This is important if (for example) the model is only accessible via an API, and contrasts our work with other methods that investigate feature influence like feature selection. We present experimental evidence for the effectiveness of our procedure using a variety of publicly available datasets and models. We also validate our procedure using techniques from interpretable learning and feature selection, as well as against other black-box auditing procedures.},
archivePrefix = {arXiv},
arxivId = {1602.07043},
author = {Adler, Philip and Falk, Casey and Friedler, Sorelle A and Rybeck, Gabriel and Scheidegger, Carlos and Smith, Brandon and Venkatasubramanian, Suresh},
booktitle = {IEEE International Conference on Data Mining, ICDM},
doi = {10.1109/ICDM.2016.0011},
eprint = {1602.07043},
file = {:Users/Walter/Documents/Literature/auditing{\_}icdm{\_}2016.pdf:pdf},
isbn = {978-1-5090-5473-2},
issn = {978-1-5090-5473-2},
title = {{Auditing Black-box Models for Indirect Influence}},
year = {2017}
}
@article{Ramesh2014,
annote = {Must read and try to understand},
author = {Ramesh, B},
file = {:Users/Walter/Documents/Literature/Support vector machine using efficient instant selection for micro array data sets.pdf:pdf},
isbn = {9781479939756},
keywords = {classification,data,gene expression data,instance selection,support vector machine},
pages = {0--3},
title = {{Support Vector Machine using Efficient Instant Selection for Micro Array Data Sets}},
year = {2014}
}
@article{Johansson2011,
abstract = {Random forest is an often used ensemble technique, renowned for its high predictive performance. Random forests models are, however, due to their sheer complexity inherently opaque, making human interpretation and analysis impossible. This paper presents a method of approximating the random forest with just one decision tree. The approach uses oracle coaching, a recently suggested technique where a weaker but transparent model is generated using combinations of regular training data and test data initially labeled by a strong classifier, called the oracle. In this study, the random forest plays the part of the oracle, while the transparent models are decision trees generated by either the standard tree inducer J48, or by evolving genetic programs. Evaluation on 30 data sets from the UCI repository shows that oracle coaching significantly improves both accuracy and area under ROC curve, compared to using training data only. As a matter of fact, resulting single tree models are as accurate as the random forest, on the specific test instances. Most importantly, this is not achieved by inducing or evolving huge trees having perfect fidelity; a large majority of all trees are instead rather compact and clearly comprehensible. The experiments also show that the evolution outperformed J48, with regard to accuracy, but that this came at the expense of slightly larger trees.},
annote = {Create one decision tree to replace a random forest},
author = {Johansson, Ulf and Sonstrod, Cecilia and Lofstrom, Tuve},
doi = {10.1109/CEC.2011.5949785},
isbn = {9781424478347},
issn = {Pending},
journal = {2011 IEEE Congress of Evolutionary Computation, CEC 2011},
pages = {1444--1451},
title = {{One tree to explain them all}},
year = {2011}
}
@article{Reinartz2002,
annote = {Has a line about data mining goals

Evaluation criteria for instance selection includes -},
author = {Reinartz, T},
journal = {Data Mining and Knowledge Discovery},
keywords = {data reduction,focusing,instance selection,sampling},
number = {2},
pages = {191--210},
title = {{A Unifying Veiw on Instance Selection}},
volume = {6},
year = {2002}
}
@article{Singh2016,
annote = {write computer programs as explanations to models},
archivePrefix = {arXiv},
arxivId = {1611.07579},
author = {Singh, Sameer},
eprint = {1611.07579},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Singh - 2016 - Programs as Black-Box Explanations.pdf:pdf},
number = {Nips},
pages = {1--5},
title = {{Programs as Black-Box Explanations}},
year = {2016}
}
@article{Lai1985,
author = {Lai, T L and Robbins, Herbert},
journal = {Adv. Appl. Math.},
pages = {4--22},
title = {{Asymptotically Efficient Adaptive Allocation Rules}},
volume = {6},
year = {1985}
}
@article{Liu2016,
author = {Liu, Bin and Yao, Li and Han, Dapeng},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Liu, Yao, Han - 2016 - Harnessing ontology and machine learning for RSO classification.pdf:pdf},
issn = {2193-1801},
journal = {SpringerPlus},
number = {1},
pages = {1655},
publisher = {Springer International Publishing},
title = {{Harnessing ontology and machine learning for RSO classification}},
volume = {5},
year = {2016}
}
@article{Jiang2008,
author = {Jiang, Jing},
file = {:Users/Walter/Library/Application Support/Mendeley Desktop/Downloaded/Jiang - 2008 - A literature survey on domain adaptation of statistical classifiers.pdf:pdf},
journal = {://Sifaka. Cs. Uiuc. Edu/Jiang4/Domainadaptation/Survey},
number = {March},
pages = {1--12},
title = {{A literature survey on domain adaptation of statistical classifiers}},
url = {http://sifaka.cs.uiuc.edu/jiang4/domain{\_}adaptation/survey/da{\_}survey.pdf},
year = {2008}
}
@article{Lopez2013,
abstract = {Training classifiers with datasets which suffer of imbalanced class distributions is an important problem in data mining. This issue occurs when the number of examples representing the class of interest is much lower than the ones of the other classes. Its presence in many real-world applications has brought along a growth of attention from researchers. We shortly review the many issues in machine learning and applications of this problem, by introducing the characteristics of the imbalanced dataset scenario in classification, presenting the specific metrics for evaluating performance in class imbalanced learning and enumerating the proposed solutions. In particular, we will describe preprocessing, cost-sensitive learning and ensemble techniques, carrying out an experimental study to contrast these approaches in an intra and inter-family comparison. We will carry out a thorough discussion on the main issues related to using data intrinsic characteristics in this classification problem. This will help to improve the current models with respect to: the presence of small disjuncts, the lack of density in the training data, the overlapping between classes, the identification of noisy data, the significance of the borderline instances, and the dataset shift between the training and the test distributions. Finally, we introduce several approaches and recommendations to address these problems in conjunction with imbalanced data, and we will show some experimental examples on the behavior of the learning algorithms on data with such intrinsic characteristics. {\textcopyright} 2013 Elsevier Inc. All rights reserved.},
author = {L{\'{o}}pez, Victoria and Fern{\'{a}}ndez, Alberto and Garc{\'{i}}a, Salvador and Palade, Vasile and Herrera, Francisco},
issn = {00200255},
journal = {Information Sciences},
keywords = {Cost-sensitive learning,Dataset shift,Imbalanced dataset,Noisy data,Sampling,Small disjuncts},
pages = {113--141},
publisher = {Elsevier Inc.},
title = {{An insight into classification with imbalanced data: Empirical results and current trends on using data intrinsic characteristics}},
url = {http://dx.doi.org/10.1016/j.ins.2013.07.007},
volume = {250},
year = {2013}
}
@article{Tran2018,
abstract = {We introduce an effective model to overcome the problem of mode collapse when training Generative Adversarial Networks (GAN). Firstly, we propose a new generator objective that finds it better to tackle mode collapse. And, we apply an independent Autoencoders (AE) to constrain the generator and consider its reconstructed samples as "real" samples to slow down the convergence of discriminator that enables to reduce the gradient vanishing problem and stabilize the model. Secondly, from mappings between latent and data spaces provided by AE, we further regularize AE by the relative distance between the latent and data samples to explicitly prevent the generator falling into mode collapse setting. This idea comes when we find a new way to visualize the mode collapse on MNIST dataset. To the best of our knowledge, our method is the first to propose and apply successfully the relative distance of latent and data samples for stabilizing GAN. Thirdly, our proposed model, namely Generative Adversarial Autoencoder Networks (GAAN), is stable and has suffered from neither gradient vanishing nor mode collapse issues, as empirically demonstrated on synthetic, MNIST, MNIST-1K, CelebA and CIFAR-10 datasets. Experimental results show that our method can approximate well multi-modal distribution and achieve better results than state-of-the-art methods on these benchmark datasets. Our model implementation is published here: https://github.com/tntrung/gaan},
archivePrefix = {arXiv},
arxivId = {1803.08887},
author = {Tran, Ngoc-Trung and Bui, Tuan-Anh and Cheung, Ngai-Man},
eprint = {1803.08887},
file = {:Users/Walter/Documents/Literature/1803.08887.pdf:pdf},
keywords = {auto-encoders,genera-,generative adversarial network,tive model,variational auto-encoders},
pages = {1--16},
title = {{Generative Adversarial Autoencoder Networks}},
url = {http://arxiv.org/abs/1803.08887},
year = {2018}
}
@article{Patnaik2011,
abstract = {This dissertation investigates algorithmic techniques for temporal process discovery in many domains. Many different formalisms have been proposed for modeling temporal processes such as motifs, dynamic Bayesian networks and partial orders, but the direct inference of such models from data has been computationally intensive or even intractable. In this work, we propose the mining of frequent episodes as a bridge to inferring more formal models of temporal processes. This enables us to combine the advantages of frequent episode mining, which conducts level wise search over constrained spaces, with the formal basis of process representations, such as probabilistic graphical models and partial orders. We also investigate the mining of frequent episodes in infinite data streams which further expands their applicability into many modern data mining contexts. To demonstrate the usefulness of our methods, we apply them in different problem contexts such as: sensor networks in data centers, multi-neuronal spike train analysis in neuroscience, and electronic medical records in medical informatics.},
author = {Patnaik, Debprakash},
journal = {Technology},
keywords = {copyright 2011,debprakash patnaik,dynamic bayesian networks,episodes in temporal process,frequent episodes,graphical models,modeling,motifs,multiple uses of frequent,temporal data mining},
title = {{Multiple Uses of Frequent Episodes in Temporal Process Modeling}},
year = {2011}
}
